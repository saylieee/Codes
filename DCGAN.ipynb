{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGANworking100.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saylieee/Codes/blob/master/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-MjTgbWSDFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0\n",
        "#https://github.com/soumith/ganhacks "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz_YwcSpt_HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install keras numpy mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jTEFiOIb6MH",
        "colab_type": "text"
      },
      "source": [
        "DCGAN is a Generative Adversarial Network (GAN) using CNN.\n",
        "The generator tries to fool the discriminator by generating fake images.\n",
        "The discriminator learns to discriminate real from fake images.\n",
        "The generator + discriminator form an adversarial network.\n",
        "DCGAN trains the discriminator and adversarial networks alternately.\n",
        "During training, not only the discriminator learns to distinguish real from\n",
        "fake images, it also coaches the generator part of the adversarial on how\n",
        "to improve its ability to generate fake images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHwlAuMQo5mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from tensorflow.keras.layers import LeakyReLU, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA_Gbpv2pPmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 100\n",
        "img_rows, img_cols = 28, 28\n",
        "img_channels = 1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "final_img = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h3sYjCBcDtV",
        "colab_type": "text"
      },
      "source": [
        "  Build a Generator Model to generate fake images\n",
        "    Output activation is sigmoid\n",
        "    Sigmoid converges easily.\n",
        "    Returns:\n",
        "        generator (Model): Generator Model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSBYGNeWpaJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model(): \n",
        "    dropout = 0.4\n",
        "    depth = 256 # 64+64+64+64\n",
        "    dim = 7\n",
        "    \n",
        "    model = Sequential()\n",
        "    # In: 100\n",
        "    # Out: dim x dim x depth\n",
        "    model.add(Dense(dim*dim*depth, input_dim=latent_dim))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Reshape((dim, dim, depth)))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    # In: dim x dim x depth\n",
        "    # Out: 2*dim x 2*dim x depth/2\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
        "    model.add(BatchNormalization(momentum=0.9))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
        "    model.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    \n",
        "    #model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYIWWxQJchar",
        "colab_type": "text"
      },
      "source": [
        "Build a Discriminator Model\n",
        "    Stack of LeakyReLU-Conv2D to discriminate real from fake.\n",
        "    The network does not converge with BN so it is not used here\n",
        "    unlike in [1] or original paper.\n",
        "    Returns:\n",
        "        discriminator (Model): Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYkLwSAdpaeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_model():\n",
        "    depth = 64\n",
        "    dropout = 0.4\n",
        "    input_shape = (img_rows, img_cols, img_channels)\n",
        "    \n",
        "    model = Sequential()\n",
        "    # In: 28 x 28 x 1, depth = 1\n",
        "    # Out: 14 x 14 x 1, depth=64\n",
        "    model.add(Conv2D(depth, 5, strides=2, input_shape=input_shape, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    # Out: 1-dim probability\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMVvofbYpajZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build discriminator model\n",
        "discriminator = discriminator_model()\n",
        "discriminator.compile(loss='binary_crossentropy', \n",
        "                      optimizer=RMSprop(lr=0.0002, decay=6e-8), \n",
        "                      metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5sjiH1Wpanp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build generator model\n",
        "generator = generator_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLmsBP5jpasM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze the weights of discriminator during adversarial training\n",
        "# original paper uses Adam, but discriminator converges easily with RMSprop\n",
        "def adversarial_model():\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    discriminator.trainable = False\n",
        "    model.add(discriminator)\n",
        "    model.compile(loss='binary_crossentropy', \n",
        "                  optimizer=RMSprop(lr=0.0001, decay=3e-8), \n",
        "                  metrics=['accuracy'])\n",
        "    discriminator.trainable = True\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "854WtXuDpkWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build adversarial model\n",
        "adversarial = adversarial_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHTaYGnNsy9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxV48FRqh6wd",
        "colab_type": "text"
      },
      "source": [
        "Generate fake images and plot them\n",
        "    For visualization purposes, generate fake images\n",
        "    then plot them in a square grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc5sXrLQpmqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_images(saveArray, saveToFile=False, fake=True, samples=16, noise=None, epoch=0 ):    \n",
        "    filename = 'mnist.png'\n",
        "\n",
        "    if fake:\n",
        "        if noise is None:\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[samples, latent_dim])\n",
        "        else:\n",
        "            filename = \"mnist_%d.png\" % epoch\n",
        "        images = generator.predict(noise)\n",
        "    else:\n",
        "        i = np.random.randint(0, x_train.shape[0], samples)\n",
        "        images = x_train[i, :, :, :]\n",
        "        \n",
        "    if saveArray:\n",
        "      im_list.append(images)\n",
        "      \n",
        "    #plt.figure(figsize=(100,100))\n",
        "\n",
        "  #  for i in range(images.shape[0]):\n",
        "   #     plt.subplot(32, 32, i+1)\n",
        "    #    image = images[i, :, :, :]\n",
        "     #   image = np.reshape(image, [img_rows, img_cols])\n",
        "       # plt.imshow(image, cmap='gray')\n",
        "       # plt.axis('off')\n",
        "    \n",
        "  #  plt.tight_layout()\n",
        "   # if saveToFile:\n",
        "        \n",
        "    #    plt.savefig(filename)\n",
        "     #   plt.close('all')\n",
        "    #else:\n",
        "     #   plt.show()\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KmRHfK0DN7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
        "  # plot loss\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(d1_hist, label='D-loss on real')\n",
        "  plt.plot(d2_hist, label='D-loss on fake')\n",
        "  plt.plot(g_hist, label='Gen loss')\n",
        "  plt.title('model losses')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # plot discriminator accuracy\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(a1_hist, label='Disc-acc on real')\n",
        "  plt.plot(a2_hist, label='Disc-acc on fake')\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\t# save plot to file\n",
        "  plt.savefig('Disc_Gen_acc_losses.png')\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJQIopu8jFBb",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbITaZYec6gF",
        "colab_type": "text"
      },
      "source": [
        "Train the Discriminator and Adversarial Networks\n",
        "Alternately train Discriminator and Adversarial networks by batch.\n",
        "Discriminator is trained first with properly real and fake images.\n",
        "Adversarial is trained next with fake images pretending to be real\n",
        "Generate sample images per save_interval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYFUJmUoppTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## the generator image is saved every 100 steps\n",
        "## noise vector to see how the generator output evolves during training\n",
        "def train(train_epochs=5000, batch_size=256, save_interval=100): \n",
        "           \n",
        "        noise_input = None\n",
        "        saveArray = False\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
        "\n",
        "        # prepare lists for storing stats each iteration\n",
        "        d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
        " \n",
        "        for epoch in range(train_epochs):\n",
        "        # train the discriminator for 1 batch\n",
        "        # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
        "        # randomly pick real images from dataset\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator : select batch of real img, generate batch of fake img, \n",
        "            #label real as 1, fake as 0, train D on batch, calculate loss\n",
        "            # ---------------------\n",
        "\n",
        "            # number of elements in train dataset\n",
        "            train_size = x_train.shape[0]\n",
        "            \n",
        "            # select a random half of images\n",
        "            images_real = x_train[np.random.randint(0, train_size, size=batch_size), :, :, :]\n",
        "            y_real = np.ones([batch_size, 1])\n",
        "\n",
        "            d_loss1, d_acc1 = discriminator.train_on_batch(images_real, y_real)\n",
        "            # sample noise and generate a batch of new images\n",
        "            # generate fake images from noise using generator \n",
        "            # generate noise using uniform distribution\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
        "            #print(noise.shape) 256*100\n",
        "\n",
        "            # generate fake images\n",
        "            images_fake = generator.predict(noise)\n",
        "            #print(images_fake.shape) (256,28,28,1)\"\"\"\n",
        "            \n",
        "            # train the discriminator (real classified as ones and generated as zeros)\n",
        "            # real + fake images = 1 batch of train data\n",
        "            #x = np.concatenate((images_real, images_fake))\n",
        "            # label real and fake images\n",
        "            # real images label is 1.0\n",
        "            y_fake = np.zeros([batch_size, 1])\n",
        "            # fake images label is 0.0\n",
        "            # train discriminator network, log the loss and accuracy\n",
        "            d_loss2, d_acc2 = discriminator.train_on_batch(images_fake, y_fake)\n",
        "            \n",
        "            d_loss = 0.5 * np.add(d_loss1,d_loss2)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Adversory Model\n",
        "            # ---------------------\n",
        "            \n",
        "            # train the adversory (wants discriminator to mistake images as real)\n",
        "            # train the adversarial network for 1 batch\n",
        "            # 1 batch of fake images with label=1.0\n",
        "            # since the discriminator weights are frozen in adversarial network\n",
        "            # only the generator is trained\n",
        "\n",
        "            # generate noise using uniform distribution\n",
        "            #noise = np.random.uniform(-1.0,1.0, size=[batch_size, latent_dim])\n",
        "\n",
        "            # label fake images as real or 1.0\n",
        "            y = np.ones([batch_size, 1])\n",
        "\n",
        "            #noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
        "            # train the adversarial network \n",
        "            # note that unlike in discriminator training, \n",
        "            # we do not save the fake images in a variable\n",
        "            # the fake images go to the discriminator input of the adversarial\n",
        "            # for classification\n",
        "            # log the loss and accuracy\n",
        "           \n",
        "            g_loss  = adversarial.train_on_batch(noise, y)\n",
        "            \n",
        "            # record history\n",
        "            d1_hist.append(d_loss1)\n",
        "            d2_hist.append(d_loss2)\n",
        "            g_hist.append(g_loss)\n",
        "            a1_hist.append(d_acc1)\n",
        "            a2_hist.append(d_acc2)\n",
        "  \n",
        "            log_msg = \"%d: [Discriminator loss: %f, real acc: %f, fake acc: %f]\" % (epoch, d_loss, d_acc1, d_acc2)\n",
        "            log_msg = \"%s  [Adverserial loss: %f, acc: %f]\" % (log_msg, g_loss[0], g_loss[1])\n",
        "            print(log_msg)\n",
        "\n",
        "\n",
        "            if save_interval>0:\n",
        "              if epoch+1==train_epochs:\n",
        "                saveArray = True\n",
        "                  # plot generator images on a periodic basis\n",
        "              plot_images(saveArray, saveToFile=True, samples=noise_input.shape[0],noise=noise_input, epoch=(epoch)+1  )\n",
        "\n",
        "        plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNLXKhrupsek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDqblkt1pxYV",
        "colab_type": "code",
        "outputId": "39a0e9a0-e38e-42ef-e538-b6cb8fd0f4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def main():\n",
        "  #timer started\n",
        "  timer = ElapsedTimer()\n",
        "  train(train_epochs=2000, batch_size=1024, save_interval=10) \n",
        "\n",
        "  im_array =  np.array(im_list)\n",
        "  im_array = np.reshape(im_array, (1024, 28, 28, 1))\n",
        "\n",
        "  #print(\"final arr : \",im_array.shape)\n",
        "  #print(im_array[0, :, :, :].shape)\n",
        "\n",
        "  #pixels = im_array[0].reshape((28, 28))\n",
        "  #plt.imshow(pixels, cmap='gray')\n",
        "  #plt.show()\n",
        "\n",
        "  #timer stopped\n",
        "  timer.elapsed_time()\n",
        "\n",
        "  #plot_images(fake=True)\n",
        "  #plot_images(fake=False, saveToFile=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [Discriminator loss: 1.096598, real acc: 0.836914, fake acc: 0.000000]  [Adverserial loss: 0.741548, acc: 0.000000]\n",
            "1: [Discriminator loss: 0.679591, real acc: 0.975586, fake acc: 0.500000]  [Adverserial loss: 0.754807, acc: 0.000000]\n",
            "2: [Discriminator loss: 0.650400, real acc: 0.981445, fake acc: 1.000000]  [Adverserial loss: 0.764473, acc: 0.000000]\n",
            "3: [Discriminator loss: 0.612535, real acc: 0.995117, fake acc: 1.000000]  [Adverserial loss: 0.750519, acc: 0.049805]\n",
            "4: [Discriminator loss: 0.557720, real acc: 0.989258, fake acc: 1.000000]  [Adverserial loss: 0.655325, acc: 0.739258]\n",
            "5: [Discriminator loss: 0.466238, real acc: 0.991211, fake acc: 1.000000]  [Adverserial loss: 0.409799, acc: 0.997070]\n",
            "6: [Discriminator loss: 0.340561, real acc: 0.988281, fake acc: 1.000000]  [Adverserial loss: 0.159428, acc: 1.000000]\n",
            "7: [Discriminator loss: 0.241098, real acc: 0.982422, fake acc: 1.000000]  [Adverserial loss: 0.053074, acc: 1.000000]\n",
            "8: [Discriminator loss: 0.161000, real acc: 0.984375, fake acc: 1.000000]  [Adverserial loss: 0.021622, acc: 1.000000]\n",
            "9: [Discriminator loss: 0.100519, real acc: 0.995117, fake acc: 1.000000]  [Adverserial loss: 0.012469, acc: 1.000000]\n",
            "10: [Discriminator loss: 0.070845, real acc: 0.993164, fake acc: 1.000000]  [Adverserial loss: 0.006567, acc: 1.000000]\n",
            "11: [Discriminator loss: 0.045484, real acc: 0.997070, fake acc: 1.000000]  [Adverserial loss: 0.003929, acc: 1.000000]\n",
            "12: [Discriminator loss: 0.026245, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.004860, acc: 1.000000]\n",
            "13: [Discriminator loss: 0.025348, real acc: 0.997070, fake acc: 1.000000]  [Adverserial loss: 0.001714, acc: 1.000000]\n",
            "14: [Discriminator loss: 0.016043, real acc: 0.999023, fake acc: 1.000000]  [Adverserial loss: 0.001786, acc: 1.000000]\n",
            "15: [Discriminator loss: 0.011251, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.002280, acc: 1.000000]\n",
            "16: [Discriminator loss: 0.011692, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.001202, acc: 1.000000]\n",
            "17: [Discriminator loss: 0.007999, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.000971, acc: 1.000000]\n",
            "18: [Discriminator loss: 0.006406, real acc: 0.999023, fake acc: 1.000000]  [Adverserial loss: 0.000971, acc: 1.000000]\n",
            "19: [Discriminator loss: 0.005186, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.001332, acc: 1.000000]\n",
            "20: [Discriminator loss: 0.005402, real acc: 0.999023, fake acc: 1.000000]  [Adverserial loss: 0.001671, acc: 0.999023]\n",
            "21: [Discriminator loss: 0.003843, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.001258, acc: 1.000000]\n",
            "22: [Discriminator loss: 0.002713, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.003809, acc: 0.999023]\n",
            "23: [Discriminator loss: 0.003867, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.004880, acc: 0.998047]\n",
            "24: [Discriminator loss: 0.003819, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.003396, acc: 1.000000]\n",
            "25: [Discriminator loss: 0.004351, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.004651, acc: 0.999023]\n",
            "26: [Discriminator loss: 0.003060, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.004627, acc: 0.999023]\n",
            "27: [Discriminator loss: 0.003595, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.011070, acc: 0.996094]\n",
            "28: [Discriminator loss: 0.015046, real acc: 0.999023, fake acc: 1.000000]  [Adverserial loss: 6.703787, acc: 0.013672]\n",
            "29: [Discriminator loss: 1.050739, real acc: 0.835938, fake acc: 0.079102]  [Adverserial loss: 0.000078, acc: 1.000000]\n",
            "30: [Discriminator loss: 0.006082, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.000157, acc: 1.000000]\n",
            "31: [Discriminator loss: 0.006479, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.000527, acc: 1.000000]\n",
            "32: [Discriminator loss: 0.006833, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.000141, acc: 1.000000]\n",
            "33: [Discriminator loss: 0.007090, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.000264, acc: 1.000000]\n",
            "34: [Discriminator loss: 0.007686, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.000579, acc: 1.000000]\n",
            "35: [Discriminator loss: 0.008523, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.000889, acc: 1.000000]\n",
            "36: [Discriminator loss: 0.008486, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.004596, acc: 0.998047]\n",
            "37: [Discriminator loss: 0.009993, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.007827, acc: 0.998047]\n",
            "38: [Discriminator loss: 0.009554, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.020472, acc: 0.996094]\n",
            "39: [Discriminator loss: 0.009778, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.079716, acc: 0.965820]\n",
            "40: [Discriminator loss: 0.010429, real acc: 0.999023, fake acc: 1.000000]  [Adverserial loss: 0.164530, acc: 0.940430]\n",
            "41: [Discriminator loss: 0.010769, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.252657, acc: 0.898438]\n",
            "42: [Discriminator loss: 0.013670, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 1.233563, acc: 0.541016]\n",
            "43: [Discriminator loss: 0.205251, real acc: 0.987305, fake acc: 0.831055]  [Adverserial loss: 29.882645, acc: 0.000000]\n",
            "44: [Discriminator loss: 5.332203, real acc: 0.000000, fake acc: 0.747070]  [Adverserial loss: 0.549997, acc: 0.728516]\n",
            "45: [Discriminator loss: 0.034048, real acc: 1.000000, fake acc: 1.000000]  [Adverserial loss: 0.913564, acc: 0.515625]\n",
            "46: [Discriminator loss: 0.041307, real acc: 1.000000, fake acc: 0.997070]  [Adverserial loss: 2.071013, acc: 0.156250]\n",
            "47: [Discriminator loss: 0.040385, real acc: 0.998047, fake acc: 0.999023]  [Adverserial loss: 3.013275, acc: 0.040039]\n",
            "48: [Discriminator loss: 0.047141, real acc: 0.996094, fake acc: 0.998047]  [Adverserial loss: 3.962358, acc: 0.007812]\n",
            "49: [Discriminator loss: 0.053967, real acc: 0.996094, fake acc: 0.997070]  [Adverserial loss: 5.097842, acc: 0.000977]\n",
            "50: [Discriminator loss: 0.056967, real acc: 0.993164, fake acc: 0.999023]  [Adverserial loss: 5.926188, acc: 0.000000]\n",
            "51: [Discriminator loss: 0.083109, real acc: 0.983398, fake acc: 0.991211]  [Adverserial loss: 8.086450, acc: 0.000000]\n",
            "52: [Discriminator loss: 0.244706, real acc: 0.952148, fake acc: 0.872070]  [Adverserial loss: 15.307983, acc: 0.000000]\n",
            "53: [Discriminator loss: 2.592465, real acc: 0.206055, fake acc: 0.030273]  [Adverserial loss: 10.237282, acc: 0.000000]\n",
            "54: [Discriminator loss: 0.256936, real acc: 0.792969, fake acc: 1.000000]  [Adverserial loss: 6.060815, acc: 0.000000]\n",
            "55: [Discriminator loss: 0.026040, real acc: 0.993164, fake acc: 1.000000]  [Adverserial loss: 5.702995, acc: 0.000000]\n",
            "56: [Discriminator loss: 0.020667, real acc: 0.998047, fake acc: 1.000000]  [Adverserial loss: 5.675344, acc: 0.000000]\n",
            "57: [Discriminator loss: 0.026459, real acc: 0.997070, fake acc: 1.000000]  [Adverserial loss: 5.671692, acc: 0.000000]\n",
            "58: [Discriminator loss: 0.031255, real acc: 0.994141, fake acc: 1.000000]  [Adverserial loss: 5.724216, acc: 0.000000]\n",
            "59: [Discriminator loss: 0.031385, real acc: 0.997070, fake acc: 1.000000]  [Adverserial loss: 6.017347, acc: 0.000000]\n",
            "60: [Discriminator loss: 0.042274, real acc: 0.991211, fake acc: 0.999023]  [Adverserial loss: 5.686651, acc: 0.000000]\n",
            "61: [Discriminator loss: 0.042030, real acc: 0.993164, fake acc: 1.000000]  [Adverserial loss: 4.994852, acc: 0.001953]\n",
            "62: [Discriminator loss: 0.042193, real acc: 0.994141, fake acc: 0.997070]  [Adverserial loss: 4.177561, acc: 0.007812]\n",
            "63: [Discriminator loss: 0.047848, real acc: 0.996094, fake acc: 0.997070]  [Adverserial loss: 3.944530, acc: 0.035156]\n",
            "64: [Discriminator loss: 0.067592, real acc: 0.993164, fake acc: 0.973633]  [Adverserial loss: 4.446050, acc: 0.027344]\n",
            "65: [Discriminator loss: 0.115894, real acc: 0.986328, fake acc: 0.949219]  [Adverserial loss: 5.438323, acc: 0.004883]\n",
            "66: [Discriminator loss: 0.403625, real acc: 0.949219, fake acc: 0.753906]  [Adverserial loss: 8.740794, acc: 0.000000]\n",
            "67: [Discriminator loss: 1.245124, real acc: 0.512695, fake acc: 0.549805]  [Adverserial loss: 0.866760, acc: 0.523438]\n",
            "68: [Discriminator loss: 0.508021, real acc: 1.000000, fake acc: 0.553711]  [Adverserial loss: 7.247056, acc: 0.000000]\n",
            "69: [Discriminator loss: 0.735259, real acc: 0.570312, fake acc: 0.791016]  [Adverserial loss: 3.264565, acc: 0.008789]\n",
            "70: [Discriminator loss: 0.221218, real acc: 0.966797, fake acc: 0.882812]  [Adverserial loss: 5.268987, acc: 0.000000]\n",
            "71: [Discriminator loss: 0.404977, real acc: 0.828125, fake acc: 0.846680]  [Adverserial loss: 5.090279, acc: 0.000000]\n",
            "72: [Discriminator loss: 0.356558, real acc: 0.840820, fake acc: 0.884766]  [Adverserial loss: 4.855224, acc: 0.000000]\n",
            "73: [Discriminator loss: 0.283126, real acc: 0.877930, fake acc: 0.918945]  [Adverserial loss: 4.796868, acc: 0.000000]\n",
            "74: [Discriminator loss: 0.262738, real acc: 0.895508, fake acc: 0.935547]  [Adverserial loss: 4.993236, acc: 0.000000]\n",
            "75: [Discriminator loss: 0.243391, real acc: 0.902344, fake acc: 0.947266]  [Adverserial loss: 4.918019, acc: 0.000000]\n",
            "76: [Discriminator loss: 0.227583, real acc: 0.920898, fake acc: 0.954102]  [Adverserial loss: 5.109563, acc: 0.000000]\n",
            "77: [Discriminator loss: 0.196929, real acc: 0.932617, fake acc: 0.962891]  [Adverserial loss: 5.127066, acc: 0.000000]\n",
            "78: [Discriminator loss: 0.217560, real acc: 0.919922, fake acc: 0.940430]  [Adverserial loss: 5.525521, acc: 0.000000]\n",
            "79: [Discriminator loss: 0.258195, real acc: 0.902344, fake acc: 0.926758]  [Adverserial loss: 5.369805, acc: 0.000000]\n",
            "80: [Discriminator loss: 0.212842, real acc: 0.916016, fake acc: 0.966797]  [Adverserial loss: 5.149558, acc: 0.000000]\n",
            "81: [Discriminator loss: 0.164323, real acc: 0.938477, fake acc: 0.987305]  [Adverserial loss: 4.865222, acc: 0.000000]\n",
            "82: [Discriminator loss: 0.140948, real acc: 0.958984, fake acc: 0.977539]  [Adverserial loss: 5.455585, acc: 0.000000]\n",
            "83: [Discriminator loss: 0.201573, real acc: 0.928711, fake acc: 0.940430]  [Adverserial loss: 5.751052, acc: 0.000000]\n",
            "84: [Discriminator loss: 0.217869, real acc: 0.910156, fake acc: 0.951172]  [Adverserial loss: 5.336175, acc: 0.000000]\n",
            "85: [Discriminator loss: 0.122161, real acc: 0.950195, fake acc: 0.996094]  [Adverserial loss: 4.703712, acc: 0.000000]\n",
            "86: [Discriminator loss: 0.071671, real acc: 0.990234, fake acc: 0.996094]  [Adverserial loss: 5.227077, acc: 0.000000]\n",
            "87: [Discriminator loss: 0.120133, real acc: 0.968750, fake acc: 0.985352]  [Adverserial loss: 5.906818, acc: 0.000000]\n",
            "88: [Discriminator loss: 0.216201, real acc: 0.931641, fake acc: 0.919922]  [Adverserial loss: 6.223878, acc: 0.000000]\n",
            "89: [Discriminator loss: 0.257809, real acc: 0.910156, fake acc: 0.931641]  [Adverserial loss: 4.955491, acc: 0.000000]\n",
            "90: [Discriminator loss: 0.105191, real acc: 0.963867, fake acc: 0.994141]  [Adverserial loss: 4.952994, acc: 0.000000]\n",
            "91: [Discriminator loss: 0.118368, real acc: 0.966797, fake acc: 0.982422]  [Adverserial loss: 5.886118, acc: 0.000000]\n",
            "92: [Discriminator loss: 0.225651, real acc: 0.931641, fake acc: 0.917969]  [Adverserial loss: 5.511704, acc: 0.000000]\n",
            "93: [Discriminator loss: 0.288828, real acc: 0.932617, fake acc: 0.837891]  [Adverserial loss: 6.613739, acc: 0.000000]\n",
            "94: [Discriminator loss: 0.545150, real acc: 0.750977, fake acc: 0.765625]  [Adverserial loss: 3.002006, acc: 0.002930]\n",
            "95: [Discriminator loss: 0.148258, real acc: 0.991211, fake acc: 0.937500]  [Adverserial loss: 6.131293, acc: 0.000000]\n",
            "96: [Discriminator loss: 0.605312, real acc: 0.781250, fake acc: 0.631836]  [Adverserial loss: 4.955250, acc: 0.000000]\n",
            "97: [Discriminator loss: 0.288530, real acc: 0.848633, fake acc: 0.957031]  [Adverserial loss: 3.603748, acc: 0.000000]\n",
            "98: [Discriminator loss: 0.169723, real acc: 0.964844, fake acc: 0.953125]  [Adverserial loss: 4.857443, acc: 0.000000]\n",
            "99: [Discriminator loss: 0.339831, real acc: 0.871094, fake acc: 0.846680]  [Adverserial loss: 4.856504, acc: 0.000000]\n",
            "100: [Discriminator loss: 0.419350, real acc: 0.847656, fake acc: 0.783203]  [Adverserial loss: 4.876456, acc: 0.000000]\n",
            "101: [Discriminator loss: 0.394032, real acc: 0.837891, fake acc: 0.830078]  [Adverserial loss: 4.531186, acc: 0.000000]\n",
            "102: [Discriminator loss: 0.337167, real acc: 0.863281, fake acc: 0.864258]  [Adverserial loss: 4.685641, acc: 0.000000]\n",
            "103: [Discriminator loss: 0.458663, real acc: 0.841797, fake acc: 0.749023]  [Adverserial loss: 5.067892, acc: 0.000000]\n",
            "104: [Discriminator loss: 0.603981, real acc: 0.720703, fake acc: 0.703125]  [Adverserial loss: 4.218944, acc: 0.000000]\n",
            "105: [Discriminator loss: 0.346786, real acc: 0.839844, fake acc: 0.901367]  [Adverserial loss: 3.962567, acc: 0.000000]\n",
            "106: [Discriminator loss: 0.333340, real acc: 0.875000, fake acc: 0.866211]  [Adverserial loss: 4.344155, acc: 0.000000]\n",
            "107: [Discriminator loss: 0.530897, real acc: 0.796875, fake acc: 0.701172]  [Adverserial loss: 4.513512, acc: 0.000000]\n",
            "108: [Discriminator loss: 0.618116, real acc: 0.684570, fake acc: 0.694336]  [Adverserial loss: 3.858206, acc: 0.000000]\n",
            "109: [Discriminator loss: 0.417883, real acc: 0.792969, fake acc: 0.875977]  [Adverserial loss: 3.572397, acc: 0.000000]\n",
            "110: [Discriminator loss: 0.385524, real acc: 0.844727, fake acc: 0.857422]  [Adverserial loss: 3.771569, acc: 0.000000]\n",
            "111: [Discriminator loss: 0.366146, real acc: 0.833008, fake acc: 0.867188]  [Adverserial loss: 3.863642, acc: 0.000000]\n",
            "112: [Discriminator loss: 0.468290, real acc: 0.798828, fake acc: 0.780273]  [Adverserial loss: 3.906863, acc: 0.000000]\n",
            "113: [Discriminator loss: 0.479577, real acc: 0.762695, fake acc: 0.822266]  [Adverserial loss: 3.642786, acc: 0.000000]\n",
            "114: [Discriminator loss: 0.415966, real acc: 0.794922, fake acc: 0.873047]  [Adverserial loss: 3.567548, acc: 0.000000]\n",
            "115: [Discriminator loss: 0.411191, real acc: 0.798828, fake acc: 0.856445]  [Adverserial loss: 3.553186, acc: 0.000000]\n",
            "116: [Discriminator loss: 0.378711, real acc: 0.812500, fake acc: 0.909180]  [Adverserial loss: 3.469583, acc: 0.000000]\n",
            "117: [Discriminator loss: 0.444209, real acc: 0.794922, fake acc: 0.826172]  [Adverserial loss: 3.692096, acc: 0.000000]\n",
            "118: [Discriminator loss: 0.482987, real acc: 0.744141, fake acc: 0.839844]  [Adverserial loss: 3.380143, acc: 0.000000]\n",
            "119: [Discriminator loss: 0.364861, real acc: 0.820312, fake acc: 0.908203]  [Adverserial loss: 3.225369, acc: 0.000000]\n",
            "120: [Discriminator loss: 0.348995, real acc: 0.844727, fake acc: 0.884766]  [Adverserial loss: 3.529904, acc: 0.000000]\n",
            "121: [Discriminator loss: 0.405893, real acc: 0.791016, fake acc: 0.872070]  [Adverserial loss: 3.374694, acc: 0.000000]\n",
            "122: [Discriminator loss: 0.410493, real acc: 0.825195, fake acc: 0.840820]  [Adverserial loss: 3.516639, acc: 0.000000]\n",
            "123: [Discriminator loss: 0.463169, real acc: 0.755859, fake acc: 0.835938]  [Adverserial loss: 3.385655, acc: 0.000000]\n",
            "124: [Discriminator loss: 0.417386, real acc: 0.786133, fake acc: 0.866211]  [Adverserial loss: 3.241838, acc: 0.000000]\n",
            "125: [Discriminator loss: 0.393949, real acc: 0.807617, fake acc: 0.859375]  [Adverserial loss: 3.238485, acc: 0.000000]\n",
            "126: [Discriminator loss: 0.419325, real acc: 0.802734, fake acc: 0.833008]  [Adverserial loss: 3.353210, acc: 0.000000]\n",
            "127: [Discriminator loss: 0.502231, real acc: 0.743164, fake acc: 0.767578]  [Adverserial loss: 3.308241, acc: 0.000000]\n",
            "128: [Discriminator loss: 0.465878, real acc: 0.738281, fake acc: 0.830078]  [Adverserial loss: 2.986385, acc: 0.000977]\n",
            "129: [Discriminator loss: 0.413701, real acc: 0.802734, fake acc: 0.823242]  [Adverserial loss: 3.138605, acc: 0.000000]\n",
            "130: [Discriminator loss: 0.467950, real acc: 0.763672, fake acc: 0.792969]  [Adverserial loss: 3.082958, acc: 0.000000]\n",
            "131: [Discriminator loss: 0.496633, real acc: 0.751953, fake acc: 0.793945]  [Adverserial loss: 2.994689, acc: 0.000000]\n",
            "132: [Discriminator loss: 0.442854, real acc: 0.770508, fake acc: 0.822266]  [Adverserial loss: 2.943330, acc: 0.000977]\n",
            "133: [Discriminator loss: 0.427198, real acc: 0.791992, fake acc: 0.820312]  [Adverserial loss: 3.038847, acc: 0.000000]\n",
            "134: [Discriminator loss: 0.516135, real acc: 0.723633, fake acc: 0.748047]  [Adverserial loss: 2.955818, acc: 0.000000]\n",
            "135: [Discriminator loss: 0.497699, real acc: 0.718750, fake acc: 0.806641]  [Adverserial loss: 2.738618, acc: 0.000000]\n",
            "136: [Discriminator loss: 0.398811, real acc: 0.802734, fake acc: 0.846680]  [Adverserial loss: 2.877464, acc: 0.000000]\n",
            "137: [Discriminator loss: 0.512092, real acc: 0.748047, fake acc: 0.730469]  [Adverserial loss: 2.945710, acc: 0.000000]\n",
            "138: [Discriminator loss: 0.540916, real acc: 0.664062, fake acc: 0.779297]  [Adverserial loss: 2.645114, acc: 0.000000]\n",
            "139: [Discriminator loss: 0.405522, real acc: 0.789062, fake acc: 0.859375]  [Adverserial loss: 2.710817, acc: 0.000000]\n",
            "140: [Discriminator loss: 0.429206, real acc: 0.784180, fake acc: 0.810547]  [Adverserial loss: 2.906178, acc: 0.000000]\n",
            "141: [Discriminator loss: 0.573789, real acc: 0.677734, fake acc: 0.725586]  [Adverserial loss: 2.808409, acc: 0.000000]\n",
            "142: [Discriminator loss: 0.522724, real acc: 0.667969, fake acc: 0.836914]  [Adverserial loss: 2.556381, acc: 0.000000]\n",
            "143: [Discriminator loss: 0.418363, real acc: 0.813477, fake acc: 0.813477]  [Adverserial loss: 2.871481, acc: 0.000000]\n",
            "144: [Discriminator loss: 0.561033, real acc: 0.653320, fake acc: 0.762695]  [Adverserial loss: 2.603327, acc: 0.000000]\n",
            "145: [Discriminator loss: 0.512289, real acc: 0.696289, fake acc: 0.798828]  [Adverserial loss: 2.539685, acc: 0.000000]\n",
            "146: [Discriminator loss: 0.482619, real acc: 0.736328, fake acc: 0.798828]  [Adverserial loss: 2.656503, acc: 0.000000]\n",
            "147: [Discriminator loss: 0.553521, real acc: 0.657227, fake acc: 0.778320]  [Adverserial loss: 2.459204, acc: 0.000000]\n",
            "148: [Discriminator loss: 0.517560, real acc: 0.660156, fake acc: 0.808594]  [Adverserial loss: 2.333953, acc: 0.000000]\n",
            "149: [Discriminator loss: 0.524186, real acc: 0.747070, fake acc: 0.708008]  [Adverserial loss: 2.586979, acc: 0.000000]\n",
            "150: [Discriminator loss: 0.640110, real acc: 0.531250, fake acc: 0.802734]  [Adverserial loss: 2.108415, acc: 0.000977]\n",
            "151: [Discriminator loss: 0.458388, real acc: 0.772461, fake acc: 0.781250]  [Adverserial loss: 2.359588, acc: 0.000000]\n",
            "152: [Discriminator loss: 0.550497, real acc: 0.629883, fake acc: 0.775391]  [Adverserial loss: 2.245905, acc: 0.000000]\n",
            "153: [Discriminator loss: 0.503432, real acc: 0.695312, fake acc: 0.799805]  [Adverserial loss: 2.213515, acc: 0.000000]\n",
            "154: [Discriminator loss: 0.506268, real acc: 0.682617, fake acc: 0.802734]  [Adverserial loss: 2.214658, acc: 0.000000]\n",
            "155: [Discriminator loss: 0.511826, real acc: 0.711914, fake acc: 0.790039]  [Adverserial loss: 2.286375, acc: 0.000000]\n",
            "156: [Discriminator loss: 0.533510, real acc: 0.651367, fake acc: 0.801758]  [Adverserial loss: 2.163265, acc: 0.000977]\n",
            "157: [Discriminator loss: 0.616944, real acc: 0.690430, fake acc: 0.643555]  [Adverserial loss: 2.335381, acc: 0.000000]\n",
            "158: [Discriminator loss: 0.679159, real acc: 0.430664, fake acc: 0.853516]  [Adverserial loss: 1.845346, acc: 0.001953]\n",
            "159: [Discriminator loss: 0.428760, real acc: 0.784180, fake acc: 0.827148]  [Adverserial loss: 2.003537, acc: 0.001953]\n",
            "160: [Discriminator loss: 0.642642, real acc: 0.752930, fake acc: 0.555664]  [Adverserial loss: 2.438033, acc: 0.000000]\n",
            "161: [Discriminator loss: 0.784723, real acc: 0.266602, fake acc: 0.930664]  [Adverserial loss: 1.702361, acc: 0.001953]\n",
            "162: [Discriminator loss: 0.463872, real acc: 0.714844, fake acc: 0.865234]  [Adverserial loss: 1.733396, acc: 0.004883]\n",
            "163: [Discriminator loss: 0.491186, real acc: 0.754883, fake acc: 0.763672]  [Adverserial loss: 1.949709, acc: 0.001953]\n",
            "164: [Discriminator loss: 0.570735, real acc: 0.658203, fake acc: 0.744141]  [Adverserial loss: 1.998828, acc: 0.000000]\n",
            "165: [Discriminator loss: 0.586424, real acc: 0.572266, fake acc: 0.813477]  [Adverserial loss: 1.812435, acc: 0.002930]\n",
            "166: [Discriminator loss: 0.582683, real acc: 0.666016, fake acc: 0.721680]  [Adverserial loss: 1.884344, acc: 0.000000]\n",
            "167: [Discriminator loss: 0.595425, real acc: 0.583008, fake acc: 0.792969]  [Adverserial loss: 1.796560, acc: 0.000000]\n",
            "168: [Discriminator loss: 0.589408, real acc: 0.638672, fake acc: 0.709961]  [Adverserial loss: 1.850837, acc: 0.000977]\n",
            "169: [Discriminator loss: 0.597691, real acc: 0.544922, fake acc: 0.802734]  [Adverserial loss: 1.682050, acc: 0.002930]\n",
            "170: [Discriminator loss: 0.668466, real acc: 0.684570, fake acc: 0.565430]  [Adverserial loss: 1.931353, acc: 0.000000]\n",
            "171: [Discriminator loss: 0.712619, real acc: 0.325195, fake acc: 0.886719]  [Adverserial loss: 1.522840, acc: 0.005859]\n",
            "172: [Discriminator loss: 0.516381, real acc: 0.683594, fake acc: 0.814453]  [Adverserial loss: 1.572748, acc: 0.007812]\n",
            "173: [Discriminator loss: 0.726272, real acc: 0.730469, fake acc: 0.493164]  [Adverserial loss: 1.885049, acc: 0.000000]\n",
            "174: [Discriminator loss: 0.753242, real acc: 0.232422, fake acc: 0.920898]  [Adverserial loss: 1.433736, acc: 0.002930]\n",
            "175: [Discriminator loss: 0.567968, real acc: 0.607422, fake acc: 0.833008]  [Adverserial loss: 1.405673, acc: 0.005859]\n",
            "176: [Discriminator loss: 0.598824, real acc: 0.672852, fake acc: 0.671875]  [Adverserial loss: 1.553517, acc: 0.003906]\n",
            "177: [Discriminator loss: 0.631751, real acc: 0.517578, fake acc: 0.742188]  [Adverserial loss: 1.487233, acc: 0.000977]\n",
            "178: [Discriminator loss: 0.630007, real acc: 0.570312, fake acc: 0.710938]  [Adverserial loss: 1.478943, acc: 0.005859]\n",
            "179: [Discriminator loss: 0.632065, real acc: 0.546875, fake acc: 0.758789]  [Adverserial loss: 1.442840, acc: 0.004883]\n",
            "180: [Discriminator loss: 0.669871, real acc: 0.567383, fake acc: 0.647461]  [Adverserial loss: 1.462931, acc: 0.001953]\n",
            "181: [Discriminator loss: 0.659950, real acc: 0.461914, fake acc: 0.763672]  [Adverserial loss: 1.370679, acc: 0.006836]\n",
            "182: [Discriminator loss: 0.697382, real acc: 0.566406, fake acc: 0.596680]  [Adverserial loss: 1.419067, acc: 0.000977]\n",
            "183: [Discriminator loss: 0.656406, real acc: 0.427734, fake acc: 0.811523]  [Adverserial loss: 1.314769, acc: 0.012695]\n",
            "184: [Discriminator loss: 0.686867, real acc: 0.641602, fake acc: 0.594727]  [Adverserial loss: 1.408271, acc: 0.001953]\n",
            "185: [Discriminator loss: 0.666113, real acc: 0.406250, fake acc: 0.806641]  [Adverserial loss: 1.276739, acc: 0.013672]\n",
            "186: [Discriminator loss: 0.716212, real acc: 0.611328, fake acc: 0.535156]  [Adverserial loss: 1.360721, acc: 0.001953]\n",
            "187: [Discriminator loss: 0.683796, real acc: 0.367188, fake acc: 0.843750]  [Adverserial loss: 1.210084, acc: 0.005859]\n",
            "188: [Discriminator loss: 0.601794, real acc: 0.619141, fake acc: 0.737305]  [Adverserial loss: 1.248567, acc: 0.007812]\n",
            "189: [Discriminator loss: 0.751976, real acc: 0.584961, fake acc: 0.499023]  [Adverserial loss: 1.330988, acc: 0.000977]\n",
            "190: [Discriminator loss: 0.709994, real acc: 0.273438, fake acc: 0.865234]  [Adverserial loss: 1.153774, acc: 0.002930]\n",
            "191: [Discriminator loss: 0.653874, real acc: 0.515625, fake acc: 0.712891]  [Adverserial loss: 1.141519, acc: 0.018555]\n",
            "192: [Discriminator loss: 0.734016, real acc: 0.584961, fake acc: 0.503906]  [Adverserial loss: 1.208367, acc: 0.000977]\n",
            "193: [Discriminator loss: 0.684971, real acc: 0.337891, fake acc: 0.844727]  [Adverserial loss: 1.099620, acc: 0.008789]\n",
            "194: [Discriminator loss: 0.644152, real acc: 0.580078, fake acc: 0.670898]  [Adverserial loss: 1.131230, acc: 0.012695]\n",
            "195: [Discriminator loss: 0.692761, real acc: 0.508789, fake acc: 0.602539]  [Adverserial loss: 1.152541, acc: 0.006836]\n",
            "196: [Discriminator loss: 0.677303, real acc: 0.451172, fake acc: 0.696289]  [Adverserial loss: 1.120465, acc: 0.009766]\n",
            "197: [Discriminator loss: 0.685831, real acc: 0.505859, fake acc: 0.612305]  [Adverserial loss: 1.109864, acc: 0.006836]\n",
            "198: [Discriminator loss: 0.677419, real acc: 0.446289, fake acc: 0.694336]  [Adverserial loss: 1.083768, acc: 0.010742]\n",
            "199: [Discriminator loss: 0.684555, real acc: 0.524414, fake acc: 0.600586]  [Adverserial loss: 1.099679, acc: 0.008789]\n",
            "200: [Discriminator loss: 0.678277, real acc: 0.409180, fake acc: 0.727539]  [Adverserial loss: 1.061453, acc: 0.010742]\n",
            "201: [Discriminator loss: 0.685112, real acc: 0.480469, fake acc: 0.604492]  [Adverserial loss: 1.059573, acc: 0.007812]\n",
            "202: [Discriminator loss: 0.685881, real acc: 0.434570, fake acc: 0.656250]  [Adverserial loss: 1.052041, acc: 0.005859]\n",
            "203: [Discriminator loss: 0.700686, real acc: 0.421875, fake acc: 0.611328]  [Adverserial loss: 1.030070, acc: 0.003906]\n",
            "204: [Discriminator loss: 0.691402, real acc: 0.388672, fake acc: 0.679688]  [Adverserial loss: 0.990605, acc: 0.015625]\n",
            "205: [Discriminator loss: 0.706512, real acc: 0.477539, fake acc: 0.551758]  [Adverserial loss: 1.009170, acc: 0.001953]\n",
            "206: [Discriminator loss: 0.679937, real acc: 0.371094, fake acc: 0.731445]  [Adverserial loss: 0.983231, acc: 0.011719]\n",
            "207: [Discriminator loss: 0.703556, real acc: 0.474609, fake acc: 0.557617]  [Adverserial loss: 0.987799, acc: 0.009766]\n",
            "208: [Discriminator loss: 0.692406, real acc: 0.377930, fake acc: 0.648438]  [Adverserial loss: 0.961383, acc: 0.005859]\n",
            "209: [Discriminator loss: 0.685402, real acc: 0.410156, fake acc: 0.616211]  [Adverserial loss: 0.959946, acc: 0.008789]\n",
            "210: [Discriminator loss: 0.690942, real acc: 0.460938, fake acc: 0.569336]  [Adverserial loss: 0.959175, acc: 0.004883]\n",
            "211: [Discriminator loss: 0.678921, real acc: 0.401367, fake acc: 0.678711]  [Adverserial loss: 0.946417, acc: 0.006836]\n",
            "212: [Discriminator loss: 0.705593, real acc: 0.468750, fake acc: 0.552734]  [Adverserial loss: 0.945067, acc: 0.008789]\n",
            "213: [Discriminator loss: 0.686284, real acc: 0.368164, fake acc: 0.679688]  [Adverserial loss: 0.918941, acc: 0.024414]\n",
            "214: [Discriminator loss: 0.698004, real acc: 0.447266, fake acc: 0.573242]  [Adverserial loss: 0.934834, acc: 0.004883]\n",
            "215: [Discriminator loss: 0.679371, real acc: 0.351562, fake acc: 0.718750]  [Adverserial loss: 0.909656, acc: 0.014648]\n",
            "216: [Discriminator loss: 0.722637, real acc: 0.481445, fake acc: 0.488281]  [Adverserial loss: 0.921507, acc: 0.001953]\n",
            "217: [Discriminator loss: 0.693470, real acc: 0.255859, fake acc: 0.773438]  [Adverserial loss: 0.880948, acc: 0.013672]\n",
            "218: [Discriminator loss: 0.673368, real acc: 0.381836, fake acc: 0.730469]  [Adverserial loss: 0.877586, acc: 0.029297]\n",
            "219: [Discriminator loss: 0.712685, real acc: 0.460938, fake acc: 0.476562]  [Adverserial loss: 0.893430, acc: 0.005859]\n",
            "220: [Discriminator loss: 0.684763, real acc: 0.312500, fake acc: 0.751953]  [Adverserial loss: 0.868111, acc: 0.016602]\n",
            "221: [Discriminator loss: 0.674113, real acc: 0.423828, fake acc: 0.656250]  [Adverserial loss: 0.866701, acc: 0.033203]\n",
            "222: [Discriminator loss: 0.706197, real acc: 0.407227, fake acc: 0.552734]  [Adverserial loss: 0.880764, acc: 0.011719]\n",
            "223: [Discriminator loss: 0.689527, real acc: 0.293945, fake acc: 0.708984]  [Adverserial loss: 0.852214, acc: 0.024414]\n",
            "224: [Discriminator loss: 0.683324, real acc: 0.394531, fake acc: 0.630859]  [Adverserial loss: 0.855372, acc: 0.031250]\n",
            "225: [Discriminator loss: 0.697328, real acc: 0.424805, fake acc: 0.549805]  [Adverserial loss: 0.863425, acc: 0.011719]\n",
            "226: [Discriminator loss: 0.683729, real acc: 0.366211, fake acc: 0.659180]  [Adverserial loss: 0.844692, acc: 0.035156]\n",
            "227: [Discriminator loss: 0.701417, real acc: 0.394531, fake acc: 0.561523]  [Adverserial loss: 0.853036, acc: 0.022461]\n",
            "228: [Discriminator loss: 0.689878, real acc: 0.354492, fake acc: 0.643555]  [Adverserial loss: 0.836195, acc: 0.022461]\n",
            "229: [Discriminator loss: 0.694998, real acc: 0.386719, fake acc: 0.602539]  [Adverserial loss: 0.841061, acc: 0.021484]\n",
            "230: [Discriminator loss: 0.686485, real acc: 0.390625, fake acc: 0.605469]  [Adverserial loss: 0.841330, acc: 0.018555]\n",
            "231: [Discriminator loss: 0.698453, real acc: 0.348633, fake acc: 0.629883]  [Adverserial loss: 0.832712, acc: 0.020508]\n",
            "232: [Discriminator loss: 0.709589, real acc: 0.362305, fake acc: 0.521484]  [Adverserial loss: 0.834559, acc: 0.013672]\n",
            "233: [Discriminator loss: 0.692413, real acc: 0.322266, fake acc: 0.615234]  [Adverserial loss: 0.821563, acc: 0.020508]\n",
            "234: [Discriminator loss: 0.693037, real acc: 0.380859, fake acc: 0.607422]  [Adverserial loss: 0.820518, acc: 0.023438]\n",
            "235: [Discriminator loss: 0.704622, real acc: 0.384766, fake acc: 0.548828]  [Adverserial loss: 0.825798, acc: 0.009766]\n",
            "236: [Discriminator loss: 0.692027, real acc: 0.324219, fake acc: 0.629883]  [Adverserial loss: 0.811874, acc: 0.025391]\n",
            "237: [Discriminator loss: 0.697697, real acc: 0.366211, fake acc: 0.594727]  [Adverserial loss: 0.813461, acc: 0.020508]\n",
            "238: [Discriminator loss: 0.691338, real acc: 0.359375, fake acc: 0.639648]  [Adverserial loss: 0.807483, acc: 0.021484]\n",
            "239: [Discriminator loss: 0.698404, real acc: 0.381836, fake acc: 0.574219]  [Adverserial loss: 0.803979, acc: 0.031250]\n",
            "240: [Discriminator loss: 0.698602, real acc: 0.343750, fake acc: 0.560547]  [Adverserial loss: 0.796600, acc: 0.027344]\n",
            "241: [Discriminator loss: 0.704382, real acc: 0.372070, fake acc: 0.508789]  [Adverserial loss: 0.800285, acc: 0.034180]\n",
            "242: [Discriminator loss: 0.693439, real acc: 0.323242, fake acc: 0.619141]  [Adverserial loss: 0.795000, acc: 0.027344]\n",
            "243: [Discriminator loss: 0.696689, real acc: 0.367188, fake acc: 0.575195]  [Adverserial loss: 0.795220, acc: 0.019531]\n",
            "244: [Discriminator loss: 0.696065, real acc: 0.344727, fake acc: 0.569336]  [Adverserial loss: 0.791950, acc: 0.041016]\n",
            "245: [Discriminator loss: 0.697660, real acc: 0.379883, fake acc: 0.541016]  [Adverserial loss: 0.790425, acc: 0.033203]\n",
            "246: [Discriminator loss: 0.696157, real acc: 0.362305, fake acc: 0.566406]  [Adverserial loss: 0.786563, acc: 0.038086]\n",
            "247: [Discriminator loss: 0.708302, real acc: 0.352539, fake acc: 0.504883]  [Adverserial loss: 0.782982, acc: 0.037109]\n",
            "248: [Discriminator loss: 0.702463, real acc: 0.329102, fake acc: 0.506836]  [Adverserial loss: 0.779833, acc: 0.036133]\n",
            "249: [Discriminator loss: 0.706538, real acc: 0.335938, fake acc: 0.552734]  [Adverserial loss: 0.781162, acc: 0.024414]\n",
            "250: [Discriminator loss: 0.700289, real acc: 0.299805, fake acc: 0.540039]  [Adverserial loss: 0.780332, acc: 0.019531]\n",
            "251: [Discriminator loss: 0.700569, real acc: 0.338867, fake acc: 0.588867]  [Adverserial loss: 0.780613, acc: 0.024414]\n",
            "252: [Discriminator loss: 0.701307, real acc: 0.334961, fake acc: 0.594727]  [Adverserial loss: 0.777699, acc: 0.040039]\n",
            "253: [Discriminator loss: 0.697496, real acc: 0.346680, fake acc: 0.543945]  [Adverserial loss: 0.770945, acc: 0.034180]\n",
            "254: [Discriminator loss: 0.726326, real acc: 0.378906, fake acc: 0.418945]  [Adverserial loss: 0.770144, acc: 0.030273]\n",
            "255: [Discriminator loss: 0.702423, real acc: 0.280273, fake acc: 0.564453]  [Adverserial loss: 0.770995, acc: 0.024414]\n",
            "256: [Discriminator loss: 0.697465, real acc: 0.255859, fake acc: 0.663086]  [Adverserial loss: 0.766505, acc: 0.035156]\n",
            "257: [Discriminator loss: 0.705219, real acc: 0.333008, fake acc: 0.531250]  [Adverserial loss: 0.767383, acc: 0.036133]\n",
            "258: [Discriminator loss: 0.695188, real acc: 0.347656, fake acc: 0.636719]  [Adverserial loss: 0.766890, acc: 0.032227]\n",
            "259: [Discriminator loss: 0.697121, real acc: 0.373047, fake acc: 0.531250]  [Adverserial loss: 0.762682, acc: 0.041016]\n",
            "260: [Discriminator loss: 0.706163, real acc: 0.374023, fake acc: 0.498047]  [Adverserial loss: 0.765222, acc: 0.043945]\n",
            "261: [Discriminator loss: 0.700134, real acc: 0.362305, fake acc: 0.473633]  [Adverserial loss: 0.762993, acc: 0.036133]\n",
            "262: [Discriminator loss: 0.701690, real acc: 0.337891, fake acc: 0.572266]  [Adverserial loss: 0.760156, acc: 0.035156]\n",
            "263: [Discriminator loss: 0.696039, real acc: 0.375000, fake acc: 0.554688]  [Adverserial loss: 0.760521, acc: 0.042969]\n",
            "264: [Discriminator loss: 0.706914, real acc: 0.364258, fake acc: 0.491211]  [Adverserial loss: 0.756403, acc: 0.054688]\n",
            "265: [Discriminator loss: 0.699194, real acc: 0.347656, fake acc: 0.500977]  [Adverserial loss: 0.756625, acc: 0.050781]\n",
            "266: [Discriminator loss: 0.700934, real acc: 0.391602, fake acc: 0.500977]  [Adverserial loss: 0.755367, acc: 0.041992]\n",
            "267: [Discriminator loss: 0.699391, real acc: 0.367188, fake acc: 0.554688]  [Adverserial loss: 0.755856, acc: 0.048828]\n",
            "268: [Discriminator loss: 0.697662, real acc: 0.372070, fake acc: 0.514648]  [Adverserial loss: 0.753418, acc: 0.053711]\n",
            "269: [Discriminator loss: 0.702982, real acc: 0.433594, fake acc: 0.459961]  [Adverserial loss: 0.752688, acc: 0.055664]\n",
            "270: [Discriminator loss: 0.693665, real acc: 0.414062, fake acc: 0.549805]  [Adverserial loss: 0.755855, acc: 0.060547]\n",
            "271: [Discriminator loss: 0.706474, real acc: 0.400391, fake acc: 0.479492]  [Adverserial loss: 0.752438, acc: 0.055664]\n",
            "272: [Discriminator loss: 0.697773, real acc: 0.397461, fake acc: 0.469727]  [Adverserial loss: 0.747477, acc: 0.057617]\n",
            "273: [Discriminator loss: 0.695081, real acc: 0.406250, fake acc: 0.547852]  [Adverserial loss: 0.747345, acc: 0.077148]\n",
            "274: [Discriminator loss: 0.701519, real acc: 0.438477, fake acc: 0.487305]  [Adverserial loss: 0.750043, acc: 0.074219]\n",
            "275: [Discriminator loss: 0.697855, real acc: 0.402344, fake acc: 0.470703]  [Adverserial loss: 0.748026, acc: 0.046875]\n",
            "276: [Discriminator loss: 0.697005, real acc: 0.435547, fake acc: 0.526367]  [Adverserial loss: 0.749028, acc: 0.069336]\n",
            "277: [Discriminator loss: 0.697107, real acc: 0.431641, fake acc: 0.496094]  [Adverserial loss: 0.749069, acc: 0.063477]\n",
            "278: [Discriminator loss: 0.698042, real acc: 0.432617, fake acc: 0.490234]  [Adverserial loss: 0.744255, acc: 0.084961]\n",
            "279: [Discriminator loss: 0.700630, real acc: 0.409180, fake acc: 0.491211]  [Adverserial loss: 0.748029, acc: 0.082031]\n",
            "280: [Discriminator loss: 0.696641, real acc: 0.453125, fake acc: 0.442383]  [Adverserial loss: 0.743114, acc: 0.073242]\n",
            "281: [Discriminator loss: 0.702920, real acc: 0.440430, fake acc: 0.449219]  [Adverserial loss: 0.747536, acc: 0.087891]\n",
            "282: [Discriminator loss: 0.696222, real acc: 0.413086, fake acc: 0.496094]  [Adverserial loss: 0.744884, acc: 0.075195]\n",
            "283: [Discriminator loss: 0.699113, real acc: 0.418945, fake acc: 0.508789]  [Adverserial loss: 0.744156, acc: 0.091797]\n",
            "284: [Discriminator loss: 0.695727, real acc: 0.438477, fake acc: 0.485352]  [Adverserial loss: 0.740055, acc: 0.084961]\n",
            "285: [Discriminator loss: 0.699570, real acc: 0.442383, fake acc: 0.495117]  [Adverserial loss: 0.741915, acc: 0.088867]\n",
            "286: [Discriminator loss: 0.698174, real acc: 0.458984, fake acc: 0.414062]  [Adverserial loss: 0.740742, acc: 0.075195]\n",
            "287: [Discriminator loss: 0.696284, real acc: 0.413086, fake acc: 0.538086]  [Adverserial loss: 0.741696, acc: 0.084961]\n",
            "288: [Discriminator loss: 0.695214, real acc: 0.399414, fake acc: 0.545898]  [Adverserial loss: 0.741836, acc: 0.096680]\n",
            "289: [Discriminator loss: 0.695834, real acc: 0.447266, fake acc: 0.484375]  [Adverserial loss: 0.736889, acc: 0.106445]\n",
            "290: [Discriminator loss: 0.695364, real acc: 0.430664, fake acc: 0.539062]  [Adverserial loss: 0.738408, acc: 0.094727]\n",
            "291: [Discriminator loss: 0.695460, real acc: 0.441406, fake acc: 0.482422]  [Adverserial loss: 0.738688, acc: 0.103516]\n",
            "292: [Discriminator loss: 0.696561, real acc: 0.452148, fake acc: 0.503906]  [Adverserial loss: 0.740599, acc: 0.109375]\n",
            "293: [Discriminator loss: 0.701885, real acc: 0.477539, fake acc: 0.327148]  [Adverserial loss: 0.733356, acc: 0.105469]\n",
            "294: [Discriminator loss: 0.699568, real acc: 0.484375, fake acc: 0.440430]  [Adverserial loss: 0.733039, acc: 0.102539]\n",
            "295: [Discriminator loss: 0.693977, real acc: 0.464844, fake acc: 0.546875]  [Adverserial loss: 0.731348, acc: 0.131836]\n",
            "296: [Discriminator loss: 0.695577, real acc: 0.484375, fake acc: 0.470703]  [Adverserial loss: 0.730105, acc: 0.139648]\n",
            "297: [Discriminator loss: 0.694723, real acc: 0.541992, fake acc: 0.394531]  [Adverserial loss: 0.729362, acc: 0.134766]\n",
            "298: [Discriminator loss: 0.696930, real acc: 0.524414, fake acc: 0.416016]  [Adverserial loss: 0.732209, acc: 0.128906]\n",
            "299: [Discriminator loss: 0.691902, real acc: 0.525391, fake acc: 0.465820]  [Adverserial loss: 0.731875, acc: 0.117188]\n",
            "300: [Discriminator loss: 0.697613, real acc: 0.518555, fake acc: 0.407227]  [Adverserial loss: 0.729269, acc: 0.161133]\n",
            "301: [Discriminator loss: 0.689724, real acc: 0.535156, fake acc: 0.454102]  [Adverserial loss: 0.728080, acc: 0.179688]\n",
            "302: [Discriminator loss: 0.693930, real acc: 0.526367, fake acc: 0.432617]  [Adverserial loss: 0.729061, acc: 0.157227]\n",
            "303: [Discriminator loss: 0.694887, real acc: 0.482422, fake acc: 0.419922]  [Adverserial loss: 0.729438, acc: 0.139648]\n",
            "304: [Discriminator loss: 0.697239, real acc: 0.497070, fake acc: 0.403320]  [Adverserial loss: 0.724857, acc: 0.164062]\n",
            "305: [Discriminator loss: 0.700508, real acc: 0.499023, fake acc: 0.414062]  [Adverserial loss: 0.729921, acc: 0.176758]\n",
            "306: [Discriminator loss: 0.695510, real acc: 0.546875, fake acc: 0.370117]  [Adverserial loss: 0.724800, acc: 0.154297]\n",
            "307: [Discriminator loss: 0.691406, real acc: 0.559570, fake acc: 0.440430]  [Adverserial loss: 0.731176, acc: 0.137695]\n",
            "308: [Discriminator loss: 0.696939, real acc: 0.531250, fake acc: 0.435547]  [Adverserial loss: 0.725124, acc: 0.173828]\n",
            "309: [Discriminator loss: 0.694368, real acc: 0.557617, fake acc: 0.436523]  [Adverserial loss: 0.723743, acc: 0.181641]\n",
            "310: [Discriminator loss: 0.691717, real acc: 0.586914, fake acc: 0.390625]  [Adverserial loss: 0.729040, acc: 0.160156]\n",
            "311: [Discriminator loss: 0.696759, real acc: 0.527344, fake acc: 0.418945]  [Adverserial loss: 0.722527, acc: 0.185547]\n",
            "312: [Discriminator loss: 0.692808, real acc: 0.575195, fake acc: 0.410156]  [Adverserial loss: 0.726279, acc: 0.178711]\n",
            "313: [Discriminator loss: 0.697642, real acc: 0.562500, fake acc: 0.392578]  [Adverserial loss: 0.724931, acc: 0.174805]\n",
            "314: [Discriminator loss: 0.696822, real acc: 0.641602, fake acc: 0.290039]  [Adverserial loss: 0.722349, acc: 0.170898]\n",
            "315: [Discriminator loss: 0.696874, real acc: 0.567383, fake acc: 0.375000]  [Adverserial loss: 0.721389, acc: 0.208008]\n",
            "316: [Discriminator loss: 0.692153, real acc: 0.599609, fake acc: 0.398438]  [Adverserial loss: 0.724478, acc: 0.184570]\n",
            "317: [Discriminator loss: 0.693074, real acc: 0.563477, fake acc: 0.426758]  [Adverserial loss: 0.723280, acc: 0.198242]\n",
            "318: [Discriminator loss: 0.694594, real acc: 0.605469, fake acc: 0.385742]  [Adverserial loss: 0.721350, acc: 0.195312]\n",
            "319: [Discriminator loss: 0.694381, real acc: 0.605469, fake acc: 0.413086]  [Adverserial loss: 0.724521, acc: 0.193359]\n",
            "320: [Discriminator loss: 0.692165, real acc: 0.596680, fake acc: 0.369141]  [Adverserial loss: 0.720370, acc: 0.173828]\n",
            "321: [Discriminator loss: 0.697148, real acc: 0.577148, fake acc: 0.416992]  [Adverserial loss: 0.722466, acc: 0.198242]\n",
            "322: [Discriminator loss: 0.692977, real acc: 0.583984, fake acc: 0.341797]  [Adverserial loss: 0.722954, acc: 0.197266]\n",
            "323: [Discriminator loss: 0.688970, real acc: 0.611328, fake acc: 0.438477]  [Adverserial loss: 0.724442, acc: 0.187500]\n",
            "324: [Discriminator loss: 0.694220, real acc: 0.578125, fake acc: 0.413086]  [Adverserial loss: 0.723570, acc: 0.191406]\n",
            "325: [Discriminator loss: 0.690172, real acc: 0.615234, fake acc: 0.412109]  [Adverserial loss: 0.724032, acc: 0.219727]\n",
            "326: [Discriminator loss: 0.694254, real acc: 0.576172, fake acc: 0.378906]  [Adverserial loss: 0.718160, acc: 0.249023]\n",
            "327: [Discriminator loss: 0.692372, real acc: 0.610352, fake acc: 0.396484]  [Adverserial loss: 0.724223, acc: 0.217773]\n",
            "328: [Discriminator loss: 0.688913, real acc: 0.624023, fake acc: 0.453125]  [Adverserial loss: 0.726275, acc: 0.204102]\n",
            "329: [Discriminator loss: 0.691120, real acc: 0.602539, fake acc: 0.406250]  [Adverserial loss: 0.720382, acc: 0.233398]\n",
            "330: [Discriminator loss: 0.688842, real acc: 0.633789, fake acc: 0.443359]  [Adverserial loss: 0.725274, acc: 0.207031]\n",
            "331: [Discriminator loss: 0.691578, real acc: 0.584961, fake acc: 0.404297]  [Adverserial loss: 0.721940, acc: 0.214844]\n",
            "332: [Discriminator loss: 0.695796, real acc: 0.592773, fake acc: 0.427734]  [Adverserial loss: 0.722537, acc: 0.245117]\n",
            "333: [Discriminator loss: 0.692592, real acc: 0.625977, fake acc: 0.346680]  [Adverserial loss: 0.718726, acc: 0.240234]\n",
            "334: [Discriminator loss: 0.690957, real acc: 0.612305, fake acc: 0.394531]  [Adverserial loss: 0.725182, acc: 0.231445]\n",
            "335: [Discriminator loss: 0.689636, real acc: 0.655273, fake acc: 0.391602]  [Adverserial loss: 0.722335, acc: 0.208984]\n",
            "336: [Discriminator loss: 0.692095, real acc: 0.604492, fake acc: 0.435547]  [Adverserial loss: 0.722673, acc: 0.240234]\n",
            "337: [Discriminator loss: 0.689507, real acc: 0.620117, fake acc: 0.404297]  [Adverserial loss: 0.718678, acc: 0.241211]\n",
            "338: [Discriminator loss: 0.689095, real acc: 0.638672, fake acc: 0.416016]  [Adverserial loss: 0.716304, acc: 0.250977]\n",
            "339: [Discriminator loss: 0.687055, real acc: 0.628906, fake acc: 0.429688]  [Adverserial loss: 0.722182, acc: 0.238281]\n",
            "340: [Discriminator loss: 0.688432, real acc: 0.622070, fake acc: 0.436523]  [Adverserial loss: 0.715993, acc: 0.256836]\n",
            "341: [Discriminator loss: 0.689505, real acc: 0.663086, fake acc: 0.388672]  [Adverserial loss: 0.727458, acc: 0.228516]\n",
            "342: [Discriminator loss: 0.687830, real acc: 0.620117, fake acc: 0.417969]  [Adverserial loss: 0.717932, acc: 0.235352]\n",
            "343: [Discriminator loss: 0.690861, real acc: 0.610352, fake acc: 0.438477]  [Adverserial loss: 0.722740, acc: 0.247070]\n",
            "344: [Discriminator loss: 0.688967, real acc: 0.612305, fake acc: 0.384766]  [Adverserial loss: 0.716399, acc: 0.256836]\n",
            "345: [Discriminator loss: 0.684894, real acc: 0.682617, fake acc: 0.436523]  [Adverserial loss: 0.721211, acc: 0.287109]\n",
            "346: [Discriminator loss: 0.688942, real acc: 0.638672, fake acc: 0.416016]  [Adverserial loss: 0.721639, acc: 0.248047]\n",
            "347: [Discriminator loss: 0.691507, real acc: 0.620117, fake acc: 0.406250]  [Adverserial loss: 0.718332, acc: 0.291016]\n",
            "348: [Discriminator loss: 0.689502, real acc: 0.675781, fake acc: 0.364258]  [Adverserial loss: 0.721483, acc: 0.259766]\n",
            "349: [Discriminator loss: 0.688954, real acc: 0.644531, fake acc: 0.399414]  [Adverserial loss: 0.722409, acc: 0.248047]\n",
            "350: [Discriminator loss: 0.688231, real acc: 0.637695, fake acc: 0.422852]  [Adverserial loss: 0.718025, acc: 0.271484]\n",
            "351: [Discriminator loss: 0.689831, real acc: 0.640625, fake acc: 0.408203]  [Adverserial loss: 0.721753, acc: 0.254883]\n",
            "352: [Discriminator loss: 0.690840, real acc: 0.624023, fake acc: 0.437500]  [Adverserial loss: 0.713864, acc: 0.299805]\n",
            "353: [Discriminator loss: 0.689538, real acc: 0.669922, fake acc: 0.368164]  [Adverserial loss: 0.718237, acc: 0.268555]\n",
            "354: [Discriminator loss: 0.693497, real acc: 0.659180, fake acc: 0.394531]  [Adverserial loss: 0.713934, acc: 0.329102]\n",
            "355: [Discriminator loss: 0.688537, real acc: 0.682617, fake acc: 0.364258]  [Adverserial loss: 0.728447, acc: 0.223633]\n",
            "356: [Discriminator loss: 0.691158, real acc: 0.618164, fake acc: 0.382812]  [Adverserial loss: 0.716675, acc: 0.291992]\n",
            "357: [Discriminator loss: 0.688994, real acc: 0.650391, fake acc: 0.416992]  [Adverserial loss: 0.716201, acc: 0.305664]\n",
            "358: [Discriminator loss: 0.689419, real acc: 0.646484, fake acc: 0.405273]  [Adverserial loss: 0.715383, acc: 0.302734]\n",
            "359: [Discriminator loss: 0.688805, real acc: 0.677734, fake acc: 0.375977]  [Adverserial loss: 0.720423, acc: 0.280273]\n",
            "360: [Discriminator loss: 0.687273, real acc: 0.638672, fake acc: 0.392578]  [Adverserial loss: 0.716451, acc: 0.268555]\n",
            "361: [Discriminator loss: 0.690192, real acc: 0.635742, fake acc: 0.422852]  [Adverserial loss: 0.720399, acc: 0.312500]\n",
            "362: [Discriminator loss: 0.688594, real acc: 0.661133, fake acc: 0.355469]  [Adverserial loss: 0.712099, acc: 0.305664]\n",
            "363: [Discriminator loss: 0.688287, real acc: 0.663086, fake acc: 0.432617]  [Adverserial loss: 0.721143, acc: 0.307617]\n",
            "364: [Discriminator loss: 0.690782, real acc: 0.676758, fake acc: 0.330078]  [Adverserial loss: 0.715880, acc: 0.291016]\n",
            "365: [Discriminator loss: 0.685082, real acc: 0.680664, fake acc: 0.424805]  [Adverserial loss: 0.720506, acc: 0.296875]\n",
            "366: [Discriminator loss: 0.688386, real acc: 0.663086, fake acc: 0.357422]  [Adverserial loss: 0.716467, acc: 0.286133]\n",
            "367: [Discriminator loss: 0.686279, real acc: 0.675781, fake acc: 0.441406]  [Adverserial loss: 0.721181, acc: 0.270508]\n",
            "368: [Discriminator loss: 0.683490, real acc: 0.666992, fake acc: 0.433594]  [Adverserial loss: 0.721370, acc: 0.274414]\n",
            "369: [Discriminator loss: 0.686773, real acc: 0.678711, fake acc: 0.416016]  [Adverserial loss: 0.718623, acc: 0.282227]\n",
            "370: [Discriminator loss: 0.685270, real acc: 0.666992, fake acc: 0.431641]  [Adverserial loss: 0.720379, acc: 0.284180]\n",
            "371: [Discriminator loss: 0.685145, real acc: 0.691406, fake acc: 0.416016]  [Adverserial loss: 0.723927, acc: 0.254883]\n",
            "372: [Discriminator loss: 0.689999, real acc: 0.597656, fake acc: 0.445312]  [Adverserial loss: 0.715965, acc: 0.320312]\n",
            "373: [Discriminator loss: 0.689708, real acc: 0.710938, fake acc: 0.316406]  [Adverserial loss: 0.714842, acc: 0.332031]\n",
            "374: [Discriminator loss: 0.686635, real acc: 0.653320, fake acc: 0.414062]  [Adverserial loss: 0.721703, acc: 0.256836]\n",
            "375: [Discriminator loss: 0.684187, real acc: 0.659180, fake acc: 0.435547]  [Adverserial loss: 0.721473, acc: 0.276367]\n",
            "376: [Discriminator loss: 0.688785, real acc: 0.664062, fake acc: 0.441406]  [Adverserial loss: 0.715933, acc: 0.305664]\n",
            "377: [Discriminator loss: 0.686693, real acc: 0.685547, fake acc: 0.425781]  [Adverserial loss: 0.719378, acc: 0.273438]\n",
            "378: [Discriminator loss: 0.689780, real acc: 0.660156, fake acc: 0.400391]  [Adverserial loss: 0.718187, acc: 0.291016]\n",
            "379: [Discriminator loss: 0.685659, real acc: 0.673828, fake acc: 0.416992]  [Adverserial loss: 0.719672, acc: 0.289062]\n",
            "380: [Discriminator loss: 0.685556, real acc: 0.660156, fake acc: 0.447266]  [Adverserial loss: 0.722181, acc: 0.291992]\n",
            "381: [Discriminator loss: 0.690067, real acc: 0.625000, fake acc: 0.410156]  [Adverserial loss: 0.717913, acc: 0.297852]\n",
            "382: [Discriminator loss: 0.688305, real acc: 0.749023, fake acc: 0.354492]  [Adverserial loss: 0.722919, acc: 0.296875]\n",
            "383: [Discriminator loss: 0.689174, real acc: 0.661133, fake acc: 0.384766]  [Adverserial loss: 0.716292, acc: 0.305664]\n",
            "384: [Discriminator loss: 0.685980, real acc: 0.697266, fake acc: 0.398438]  [Adverserial loss: 0.722399, acc: 0.301758]\n",
            "385: [Discriminator loss: 0.683604, real acc: 0.699219, fake acc: 0.417969]  [Adverserial loss: 0.718870, acc: 0.294922]\n",
            "386: [Discriminator loss: 0.687601, real acc: 0.670898, fake acc: 0.417969]  [Adverserial loss: 0.724009, acc: 0.290039]\n",
            "387: [Discriminator loss: 0.688317, real acc: 0.668945, fake acc: 0.375000]  [Adverserial loss: 0.718818, acc: 0.289062]\n",
            "388: [Discriminator loss: 0.689861, real acc: 0.703125, fake acc: 0.402344]  [Adverserial loss: 0.725581, acc: 0.285156]\n",
            "389: [Discriminator loss: 0.684164, real acc: 0.673828, fake acc: 0.425781]  [Adverserial loss: 0.723113, acc: 0.287109]\n",
            "390: [Discriminator loss: 0.686211, real acc: 0.662109, fake acc: 0.437500]  [Adverserial loss: 0.719818, acc: 0.290039]\n",
            "391: [Discriminator loss: 0.688051, real acc: 0.666992, fake acc: 0.447266]  [Adverserial loss: 0.722545, acc: 0.328125]\n",
            "392: [Discriminator loss: 0.685680, real acc: 0.705078, fake acc: 0.383789]  [Adverserial loss: 0.717893, acc: 0.351562]\n",
            "393: [Discriminator loss: 0.690102, real acc: 0.707031, fake acc: 0.370117]  [Adverserial loss: 0.721106, acc: 0.303711]\n",
            "394: [Discriminator loss: 0.685637, real acc: 0.667969, fake acc: 0.408203]  [Adverserial loss: 0.724489, acc: 0.275391]\n",
            "395: [Discriminator loss: 0.685227, real acc: 0.641602, fake acc: 0.427734]  [Adverserial loss: 0.722990, acc: 0.259766]\n",
            "396: [Discriminator loss: 0.686615, real acc: 0.660156, fake acc: 0.422852]  [Adverserial loss: 0.721419, acc: 0.299805]\n",
            "397: [Discriminator loss: 0.696157, real acc: 0.676758, fake acc: 0.284180]  [Adverserial loss: 0.716880, acc: 0.310547]\n",
            "398: [Discriminator loss: 0.681982, real acc: 0.683594, fake acc: 0.455078]  [Adverserial loss: 0.730497, acc: 0.246094]\n",
            "399: [Discriminator loss: 0.685420, real acc: 0.676758, fake acc: 0.463867]  [Adverserial loss: 0.717940, acc: 0.338867]\n",
            "400: [Discriminator loss: 0.685573, real acc: 0.681641, fake acc: 0.443359]  [Adverserial loss: 0.725167, acc: 0.298828]\n",
            "401: [Discriminator loss: 0.686832, real acc: 0.692383, fake acc: 0.396484]  [Adverserial loss: 0.719551, acc: 0.298828]\n",
            "402: [Discriminator loss: 0.684814, real acc: 0.695312, fake acc: 0.426758]  [Adverserial loss: 0.728090, acc: 0.269531]\n",
            "403: [Discriminator loss: 0.683592, real acc: 0.675781, fake acc: 0.453125]  [Adverserial loss: 0.726182, acc: 0.257812]\n",
            "404: [Discriminator loss: 0.682692, real acc: 0.695312, fake acc: 0.467773]  [Adverserial loss: 0.726257, acc: 0.290039]\n",
            "405: [Discriminator loss: 0.683111, real acc: 0.705078, fake acc: 0.393555]  [Adverserial loss: 0.725260, acc: 0.300781]\n",
            "406: [Discriminator loss: 0.684629, real acc: 0.668945, fake acc: 0.422852]  [Adverserial loss: 0.722837, acc: 0.307617]\n",
            "407: [Discriminator loss: 0.688229, real acc: 0.688477, fake acc: 0.383789]  [Adverserial loss: 0.720419, acc: 0.318359]\n",
            "408: [Discriminator loss: 0.685081, real acc: 0.702148, fake acc: 0.449219]  [Adverserial loss: 0.727688, acc: 0.301758]\n",
            "409: [Discriminator loss: 0.684151, real acc: 0.667969, fake acc: 0.400391]  [Adverserial loss: 0.716881, acc: 0.332031]\n",
            "410: [Discriminator loss: 0.679858, real acc: 0.723633, fake acc: 0.429688]  [Adverserial loss: 0.725697, acc: 0.310547]\n",
            "411: [Discriminator loss: 0.679178, real acc: 0.706055, fake acc: 0.433594]  [Adverserial loss: 0.725158, acc: 0.314453]\n",
            "412: [Discriminator loss: 0.680307, real acc: 0.708984, fake acc: 0.456055]  [Adverserial loss: 0.730086, acc: 0.274414]\n",
            "413: [Discriminator loss: 0.685791, real acc: 0.667969, fake acc: 0.392578]  [Adverserial loss: 0.727802, acc: 0.299805]\n",
            "414: [Discriminator loss: 0.681735, real acc: 0.705078, fake acc: 0.429688]  [Adverserial loss: 0.733783, acc: 0.264648]\n",
            "415: [Discriminator loss: 0.681327, real acc: 0.701172, fake acc: 0.442383]  [Adverserial loss: 0.724771, acc: 0.304688]\n",
            "416: [Discriminator loss: 0.682673, real acc: 0.696289, fake acc: 0.427734]  [Adverserial loss: 0.730682, acc: 0.278320]\n",
            "417: [Discriminator loss: 0.681582, real acc: 0.694336, fake acc: 0.452148]  [Adverserial loss: 0.723207, acc: 0.318359]\n",
            "418: [Discriminator loss: 0.681070, real acc: 0.694336, fake acc: 0.428711]  [Adverserial loss: 0.733072, acc: 0.298828]\n",
            "419: [Discriminator loss: 0.685368, real acc: 0.697266, fake acc: 0.425781]  [Adverserial loss: 0.726148, acc: 0.309570]\n",
            "420: [Discriminator loss: 0.693374, real acc: 0.715820, fake acc: 0.322266]  [Adverserial loss: 0.729845, acc: 0.293945]\n",
            "421: [Discriminator loss: 0.685286, real acc: 0.691406, fake acc: 0.416016]  [Adverserial loss: 0.726087, acc: 0.312500]\n",
            "422: [Discriminator loss: 0.682240, real acc: 0.672852, fake acc: 0.466797]  [Adverserial loss: 0.733339, acc: 0.278320]\n",
            "423: [Discriminator loss: 0.673518, real acc: 0.711914, fake acc: 0.482422]  [Adverserial loss: 0.732006, acc: 0.275391]\n",
            "424: [Discriminator loss: 0.681320, real acc: 0.671875, fake acc: 0.451172]  [Adverserial loss: 0.726456, acc: 0.308594]\n",
            "425: [Discriminator loss: 0.684642, real acc: 0.736328, fake acc: 0.383789]  [Adverserial loss: 0.724722, acc: 0.317383]\n",
            "426: [Discriminator loss: 0.685496, real acc: 0.679688, fake acc: 0.394531]  [Adverserial loss: 0.724151, acc: 0.295898]\n",
            "427: [Discriminator loss: 0.678121, real acc: 0.707031, fake acc: 0.467773]  [Adverserial loss: 0.737892, acc: 0.314453]\n",
            "428: [Discriminator loss: 0.681055, real acc: 0.665039, fake acc: 0.456055]  [Adverserial loss: 0.725112, acc: 0.307617]\n",
            "429: [Discriminator loss: 0.679426, real acc: 0.717773, fake acc: 0.468750]  [Adverserial loss: 0.733446, acc: 0.307617]\n",
            "430: [Discriminator loss: 0.682522, real acc: 0.665039, fake acc: 0.433594]  [Adverserial loss: 0.731309, acc: 0.277344]\n",
            "431: [Discriminator loss: 0.685096, real acc: 0.691406, fake acc: 0.422852]  [Adverserial loss: 0.730734, acc: 0.300781]\n",
            "432: [Discriminator loss: 0.680922, real acc: 0.693359, fake acc: 0.478516]  [Adverserial loss: 0.737888, acc: 0.277344]\n",
            "433: [Discriminator loss: 0.679210, real acc: 0.710938, fake acc: 0.431641]  [Adverserial loss: 0.734611, acc: 0.289062]\n",
            "434: [Discriminator loss: 0.683746, real acc: 0.679688, fake acc: 0.434570]  [Adverserial loss: 0.727152, acc: 0.318359]\n",
            "435: [Discriminator loss: 0.675042, real acc: 0.714844, fake acc: 0.464844]  [Adverserial loss: 0.744619, acc: 0.250000]\n",
            "436: [Discriminator loss: 0.682298, real acc: 0.669922, fake acc: 0.448242]  [Adverserial loss: 0.737108, acc: 0.254883]\n",
            "437: [Discriminator loss: 0.678621, real acc: 0.676758, fake acc: 0.488281]  [Adverserial loss: 0.742071, acc: 0.257812]\n",
            "438: [Discriminator loss: 0.694906, real acc: 0.697266, fake acc: 0.333008]  [Adverserial loss: 0.727215, acc: 0.289062]\n",
            "439: [Discriminator loss: 0.678506, real acc: 0.710938, fake acc: 0.492188]  [Adverserial loss: 0.738973, acc: 0.286133]\n",
            "440: [Discriminator loss: 0.673576, real acc: 0.712891, fake acc: 0.483398]  [Adverserial loss: 0.735766, acc: 0.276367]\n",
            "441: [Discriminator loss: 0.682088, real acc: 0.710938, fake acc: 0.426758]  [Adverserial loss: 0.732501, acc: 0.289062]\n",
            "442: [Discriminator loss: 0.681817, real acc: 0.733398, fake acc: 0.441406]  [Adverserial loss: 0.738240, acc: 0.297852]\n",
            "443: [Discriminator loss: 0.680733, real acc: 0.704102, fake acc: 0.444336]  [Adverserial loss: 0.738471, acc: 0.294922]\n",
            "444: [Discriminator loss: 0.675251, real acc: 0.695312, fake acc: 0.448242]  [Adverserial loss: 0.738466, acc: 0.277344]\n",
            "445: [Discriminator loss: 0.682481, real acc: 0.708008, fake acc: 0.435547]  [Adverserial loss: 0.738052, acc: 0.300781]\n",
            "446: [Discriminator loss: 0.677745, real acc: 0.690430, fake acc: 0.462891]  [Adverserial loss: 0.740231, acc: 0.290039]\n",
            "447: [Discriminator loss: 0.674736, real acc: 0.708008, fake acc: 0.498047]  [Adverserial loss: 0.740358, acc: 0.272461]\n",
            "448: [Discriminator loss: 0.677516, real acc: 0.675781, fake acc: 0.500977]  [Adverserial loss: 0.750353, acc: 0.264648]\n",
            "449: [Discriminator loss: 0.674180, real acc: 0.671875, fake acc: 0.517578]  [Adverserial loss: 0.739288, acc: 0.264648]\n",
            "450: [Discriminator loss: 0.677198, real acc: 0.653320, fake acc: 0.489258]  [Adverserial loss: 0.750983, acc: 0.249023]\n",
            "451: [Discriminator loss: 0.682325, real acc: 0.669922, fake acc: 0.467773]  [Adverserial loss: 0.740078, acc: 0.270508]\n",
            "452: [Discriminator loss: 0.678505, real acc: 0.666016, fake acc: 0.496094]  [Adverserial loss: 0.748280, acc: 0.290039]\n",
            "453: [Discriminator loss: 0.678135, real acc: 0.657227, fake acc: 0.468750]  [Adverserial loss: 0.741435, acc: 0.261719]\n",
            "454: [Discriminator loss: 0.675078, real acc: 0.665039, fake acc: 0.518555]  [Adverserial loss: 0.750209, acc: 0.233398]\n",
            "455: [Discriminator loss: 0.677078, real acc: 0.648438, fake acc: 0.490234]  [Adverserial loss: 0.749843, acc: 0.263672]\n",
            "456: [Discriminator loss: 0.684734, real acc: 0.674805, fake acc: 0.407227]  [Adverserial loss: 0.737325, acc: 0.289062]\n",
            "457: [Discriminator loss: 0.676533, real acc: 0.701172, fake acc: 0.461914]  [Adverserial loss: 0.753571, acc: 0.245117]\n",
            "458: [Discriminator loss: 0.681518, real acc: 0.636719, fake acc: 0.479492]  [Adverserial loss: 0.736239, acc: 0.301758]\n",
            "459: [Discriminator loss: 0.677803, real acc: 0.701172, fake acc: 0.484375]  [Adverserial loss: 0.749312, acc: 0.271484]\n",
            "460: [Discriminator loss: 0.679904, real acc: 0.688477, fake acc: 0.419922]  [Adverserial loss: 0.734872, acc: 0.296875]\n",
            "461: [Discriminator loss: 0.678811, real acc: 0.692383, fake acc: 0.447266]  [Adverserial loss: 0.753171, acc: 0.267578]\n",
            "462: [Discriminator loss: 0.676503, real acc: 0.672852, fake acc: 0.456055]  [Adverserial loss: 0.745764, acc: 0.262695]\n",
            "463: [Discriminator loss: 0.675174, real acc: 0.701172, fake acc: 0.483398]  [Adverserial loss: 0.743531, acc: 0.293945]\n",
            "464: [Discriminator loss: 0.677691, real acc: 0.698242, fake acc: 0.467773]  [Adverserial loss: 0.741805, acc: 0.280273]\n",
            "465: [Discriminator loss: 0.678516, real acc: 0.661133, fake acc: 0.474609]  [Adverserial loss: 0.741276, acc: 0.296875]\n",
            "466: [Discriminator loss: 0.682467, real acc: 0.666016, fake acc: 0.461914]  [Adverserial loss: 0.743056, acc: 0.283203]\n",
            "467: [Discriminator loss: 0.679840, real acc: 0.671875, fake acc: 0.479492]  [Adverserial loss: 0.753451, acc: 0.287109]\n",
            "468: [Discriminator loss: 0.677619, real acc: 0.670898, fake acc: 0.477539]  [Adverserial loss: 0.748762, acc: 0.265625]\n",
            "469: [Discriminator loss: 0.685107, real acc: 0.664062, fake acc: 0.506836]  [Adverserial loss: 0.747343, acc: 0.284180]\n",
            "470: [Discriminator loss: 0.678401, real acc: 0.698242, fake acc: 0.462891]  [Adverserial loss: 0.745345, acc: 0.297852]\n",
            "471: [Discriminator loss: 0.677462, real acc: 0.680664, fake acc: 0.487305]  [Adverserial loss: 0.753952, acc: 0.274414]\n",
            "472: [Discriminator loss: 0.676403, real acc: 0.647461, fake acc: 0.507812]  [Adverserial loss: 0.752242, acc: 0.267578]\n",
            "473: [Discriminator loss: 0.679564, real acc: 0.649414, fake acc: 0.496094]  [Adverserial loss: 0.751759, acc: 0.246094]\n",
            "474: [Discriminator loss: 0.677262, real acc: 0.658203, fake acc: 0.500977]  [Adverserial loss: 0.753038, acc: 0.280273]\n",
            "475: [Discriminator loss: 0.681619, real acc: 0.673828, fake acc: 0.477539]  [Adverserial loss: 0.749897, acc: 0.266602]\n",
            "476: [Discriminator loss: 0.677918, real acc: 0.654297, fake acc: 0.491211]  [Adverserial loss: 0.755882, acc: 0.266602]\n",
            "477: [Discriminator loss: 0.677110, real acc: 0.671875, fake acc: 0.506836]  [Adverserial loss: 0.746938, acc: 0.259766]\n",
            "478: [Discriminator loss: 0.681717, real acc: 0.680664, fake acc: 0.487305]  [Adverserial loss: 0.746584, acc: 0.291016]\n",
            "479: [Discriminator loss: 0.672981, real acc: 0.698242, fake acc: 0.496094]  [Adverserial loss: 0.758239, acc: 0.271484]\n",
            "480: [Discriminator loss: 0.676708, real acc: 0.675781, fake acc: 0.498047]  [Adverserial loss: 0.750028, acc: 0.282227]\n",
            "481: [Discriminator loss: 0.678228, real acc: 0.641602, fake acc: 0.496094]  [Adverserial loss: 0.748012, acc: 0.257812]\n",
            "482: [Discriminator loss: 0.674801, real acc: 0.682617, fake acc: 0.521484]  [Adverserial loss: 0.760902, acc: 0.260742]\n",
            "483: [Discriminator loss: 0.681415, real acc: 0.669922, fake acc: 0.419922]  [Adverserial loss: 0.749903, acc: 0.286133]\n",
            "484: [Discriminator loss: 0.674674, real acc: 0.680664, fake acc: 0.523438]  [Adverserial loss: 0.762393, acc: 0.262695]\n",
            "485: [Discriminator loss: 0.676048, real acc: 0.643555, fake acc: 0.515625]  [Adverserial loss: 0.752938, acc: 0.233398]\n",
            "486: [Discriminator loss: 0.679837, real acc: 0.676758, fake acc: 0.488281]  [Adverserial loss: 0.745332, acc: 0.310547]\n",
            "487: [Discriminator loss: 0.673086, real acc: 0.679688, fake acc: 0.513672]  [Adverserial loss: 0.759763, acc: 0.249023]\n",
            "488: [Discriminator loss: 0.677445, real acc: 0.655273, fake acc: 0.494141]  [Adverserial loss: 0.758108, acc: 0.252930]\n",
            "489: [Discriminator loss: 0.676412, real acc: 0.666992, fake acc: 0.479492]  [Adverserial loss: 0.745253, acc: 0.282227]\n",
            "490: [Discriminator loss: 0.676688, real acc: 0.659180, fake acc: 0.519531]  [Adverserial loss: 0.752854, acc: 0.286133]\n",
            "491: [Discriminator loss: 0.675055, real acc: 0.676758, fake acc: 0.481445]  [Adverserial loss: 0.759563, acc: 0.258789]\n",
            "492: [Discriminator loss: 0.676374, real acc: 0.663086, fake acc: 0.500000]  [Adverserial loss: 0.759335, acc: 0.247070]\n",
            "493: [Discriminator loss: 0.674504, real acc: 0.685547, fake acc: 0.465820]  [Adverserial loss: 0.754892, acc: 0.273438]\n",
            "494: [Discriminator loss: 0.677723, real acc: 0.657227, fake acc: 0.502930]  [Adverserial loss: 0.761277, acc: 0.283203]\n",
            "495: [Discriminator loss: 0.678614, real acc: 0.671875, fake acc: 0.466797]  [Adverserial loss: 0.759439, acc: 0.259766]\n",
            "496: [Discriminator loss: 0.673705, real acc: 0.653320, fake acc: 0.516602]  [Adverserial loss: 0.767278, acc: 0.238281]\n",
            "497: [Discriminator loss: 0.673948, real acc: 0.654297, fake acc: 0.522461]  [Adverserial loss: 0.750940, acc: 0.279297]\n",
            "498: [Discriminator loss: 0.678821, real acc: 0.663086, fake acc: 0.485352]  [Adverserial loss: 0.761507, acc: 0.277344]\n",
            "499: [Discriminator loss: 0.674431, real acc: 0.642578, fake acc: 0.516602]  [Adverserial loss: 0.768564, acc: 0.219727]\n",
            "500: [Discriminator loss: 0.674238, real acc: 0.650391, fake acc: 0.538086]  [Adverserial loss: 0.771658, acc: 0.242188]\n",
            "501: [Discriminator loss: 0.681574, real acc: 0.618164, fake acc: 0.499023]  [Adverserial loss: 0.766534, acc: 0.220703]\n",
            "502: [Discriminator loss: 0.675696, real acc: 0.647461, fake acc: 0.548828]  [Adverserial loss: 0.774381, acc: 0.249023]\n",
            "503: [Discriminator loss: 0.681382, real acc: 0.636719, fake acc: 0.498047]  [Adverserial loss: 0.759771, acc: 0.259766]\n",
            "504: [Discriminator loss: 0.676509, real acc: 0.649414, fake acc: 0.521484]  [Adverserial loss: 0.758448, acc: 0.285156]\n",
            "505: [Discriminator loss: 0.673354, real acc: 0.678711, fake acc: 0.485352]  [Adverserial loss: 0.761108, acc: 0.266602]\n",
            "506: [Discriminator loss: 0.674112, real acc: 0.647461, fake acc: 0.528320]  [Adverserial loss: 0.762954, acc: 0.245117]\n",
            "507: [Discriminator loss: 0.674180, real acc: 0.639648, fake acc: 0.547852]  [Adverserial loss: 0.755559, acc: 0.276367]\n",
            "508: [Discriminator loss: 0.676769, real acc: 0.635742, fake acc: 0.524414]  [Adverserial loss: 0.753646, acc: 0.272461]\n",
            "509: [Discriminator loss: 0.674113, real acc: 0.648438, fake acc: 0.512695]  [Adverserial loss: 0.761544, acc: 0.279297]\n",
            "510: [Discriminator loss: 0.674844, real acc: 0.666992, fake acc: 0.507812]  [Adverserial loss: 0.759952, acc: 0.244141]\n",
            "511: [Discriminator loss: 0.677746, real acc: 0.685547, fake acc: 0.497070]  [Adverserial loss: 0.762390, acc: 0.273438]\n",
            "512: [Discriminator loss: 0.670408, real acc: 0.636719, fake acc: 0.551758]  [Adverserial loss: 0.765333, acc: 0.255859]\n",
            "513: [Discriminator loss: 0.674978, real acc: 0.651367, fake acc: 0.542969]  [Adverserial loss: 0.764541, acc: 0.266602]\n",
            "514: [Discriminator loss: 0.676647, real acc: 0.627930, fake acc: 0.523438]  [Adverserial loss: 0.768022, acc: 0.259766]\n",
            "515: [Discriminator loss: 0.674439, real acc: 0.694336, fake acc: 0.519531]  [Adverserial loss: 0.770306, acc: 0.262695]\n",
            "516: [Discriminator loss: 0.673947, real acc: 0.625977, fake acc: 0.537109]  [Adverserial loss: 0.768093, acc: 0.235352]\n",
            "517: [Discriminator loss: 0.677290, real acc: 0.623047, fake acc: 0.508789]  [Adverserial loss: 0.772079, acc: 0.237305]\n",
            "518: [Discriminator loss: 0.673987, real acc: 0.633789, fake acc: 0.501953]  [Adverserial loss: 0.769792, acc: 0.249023]\n",
            "519: [Discriminator loss: 0.671747, real acc: 0.668945, fake acc: 0.521484]  [Adverserial loss: 0.765503, acc: 0.281250]\n",
            "520: [Discriminator loss: 0.674177, real acc: 0.657227, fake acc: 0.521484]  [Adverserial loss: 0.767208, acc: 0.240234]\n",
            "521: [Discriminator loss: 0.674850, real acc: 0.656250, fake acc: 0.540039]  [Adverserial loss: 0.772746, acc: 0.269531]\n",
            "522: [Discriminator loss: 0.682584, real acc: 0.663086, fake acc: 0.477539]  [Adverserial loss: 0.756821, acc: 0.269531]\n",
            "523: [Discriminator loss: 0.678586, real acc: 0.676758, fake acc: 0.488281]  [Adverserial loss: 0.769086, acc: 0.269531]\n",
            "524: [Discriminator loss: 0.671607, real acc: 0.633789, fake acc: 0.568359]  [Adverserial loss: 0.767659, acc: 0.232422]\n",
            "525: [Discriminator loss: 0.678389, real acc: 0.647461, fake acc: 0.534180]  [Adverserial loss: 0.767019, acc: 0.295898]\n",
            "526: [Discriminator loss: 0.677844, real acc: 0.666992, fake acc: 0.501953]  [Adverserial loss: 0.767676, acc: 0.247070]\n",
            "527: [Discriminator loss: 0.673510, real acc: 0.650391, fake acc: 0.526367]  [Adverserial loss: 0.769701, acc: 0.268555]\n",
            "528: [Discriminator loss: 0.670284, real acc: 0.680664, fake acc: 0.537109]  [Adverserial loss: 0.778980, acc: 0.234375]\n",
            "529: [Discriminator loss: 0.670986, real acc: 0.648438, fake acc: 0.552734]  [Adverserial loss: 0.773185, acc: 0.250000]\n",
            "530: [Discriminator loss: 0.667674, real acc: 0.654297, fake acc: 0.541992]  [Adverserial loss: 0.775012, acc: 0.229492]\n",
            "531: [Discriminator loss: 0.676710, real acc: 0.627930, fake acc: 0.528320]  [Adverserial loss: 0.777446, acc: 0.252930]\n",
            "532: [Discriminator loss: 0.668548, real acc: 0.660156, fake acc: 0.520508]  [Adverserial loss: 0.780073, acc: 0.212891]\n",
            "533: [Discriminator loss: 0.671458, real acc: 0.639648, fake acc: 0.574219]  [Adverserial loss: 0.780466, acc: 0.249023]\n",
            "534: [Discriminator loss: 0.676136, real acc: 0.633789, fake acc: 0.509766]  [Adverserial loss: 0.774256, acc: 0.244141]\n",
            "535: [Discriminator loss: 0.674761, real acc: 0.652344, fake acc: 0.531250]  [Adverserial loss: 0.780033, acc: 0.253906]\n",
            "536: [Discriminator loss: 0.670443, real acc: 0.655273, fake acc: 0.540039]  [Adverserial loss: 0.779708, acc: 0.230469]\n",
            "537: [Discriminator loss: 0.679188, real acc: 0.639648, fake acc: 0.530273]  [Adverserial loss: 0.763314, acc: 0.298828]\n",
            "538: [Discriminator loss: 0.664523, real acc: 0.691406, fake acc: 0.553711]  [Adverserial loss: 0.777361, acc: 0.256836]\n",
            "539: [Discriminator loss: 0.671231, real acc: 0.640625, fake acc: 0.544922]  [Adverserial loss: 0.780409, acc: 0.240234]\n",
            "540: [Discriminator loss: 0.672310, real acc: 0.661133, fake acc: 0.535156]  [Adverserial loss: 0.765448, acc: 0.269531]\n",
            "541: [Discriminator loss: 0.664998, real acc: 0.678711, fake acc: 0.544922]  [Adverserial loss: 0.785152, acc: 0.239258]\n",
            "542: [Discriminator loss: 0.671790, real acc: 0.611328, fake acc: 0.540039]  [Adverserial loss: 0.782863, acc: 0.249023]\n",
            "543: [Discriminator loss: 0.674754, real acc: 0.640625, fake acc: 0.526367]  [Adverserial loss: 0.783475, acc: 0.213867]\n",
            "544: [Discriminator loss: 0.667167, real acc: 0.629883, fake acc: 0.553711]  [Adverserial loss: 0.786894, acc: 0.245117]\n",
            "545: [Discriminator loss: 0.673317, real acc: 0.648438, fake acc: 0.558594]  [Adverserial loss: 0.777180, acc: 0.260742]\n",
            "546: [Discriminator loss: 0.669534, real acc: 0.644531, fake acc: 0.542969]  [Adverserial loss: 0.787714, acc: 0.239258]\n",
            "547: [Discriminator loss: 0.664516, real acc: 0.662109, fake acc: 0.558594]  [Adverserial loss: 0.785295, acc: 0.245117]\n",
            "548: [Discriminator loss: 0.675084, real acc: 0.641602, fake acc: 0.521484]  [Adverserial loss: 0.774447, acc: 0.286133]\n",
            "549: [Discriminator loss: 0.671621, real acc: 0.664062, fake acc: 0.556641]  [Adverserial loss: 0.787928, acc: 0.245117]\n",
            "550: [Discriminator loss: 0.673279, real acc: 0.658203, fake acc: 0.500977]  [Adverserial loss: 0.776990, acc: 0.236328]\n",
            "551: [Discriminator loss: 0.669411, real acc: 0.660156, fake acc: 0.542969]  [Adverserial loss: 0.782126, acc: 0.246094]\n",
            "552: [Discriminator loss: 0.668997, real acc: 0.647461, fake acc: 0.552734]  [Adverserial loss: 0.778126, acc: 0.225586]\n",
            "553: [Discriminator loss: 0.669434, real acc: 0.653320, fake acc: 0.540039]  [Adverserial loss: 0.792409, acc: 0.244141]\n",
            "554: [Discriminator loss: 0.675314, real acc: 0.649414, fake acc: 0.535156]  [Adverserial loss: 0.778823, acc: 0.259766]\n",
            "555: [Discriminator loss: 0.672372, real acc: 0.654297, fake acc: 0.534180]  [Adverserial loss: 0.786065, acc: 0.251953]\n",
            "556: [Discriminator loss: 0.673183, real acc: 0.656250, fake acc: 0.537109]  [Adverserial loss: 0.781759, acc: 0.235352]\n",
            "557: [Discriminator loss: 0.667625, real acc: 0.660156, fake acc: 0.537109]  [Adverserial loss: 0.797955, acc: 0.228516]\n",
            "558: [Discriminator loss: 0.679573, real acc: 0.633789, fake acc: 0.511719]  [Adverserial loss: 0.781058, acc: 0.245117]\n",
            "559: [Discriminator loss: 0.676145, real acc: 0.597656, fake acc: 0.557617]  [Adverserial loss: 0.800245, acc: 0.189453]\n",
            "560: [Discriminator loss: 0.667350, real acc: 0.638672, fake acc: 0.543945]  [Adverserial loss: 0.782115, acc: 0.254883]\n",
            "561: [Discriminator loss: 0.672367, real acc: 0.666016, fake acc: 0.522461]  [Adverserial loss: 0.781553, acc: 0.258789]\n",
            "562: [Discriminator loss: 0.671837, real acc: 0.637695, fake acc: 0.559570]  [Adverserial loss: 0.794500, acc: 0.199219]\n",
            "563: [Discriminator loss: 0.667824, real acc: 0.654297, fake acc: 0.539062]  [Adverserial loss: 0.787717, acc: 0.255859]\n",
            "564: [Discriminator loss: 0.665274, real acc: 0.658203, fake acc: 0.546875]  [Adverserial loss: 0.805934, acc: 0.191406]\n",
            "565: [Discriminator loss: 0.669281, real acc: 0.628906, fake acc: 0.578125]  [Adverserial loss: 0.800441, acc: 0.228516]\n",
            "566: [Discriminator loss: 0.681750, real acc: 0.648438, fake acc: 0.514648]  [Adverserial loss: 0.790217, acc: 0.223633]\n",
            "567: [Discriminator loss: 0.675821, real acc: 0.643555, fake acc: 0.530273]  [Adverserial loss: 0.780878, acc: 0.256836]\n",
            "568: [Discriminator loss: 0.668259, real acc: 0.648438, fake acc: 0.525391]  [Adverserial loss: 0.778754, acc: 0.258789]\n",
            "569: [Discriminator loss: 0.669388, real acc: 0.643555, fake acc: 0.518555]  [Adverserial loss: 0.786435, acc: 0.259766]\n",
            "570: [Discriminator loss: 0.663392, real acc: 0.647461, fake acc: 0.559570]  [Adverserial loss: 0.797565, acc: 0.241211]\n",
            "571: [Discriminator loss: 0.671001, real acc: 0.621094, fake acc: 0.566406]  [Adverserial loss: 0.802615, acc: 0.227539]\n",
            "572: [Discriminator loss: 0.667553, real acc: 0.641602, fake acc: 0.574219]  [Adverserial loss: 0.789950, acc: 0.230469]\n",
            "573: [Discriminator loss: 0.674025, real acc: 0.644531, fake acc: 0.555664]  [Adverserial loss: 0.800573, acc: 0.212891]\n",
            "574: [Discriminator loss: 0.673571, real acc: 0.650391, fake acc: 0.530273]  [Adverserial loss: 0.789037, acc: 0.236328]\n",
            "575: [Discriminator loss: 0.669556, real acc: 0.658203, fake acc: 0.514648]  [Adverserial loss: 0.789989, acc: 0.231445]\n",
            "576: [Discriminator loss: 0.669827, real acc: 0.642578, fake acc: 0.536133]  [Adverserial loss: 0.802238, acc: 0.218750]\n",
            "577: [Discriminator loss: 0.669054, real acc: 0.626953, fake acc: 0.553711]  [Adverserial loss: 0.806791, acc: 0.209961]\n",
            "578: [Discriminator loss: 0.673471, real acc: 0.623047, fake acc: 0.562500]  [Adverserial loss: 0.788985, acc: 0.253906]\n",
            "579: [Discriminator loss: 0.668902, real acc: 0.661133, fake acc: 0.524414]  [Adverserial loss: 0.806757, acc: 0.211914]\n",
            "580: [Discriminator loss: 0.676033, real acc: 0.601562, fake acc: 0.575195]  [Adverserial loss: 0.798977, acc: 0.225586]\n",
            "581: [Discriminator loss: 0.657318, real acc: 0.670898, fake acc: 0.613281]  [Adverserial loss: 0.799356, acc: 0.239258]\n",
            "582: [Discriminator loss: 0.666532, real acc: 0.671875, fake acc: 0.536133]  [Adverserial loss: 0.795112, acc: 0.225586]\n",
            "583: [Discriminator loss: 0.670227, real acc: 0.648438, fake acc: 0.537109]  [Adverserial loss: 0.798148, acc: 0.229492]\n",
            "584: [Discriminator loss: 0.672847, real acc: 0.625977, fake acc: 0.559570]  [Adverserial loss: 0.803059, acc: 0.243164]\n",
            "585: [Discriminator loss: 0.662207, real acc: 0.686523, fake acc: 0.552734]  [Adverserial loss: 0.801386, acc: 0.225586]\n",
            "586: [Discriminator loss: 0.663235, real acc: 0.649414, fake acc: 0.570312]  [Adverserial loss: 0.799962, acc: 0.234375]\n",
            "587: [Discriminator loss: 0.674243, real acc: 0.642578, fake acc: 0.525391]  [Adverserial loss: 0.790232, acc: 0.251953]\n",
            "588: [Discriminator loss: 0.662183, real acc: 0.678711, fake acc: 0.578125]  [Adverserial loss: 0.802525, acc: 0.238281]\n",
            "589: [Discriminator loss: 0.665740, real acc: 0.636719, fake acc: 0.559570]  [Adverserial loss: 0.801454, acc: 0.228516]\n",
            "590: [Discriminator loss: 0.671919, real acc: 0.615234, fake acc: 0.572266]  [Adverserial loss: 0.794832, acc: 0.240234]\n",
            "591: [Discriminator loss: 0.667949, real acc: 0.645508, fake acc: 0.545898]  [Adverserial loss: 0.808302, acc: 0.228516]\n",
            "592: [Discriminator loss: 0.670770, real acc: 0.637695, fake acc: 0.577148]  [Adverserial loss: 0.816169, acc: 0.216797]\n",
            "593: [Discriminator loss: 0.666809, real acc: 0.617188, fake acc: 0.542969]  [Adverserial loss: 0.806949, acc: 0.215820]\n",
            "594: [Discriminator loss: 0.677720, real acc: 0.619141, fake acc: 0.557617]  [Adverserial loss: 0.805425, acc: 0.234375]\n",
            "595: [Discriminator loss: 0.672436, real acc: 0.641602, fake acc: 0.540039]  [Adverserial loss: 0.814560, acc: 0.223633]\n",
            "596: [Discriminator loss: 0.668535, real acc: 0.622070, fake acc: 0.583008]  [Adverserial loss: 0.800855, acc: 0.241211]\n",
            "597: [Discriminator loss: 0.674830, real acc: 0.643555, fake acc: 0.513672]  [Adverserial loss: 0.800537, acc: 0.256836]\n",
            "598: [Discriminator loss: 0.667015, real acc: 0.643555, fake acc: 0.581055]  [Adverserial loss: 0.808036, acc: 0.233398]\n",
            "599: [Discriminator loss: 0.663559, real acc: 0.644531, fake acc: 0.570312]  [Adverserial loss: 0.804449, acc: 0.220703]\n",
            "600: [Discriminator loss: 0.659679, real acc: 0.652344, fake acc: 0.607422]  [Adverserial loss: 0.803438, acc: 0.227539]\n",
            "601: [Discriminator loss: 0.661665, real acc: 0.615234, fake acc: 0.577148]  [Adverserial loss: 0.807622, acc: 0.235352]\n",
            "602: [Discriminator loss: 0.680889, real acc: 0.635742, fake acc: 0.503906]  [Adverserial loss: 0.798761, acc: 0.235352]\n",
            "603: [Discriminator loss: 0.665964, real acc: 0.672852, fake acc: 0.533203]  [Adverserial loss: 0.809154, acc: 0.227539]\n",
            "604: [Discriminator loss: 0.669792, real acc: 0.639648, fake acc: 0.536133]  [Adverserial loss: 0.810290, acc: 0.219727]\n",
            "605: [Discriminator loss: 0.666110, real acc: 0.654297, fake acc: 0.560547]  [Adverserial loss: 0.804794, acc: 0.240234]\n",
            "606: [Discriminator loss: 0.672273, real acc: 0.636719, fake acc: 0.548828]  [Adverserial loss: 0.803295, acc: 0.246094]\n",
            "607: [Discriminator loss: 0.676972, real acc: 0.652344, fake acc: 0.509766]  [Adverserial loss: 0.796032, acc: 0.254883]\n",
            "608: [Discriminator loss: 0.670863, real acc: 0.663086, fake acc: 0.527344]  [Adverserial loss: 0.816358, acc: 0.222656]\n",
            "609: [Discriminator loss: 0.667229, real acc: 0.637695, fake acc: 0.547852]  [Adverserial loss: 0.812702, acc: 0.221680]\n",
            "610: [Discriminator loss: 0.665522, real acc: 0.615234, fake acc: 0.583008]  [Adverserial loss: 0.825302, acc: 0.199219]\n",
            "611: [Discriminator loss: 0.675261, real acc: 0.608398, fake acc: 0.592773]  [Adverserial loss: 0.819635, acc: 0.219727]\n",
            "612: [Discriminator loss: 0.671026, real acc: 0.616211, fake acc: 0.556641]  [Adverserial loss: 0.831639, acc: 0.176758]\n",
            "613: [Discriminator loss: 0.670027, real acc: 0.594727, fake acc: 0.605469]  [Adverserial loss: 0.817766, acc: 0.210938]\n",
            "614: [Discriminator loss: 0.665276, real acc: 0.620117, fake acc: 0.583008]  [Adverserial loss: 0.821248, acc: 0.208984]\n",
            "615: [Discriminator loss: 0.664640, real acc: 0.632812, fake acc: 0.601562]  [Adverserial loss: 0.811972, acc: 0.220703]\n",
            "616: [Discriminator loss: 0.669723, real acc: 0.615234, fake acc: 0.541016]  [Adverserial loss: 0.806559, acc: 0.250977]\n",
            "617: [Discriminator loss: 0.665030, real acc: 0.645508, fake acc: 0.577148]  [Adverserial loss: 0.825767, acc: 0.215820]\n",
            "618: [Discriminator loss: 0.665672, real acc: 0.631836, fake acc: 0.575195]  [Adverserial loss: 0.817375, acc: 0.224609]\n",
            "619: [Discriminator loss: 0.669534, real acc: 0.617188, fake acc: 0.591797]  [Adverserial loss: 0.814640, acc: 0.232422]\n",
            "620: [Discriminator loss: 0.662248, real acc: 0.661133, fake acc: 0.589844]  [Adverserial loss: 0.828335, acc: 0.187500]\n",
            "621: [Discriminator loss: 0.667397, real acc: 0.623047, fake acc: 0.580078]  [Adverserial loss: 0.816470, acc: 0.236328]\n",
            "622: [Discriminator loss: 0.666796, real acc: 0.626953, fake acc: 0.575195]  [Adverserial loss: 0.832219, acc: 0.199219]\n",
            "623: [Discriminator loss: 0.676012, real acc: 0.586914, fake acc: 0.590820]  [Adverserial loss: 0.814390, acc: 0.241211]\n",
            "624: [Discriminator loss: 0.662404, real acc: 0.617188, fake acc: 0.588867]  [Adverserial loss: 0.822244, acc: 0.206055]\n",
            "625: [Discriminator loss: 0.670982, real acc: 0.637695, fake acc: 0.567383]  [Adverserial loss: 0.800052, acc: 0.237305]\n",
            "626: [Discriminator loss: 0.661696, real acc: 0.636719, fake acc: 0.582031]  [Adverserial loss: 0.820978, acc: 0.216797]\n",
            "627: [Discriminator loss: 0.666540, real acc: 0.605469, fake acc: 0.577148]  [Adverserial loss: 0.818304, acc: 0.227539]\n",
            "628: [Discriminator loss: 0.668411, real acc: 0.618164, fake acc: 0.572266]  [Adverserial loss: 0.826280, acc: 0.204102]\n",
            "629: [Discriminator loss: 0.665529, real acc: 0.633789, fake acc: 0.603516]  [Adverserial loss: 0.826041, acc: 0.201172]\n",
            "630: [Discriminator loss: 0.680411, real acc: 0.598633, fake acc: 0.550781]  [Adverserial loss: 0.822412, acc: 0.211914]\n",
            "631: [Discriminator loss: 0.669058, real acc: 0.597656, fake acc: 0.570312]  [Adverserial loss: 0.825395, acc: 0.201172]\n",
            "632: [Discriminator loss: 0.671151, real acc: 0.606445, fake acc: 0.572266]  [Adverserial loss: 0.823056, acc: 0.211914]\n",
            "633: [Discriminator loss: 0.675807, real acc: 0.618164, fake acc: 0.540039]  [Adverserial loss: 0.802819, acc: 0.243164]\n",
            "634: [Discriminator loss: 0.673868, real acc: 0.631836, fake acc: 0.540039]  [Adverserial loss: 0.804013, acc: 0.238281]\n",
            "635: [Discriminator loss: 0.662661, real acc: 0.638672, fake acc: 0.577148]  [Adverserial loss: 0.822694, acc: 0.215820]\n",
            "636: [Discriminator loss: 0.673223, real acc: 0.592773, fake acc: 0.544922]  [Adverserial loss: 0.814830, acc: 0.209961]\n",
            "637: [Discriminator loss: 0.670986, real acc: 0.614258, fake acc: 0.584961]  [Adverserial loss: 0.823262, acc: 0.218750]\n",
            "638: [Discriminator loss: 0.660578, real acc: 0.599609, fake acc: 0.612305]  [Adverserial loss: 0.816995, acc: 0.217773]\n",
            "639: [Discriminator loss: 0.666195, real acc: 0.615234, fake acc: 0.576172]  [Adverserial loss: 0.808854, acc: 0.254883]\n",
            "640: [Discriminator loss: 0.669419, real acc: 0.643555, fake acc: 0.539062]  [Adverserial loss: 0.810913, acc: 0.234375]\n",
            "641: [Discriminator loss: 0.660041, real acc: 0.618164, fake acc: 0.624023]  [Adverserial loss: 0.820543, acc: 0.208008]\n",
            "642: [Discriminator loss: 0.673317, real acc: 0.605469, fake acc: 0.561523]  [Adverserial loss: 0.812502, acc: 0.232422]\n",
            "643: [Discriminator loss: 0.663285, real acc: 0.614258, fake acc: 0.621094]  [Adverserial loss: 0.819966, acc: 0.229492]\n",
            "644: [Discriminator loss: 0.660886, real acc: 0.639648, fake acc: 0.596680]  [Adverserial loss: 0.812657, acc: 0.223633]\n",
            "645: [Discriminator loss: 0.663195, real acc: 0.641602, fake acc: 0.569336]  [Adverserial loss: 0.838490, acc: 0.183594]\n",
            "646: [Discriminator loss: 0.663268, real acc: 0.605469, fake acc: 0.633789]  [Adverserial loss: 0.835588, acc: 0.218750]\n",
            "647: [Discriminator loss: 0.664443, real acc: 0.625000, fake acc: 0.569336]  [Adverserial loss: 0.838009, acc: 0.198242]\n",
            "648: [Discriminator loss: 0.673796, real acc: 0.598633, fake acc: 0.566406]  [Adverserial loss: 0.819298, acc: 0.224609]\n",
            "649: [Discriminator loss: 0.663159, real acc: 0.632812, fake acc: 0.607422]  [Adverserial loss: 0.824036, acc: 0.208984]\n",
            "650: [Discriminator loss: 0.663328, real acc: 0.631836, fake acc: 0.581055]  [Adverserial loss: 0.810520, acc: 0.237305]\n",
            "651: [Discriminator loss: 0.661237, real acc: 0.639648, fake acc: 0.591797]  [Adverserial loss: 0.831182, acc: 0.209961]\n",
            "652: [Discriminator loss: 0.662033, real acc: 0.654297, fake acc: 0.554688]  [Adverserial loss: 0.817477, acc: 0.242188]\n",
            "653: [Discriminator loss: 0.663294, real acc: 0.665039, fake acc: 0.565430]  [Adverserial loss: 0.831619, acc: 0.226562]\n",
            "654: [Discriminator loss: 0.661880, real acc: 0.622070, fake acc: 0.584961]  [Adverserial loss: 0.831488, acc: 0.207031]\n",
            "655: [Discriminator loss: 0.670500, real acc: 0.605469, fake acc: 0.552734]  [Adverserial loss: 0.828835, acc: 0.208008]\n",
            "656: [Discriminator loss: 0.663093, real acc: 0.599609, fake acc: 0.618164]  [Adverserial loss: 0.858341, acc: 0.174805]\n",
            "657: [Discriminator loss: 0.669637, real acc: 0.552734, fake acc: 0.629883]  [Adverserial loss: 0.813844, acc: 0.232422]\n",
            "658: [Discriminator loss: 0.662360, real acc: 0.615234, fake acc: 0.595703]  [Adverserial loss: 0.825978, acc: 0.218750]\n",
            "659: [Discriminator loss: 0.673607, real acc: 0.647461, fake acc: 0.526367]  [Adverserial loss: 0.810362, acc: 0.231445]\n",
            "660: [Discriminator loss: 0.665847, real acc: 0.633789, fake acc: 0.581055]  [Adverserial loss: 0.827050, acc: 0.223633]\n",
            "661: [Discriminator loss: 0.662189, real acc: 0.614258, fake acc: 0.588867]  [Adverserial loss: 0.832623, acc: 0.215820]\n",
            "662: [Discriminator loss: 0.672654, real acc: 0.608398, fake acc: 0.555664]  [Adverserial loss: 0.806779, acc: 0.250977]\n",
            "663: [Discriminator loss: 0.666368, real acc: 0.642578, fake acc: 0.559570]  [Adverserial loss: 0.826825, acc: 0.218750]\n",
            "664: [Discriminator loss: 0.661584, real acc: 0.604492, fake acc: 0.603516]  [Adverserial loss: 0.828426, acc: 0.209961]\n",
            "665: [Discriminator loss: 0.673650, real acc: 0.628906, fake acc: 0.591797]  [Adverserial loss: 0.816651, acc: 0.230469]\n",
            "666: [Discriminator loss: 0.662840, real acc: 0.643555, fake acc: 0.579102]  [Adverserial loss: 0.847615, acc: 0.177734]\n",
            "667: [Discriminator loss: 0.668449, real acc: 0.598633, fake acc: 0.619141]  [Adverserial loss: 0.824777, acc: 0.215820]\n",
            "668: [Discriminator loss: 0.668847, real acc: 0.606445, fake acc: 0.593750]  [Adverserial loss: 0.828178, acc: 0.234375]\n",
            "669: [Discriminator loss: 0.678623, real acc: 0.589844, fake acc: 0.568359]  [Adverserial loss: 0.811844, acc: 0.228516]\n",
            "670: [Discriminator loss: 0.672799, real acc: 0.612305, fake acc: 0.535156]  [Adverserial loss: 0.823972, acc: 0.219727]\n",
            "671: [Discriminator loss: 0.670540, real acc: 0.612305, fake acc: 0.585938]  [Adverserial loss: 0.827868, acc: 0.214844]\n",
            "672: [Discriminator loss: 0.655059, real acc: 0.625000, fake acc: 0.604492]  [Adverserial loss: 0.821791, acc: 0.221680]\n",
            "673: [Discriminator loss: 0.669991, real acc: 0.604492, fake acc: 0.563477]  [Adverserial loss: 0.812978, acc: 0.218750]\n",
            "674: [Discriminator loss: 0.658628, real acc: 0.641602, fake acc: 0.579102]  [Adverserial loss: 0.848929, acc: 0.207031]\n",
            "675: [Discriminator loss: 0.661069, real acc: 0.587891, fake acc: 0.607422]  [Adverserial loss: 0.827765, acc: 0.215820]\n",
            "676: [Discriminator loss: 0.659911, real acc: 0.628906, fake acc: 0.603516]  [Adverserial loss: 0.837611, acc: 0.200195]\n",
            "677: [Discriminator loss: 0.663843, real acc: 0.635742, fake acc: 0.589844]  [Adverserial loss: 0.822840, acc: 0.256836]\n",
            "678: [Discriminator loss: 0.671102, real acc: 0.623047, fake acc: 0.541016]  [Adverserial loss: 0.836847, acc: 0.208008]\n",
            "679: [Discriminator loss: 0.661618, real acc: 0.646484, fake acc: 0.617188]  [Adverserial loss: 0.820178, acc: 0.235352]\n",
            "680: [Discriminator loss: 0.658164, real acc: 0.635742, fake acc: 0.583008]  [Adverserial loss: 0.855920, acc: 0.196289]\n",
            "681: [Discriminator loss: 0.668868, real acc: 0.580078, fake acc: 0.631836]  [Adverserial loss: 0.841975, acc: 0.199219]\n",
            "682: [Discriminator loss: 0.663329, real acc: 0.584961, fake acc: 0.591797]  [Adverserial loss: 0.854583, acc: 0.199219]\n",
            "683: [Discriminator loss: 0.678306, real acc: 0.588867, fake acc: 0.592773]  [Adverserial loss: 0.822256, acc: 0.239258]\n",
            "684: [Discriminator loss: 0.661833, real acc: 0.631836, fake acc: 0.598633]  [Adverserial loss: 0.839156, acc: 0.207031]\n",
            "685: [Discriminator loss: 0.659269, real acc: 0.614258, fake acc: 0.618164]  [Adverserial loss: 0.831453, acc: 0.208008]\n",
            "686: [Discriminator loss: 0.657301, real acc: 0.643555, fake acc: 0.588867]  [Adverserial loss: 0.837179, acc: 0.213867]\n",
            "687: [Discriminator loss: 0.660388, real acc: 0.604492, fake acc: 0.619141]  [Adverserial loss: 0.845218, acc: 0.190430]\n",
            "688: [Discriminator loss: 0.672601, real acc: 0.583008, fake acc: 0.568359]  [Adverserial loss: 0.807328, acc: 0.256836]\n",
            "689: [Discriminator loss: 0.664811, real acc: 0.641602, fake acc: 0.570312]  [Adverserial loss: 0.851062, acc: 0.206055]\n",
            "690: [Discriminator loss: 0.676082, real acc: 0.663086, fake acc: 0.495117]  [Adverserial loss: 0.810258, acc: 0.266602]\n",
            "691: [Discriminator loss: 0.669949, real acc: 0.654297, fake acc: 0.528320]  [Adverserial loss: 0.825826, acc: 0.222656]\n",
            "692: [Discriminator loss: 0.664392, real acc: 0.639648, fake acc: 0.547852]  [Adverserial loss: 0.819293, acc: 0.224609]\n",
            "693: [Discriminator loss: 0.663683, real acc: 0.628906, fake acc: 0.584961]  [Adverserial loss: 0.817139, acc: 0.219727]\n",
            "694: [Discriminator loss: 0.670554, real acc: 0.640625, fake acc: 0.575195]  [Adverserial loss: 0.822383, acc: 0.234375]\n",
            "695: [Discriminator loss: 0.661688, real acc: 0.651367, fake acc: 0.604492]  [Adverserial loss: 0.838490, acc: 0.203125]\n",
            "696: [Discriminator loss: 0.657795, real acc: 0.603516, fake acc: 0.637695]  [Adverserial loss: 0.818673, acc: 0.234375]\n",
            "697: [Discriminator loss: 0.668037, real acc: 0.591797, fake acc: 0.607422]  [Adverserial loss: 0.872696, acc: 0.166992]\n",
            "698: [Discriminator loss: 0.673657, real acc: 0.570312, fake acc: 0.612305]  [Adverserial loss: 0.834237, acc: 0.208984]\n",
            "699: [Discriminator loss: 0.665863, real acc: 0.574219, fake acc: 0.599609]  [Adverserial loss: 0.844245, acc: 0.202148]\n",
            "700: [Discriminator loss: 0.672690, real acc: 0.599609, fake acc: 0.579102]  [Adverserial loss: 0.828126, acc: 0.227539]\n",
            "701: [Discriminator loss: 0.661808, real acc: 0.614258, fake acc: 0.588867]  [Adverserial loss: 0.845727, acc: 0.185547]\n",
            "702: [Discriminator loss: 0.661028, real acc: 0.605469, fake acc: 0.606445]  [Adverserial loss: 0.831232, acc: 0.214844]\n",
            "703: [Discriminator loss: 0.665447, real acc: 0.613281, fake acc: 0.583984]  [Adverserial loss: 0.833826, acc: 0.226562]\n",
            "704: [Discriminator loss: 0.663970, real acc: 0.606445, fake acc: 0.594727]  [Adverserial loss: 0.825000, acc: 0.233398]\n",
            "705: [Discriminator loss: 0.659851, real acc: 0.641602, fake acc: 0.591797]  [Adverserial loss: 0.836581, acc: 0.220703]\n",
            "706: [Discriminator loss: 0.654859, real acc: 0.616211, fake acc: 0.597656]  [Adverserial loss: 0.855738, acc: 0.195312]\n",
            "707: [Discriminator loss: 0.671575, real acc: 0.617188, fake acc: 0.584961]  [Adverserial loss: 0.826230, acc: 0.225586]\n",
            "708: [Discriminator loss: 0.665808, real acc: 0.617188, fake acc: 0.594727]  [Adverserial loss: 0.844814, acc: 0.222656]\n",
            "709: [Discriminator loss: 0.668274, real acc: 0.598633, fake acc: 0.590820]  [Adverserial loss: 0.826647, acc: 0.210938]\n",
            "710: [Discriminator loss: 0.663375, real acc: 0.601562, fake acc: 0.623047]  [Adverserial loss: 0.851944, acc: 0.178711]\n",
            "711: [Discriminator loss: 0.665440, real acc: 0.612305, fake acc: 0.604492]  [Adverserial loss: 0.829197, acc: 0.209961]\n",
            "712: [Discriminator loss: 0.665419, real acc: 0.625977, fake acc: 0.556641]  [Adverserial loss: 0.850327, acc: 0.202148]\n",
            "713: [Discriminator loss: 0.669232, real acc: 0.597656, fake acc: 0.599609]  [Adverserial loss: 0.836152, acc: 0.219727]\n",
            "714: [Discriminator loss: 0.669581, real acc: 0.590820, fake acc: 0.584961]  [Adverserial loss: 0.841502, acc: 0.214844]\n",
            "715: [Discriminator loss: 0.669429, real acc: 0.599609, fake acc: 0.598633]  [Adverserial loss: 0.827066, acc: 0.249023]\n",
            "716: [Discriminator loss: 0.663468, real acc: 0.628906, fake acc: 0.593750]  [Adverserial loss: 0.837343, acc: 0.225586]\n",
            "717: [Discriminator loss: 0.659949, real acc: 0.625977, fake acc: 0.600586]  [Adverserial loss: 0.838886, acc: 0.228516]\n",
            "718: [Discriminator loss: 0.659837, real acc: 0.640625, fake acc: 0.580078]  [Adverserial loss: 0.839108, acc: 0.220703]\n",
            "719: [Discriminator loss: 0.665786, real acc: 0.596680, fake acc: 0.585938]  [Adverserial loss: 0.834065, acc: 0.219727]\n",
            "720: [Discriminator loss: 0.672913, real acc: 0.595703, fake acc: 0.601562]  [Adverserial loss: 0.841214, acc: 0.230469]\n",
            "721: [Discriminator loss: 0.675358, real acc: 0.593750, fake acc: 0.578125]  [Adverserial loss: 0.848732, acc: 0.218750]\n",
            "722: [Discriminator loss: 0.663012, real acc: 0.603516, fake acc: 0.625977]  [Adverserial loss: 0.838691, acc: 0.208984]\n",
            "723: [Discriminator loss: 0.655294, real acc: 0.592773, fake acc: 0.632812]  [Adverserial loss: 0.885442, acc: 0.177734]\n",
            "724: [Discriminator loss: 0.679114, real acc: 0.576172, fake acc: 0.580078]  [Adverserial loss: 0.823577, acc: 0.241211]\n",
            "725: [Discriminator loss: 0.660423, real acc: 0.644531, fake acc: 0.591797]  [Adverserial loss: 0.858293, acc: 0.197266]\n",
            "726: [Discriminator loss: 0.667708, real acc: 0.565430, fake acc: 0.597656]  [Adverserial loss: 0.823505, acc: 0.224609]\n",
            "727: [Discriminator loss: 0.656114, real acc: 0.635742, fake acc: 0.587891]  [Adverserial loss: 0.829294, acc: 0.218750]\n",
            "728: [Discriminator loss: 0.662026, real acc: 0.602539, fake acc: 0.602539]  [Adverserial loss: 0.823889, acc: 0.221680]\n",
            "729: [Discriminator loss: 0.669061, real acc: 0.599609, fake acc: 0.578125]  [Adverserial loss: 0.832353, acc: 0.208984]\n",
            "730: [Discriminator loss: 0.668012, real acc: 0.610352, fake acc: 0.579102]  [Adverserial loss: 0.841713, acc: 0.218750]\n",
            "731: [Discriminator loss: 0.669287, real acc: 0.608398, fake acc: 0.601562]  [Adverserial loss: 0.836859, acc: 0.244141]\n",
            "732: [Discriminator loss: 0.667835, real acc: 0.617188, fake acc: 0.550781]  [Adverserial loss: 0.837828, acc: 0.213867]\n",
            "733: [Discriminator loss: 0.669776, real acc: 0.609375, fake acc: 0.603516]  [Adverserial loss: 0.823573, acc: 0.247070]\n",
            "734: [Discriminator loss: 0.664500, real acc: 0.600586, fake acc: 0.587891]  [Adverserial loss: 0.849784, acc: 0.195312]\n",
            "735: [Discriminator loss: 0.671093, real acc: 0.581055, fake acc: 0.579102]  [Adverserial loss: 0.838122, acc: 0.197266]\n",
            "736: [Discriminator loss: 0.666389, real acc: 0.603516, fake acc: 0.612305]  [Adverserial loss: 0.903104, acc: 0.147461]\n",
            "737: [Discriminator loss: 0.667408, real acc: 0.582031, fake acc: 0.646484]  [Adverserial loss: 0.839026, acc: 0.193359]\n",
            "738: [Discriminator loss: 0.660730, real acc: 0.587891, fake acc: 0.641602]  [Adverserial loss: 0.835847, acc: 0.225586]\n",
            "739: [Discriminator loss: 0.669671, real acc: 0.626953, fake acc: 0.584961]  [Adverserial loss: 0.838784, acc: 0.208984]\n",
            "740: [Discriminator loss: 0.664018, real acc: 0.619141, fake acc: 0.571289]  [Adverserial loss: 0.840477, acc: 0.221680]\n",
            "741: [Discriminator loss: 0.661669, real acc: 0.580078, fake acc: 0.619141]  [Adverserial loss: 0.824240, acc: 0.232422]\n",
            "742: [Discriminator loss: 0.666508, real acc: 0.620117, fake acc: 0.595703]  [Adverserial loss: 0.842506, acc: 0.208984]\n",
            "743: [Discriminator loss: 0.661564, real acc: 0.593750, fake acc: 0.614258]  [Adverserial loss: 0.830824, acc: 0.239258]\n",
            "744: [Discriminator loss: 0.659203, real acc: 0.632812, fake acc: 0.602539]  [Adverserial loss: 0.848368, acc: 0.205078]\n",
            "745: [Discriminator loss: 0.656843, real acc: 0.634766, fake acc: 0.612305]  [Adverserial loss: 0.844103, acc: 0.206055]\n",
            "746: [Discriminator loss: 0.662307, real acc: 0.604492, fake acc: 0.594727]  [Adverserial loss: 0.857765, acc: 0.205078]\n",
            "747: [Discriminator loss: 0.667639, real acc: 0.601562, fake acc: 0.590820]  [Adverserial loss: 0.851324, acc: 0.207031]\n",
            "748: [Discriminator loss: 0.668580, real acc: 0.584961, fake acc: 0.612305]  [Adverserial loss: 0.890559, acc: 0.166992]\n",
            "749: [Discriminator loss: 0.679400, real acc: 0.572266, fake acc: 0.577148]  [Adverserial loss: 0.849181, acc: 0.206055]\n",
            "750: [Discriminator loss: 0.656500, real acc: 0.614258, fake acc: 0.630859]  [Adverserial loss: 0.840884, acc: 0.212891]\n",
            "751: [Discriminator loss: 0.673007, real acc: 0.593750, fake acc: 0.599609]  [Adverserial loss: 0.816406, acc: 0.244141]\n",
            "752: [Discriminator loss: 0.667307, real acc: 0.629883, fake acc: 0.600586]  [Adverserial loss: 0.853115, acc: 0.217773]\n",
            "753: [Discriminator loss: 0.656601, real acc: 0.633789, fake acc: 0.625000]  [Adverserial loss: 0.835260, acc: 0.209961]\n",
            "754: [Discriminator loss: 0.665587, real acc: 0.582031, fake acc: 0.606445]  [Adverserial loss: 0.854956, acc: 0.203125]\n",
            "755: [Discriminator loss: 0.664158, real acc: 0.622070, fake acc: 0.607422]  [Adverserial loss: 0.833744, acc: 0.227539]\n",
            "756: [Discriminator loss: 0.658594, real acc: 0.604492, fake acc: 0.611328]  [Adverserial loss: 0.876778, acc: 0.179688]\n",
            "757: [Discriminator loss: 0.667834, real acc: 0.586914, fake acc: 0.616211]  [Adverserial loss: 0.841778, acc: 0.211914]\n",
            "758: [Discriminator loss: 0.668865, real acc: 0.574219, fake acc: 0.599609]  [Adverserial loss: 0.874334, acc: 0.189453]\n",
            "759: [Discriminator loss: 0.669785, real acc: 0.575195, fake acc: 0.623047]  [Adverserial loss: 0.824211, acc: 0.222656]\n",
            "760: [Discriminator loss: 0.664555, real acc: 0.587891, fake acc: 0.586914]  [Adverserial loss: 0.839396, acc: 0.230469]\n",
            "761: [Discriminator loss: 0.662609, real acc: 0.597656, fake acc: 0.612305]  [Adverserial loss: 0.836971, acc: 0.211914]\n",
            "762: [Discriminator loss: 0.662726, real acc: 0.601562, fake acc: 0.604492]  [Adverserial loss: 0.848233, acc: 0.207031]\n",
            "763: [Discriminator loss: 0.665298, real acc: 0.595703, fake acc: 0.598633]  [Adverserial loss: 0.841756, acc: 0.216797]\n",
            "764: [Discriminator loss: 0.663882, real acc: 0.613281, fake acc: 0.581055]  [Adverserial loss: 0.845805, acc: 0.208984]\n",
            "765: [Discriminator loss: 0.658558, real acc: 0.646484, fake acc: 0.608398]  [Adverserial loss: 0.845994, acc: 0.220703]\n",
            "766: [Discriminator loss: 0.674770, real acc: 0.572266, fake acc: 0.566406]  [Adverserial loss: 0.844326, acc: 0.192383]\n",
            "767: [Discriminator loss: 0.668372, real acc: 0.599609, fake acc: 0.597656]  [Adverserial loss: 0.841875, acc: 0.212891]\n",
            "768: [Discriminator loss: 0.670859, real acc: 0.575195, fake acc: 0.569336]  [Adverserial loss: 0.846343, acc: 0.190430]\n",
            "769: [Discriminator loss: 0.664896, real acc: 0.604492, fake acc: 0.647461]  [Adverserial loss: 0.844890, acc: 0.212891]\n",
            "770: [Discriminator loss: 0.667682, real acc: 0.615234, fake acc: 0.587891]  [Adverserial loss: 0.873173, acc: 0.180664]\n",
            "771: [Discriminator loss: 0.670635, real acc: 0.577148, fake acc: 0.598633]  [Adverserial loss: 0.846394, acc: 0.188477]\n",
            "772: [Discriminator loss: 0.665608, real acc: 0.549805, fake acc: 0.638672]  [Adverserial loss: 0.855816, acc: 0.209961]\n",
            "773: [Discriminator loss: 0.659671, real acc: 0.642578, fake acc: 0.588867]  [Adverserial loss: 0.837035, acc: 0.225586]\n",
            "774: [Discriminator loss: 0.663817, real acc: 0.629883, fake acc: 0.602539]  [Adverserial loss: 0.843722, acc: 0.207031]\n",
            "775: [Discriminator loss: 0.672385, real acc: 0.560547, fake acc: 0.614258]  [Adverserial loss: 0.828068, acc: 0.225586]\n",
            "776: [Discriminator loss: 0.671469, real acc: 0.556641, fake acc: 0.625000]  [Adverserial loss: 0.845137, acc: 0.220703]\n",
            "777: [Discriminator loss: 0.667723, real acc: 0.602539, fake acc: 0.583984]  [Adverserial loss: 0.832737, acc: 0.208984]\n",
            "778: [Discriminator loss: 0.655237, real acc: 0.613281, fake acc: 0.637695]  [Adverserial loss: 0.852786, acc: 0.209961]\n",
            "779: [Discriminator loss: 0.664969, real acc: 0.587891, fake acc: 0.595703]  [Adverserial loss: 0.835665, acc: 0.221680]\n",
            "780: [Discriminator loss: 0.657710, real acc: 0.602539, fake acc: 0.623047]  [Adverserial loss: 0.850811, acc: 0.220703]\n",
            "781: [Discriminator loss: 0.671344, real acc: 0.617188, fake acc: 0.546875]  [Adverserial loss: 0.830762, acc: 0.231445]\n",
            "782: [Discriminator loss: 0.667402, real acc: 0.627930, fake acc: 0.571289]  [Adverserial loss: 0.893370, acc: 0.177734]\n",
            "783: [Discriminator loss: 0.669891, real acc: 0.583984, fake acc: 0.611328]  [Adverserial loss: 0.842218, acc: 0.201172]\n",
            "784: [Discriminator loss: 0.667093, real acc: 0.565430, fake acc: 0.623047]  [Adverserial loss: 0.869619, acc: 0.191406]\n",
            "785: [Discriminator loss: 0.672992, real acc: 0.589844, fake acc: 0.596680]  [Adverserial loss: 0.841465, acc: 0.222656]\n",
            "786: [Discriminator loss: 0.662759, real acc: 0.594727, fake acc: 0.591797]  [Adverserial loss: 0.844416, acc: 0.207031]\n",
            "787: [Discriminator loss: 0.666229, real acc: 0.601562, fake acc: 0.597656]  [Adverserial loss: 0.839281, acc: 0.227539]\n",
            "788: [Discriminator loss: 0.664520, real acc: 0.614258, fake acc: 0.578125]  [Adverserial loss: 0.855088, acc: 0.221680]\n",
            "789: [Discriminator loss: 0.666865, real acc: 0.600586, fake acc: 0.621094]  [Adverserial loss: 0.844782, acc: 0.216797]\n",
            "790: [Discriminator loss: 0.662832, real acc: 0.608398, fake acc: 0.607422]  [Adverserial loss: 0.845975, acc: 0.196289]\n",
            "791: [Discriminator loss: 0.674145, real acc: 0.593750, fake acc: 0.582031]  [Adverserial loss: 0.826977, acc: 0.251953]\n",
            "792: [Discriminator loss: 0.661488, real acc: 0.616211, fake acc: 0.578125]  [Adverserial loss: 0.852297, acc: 0.217773]\n",
            "793: [Discriminator loss: 0.666984, real acc: 0.596680, fake acc: 0.585938]  [Adverserial loss: 0.827530, acc: 0.226562]\n",
            "794: [Discriminator loss: 0.662042, real acc: 0.629883, fake acc: 0.596680]  [Adverserial loss: 0.865977, acc: 0.222656]\n",
            "795: [Discriminator loss: 0.669341, real acc: 0.622070, fake acc: 0.598633]  [Adverserial loss: 0.840395, acc: 0.199219]\n",
            "796: [Discriminator loss: 0.671493, real acc: 0.584961, fake acc: 0.572266]  [Adverserial loss: 0.878570, acc: 0.191406]\n",
            "797: [Discriminator loss: 0.672381, real acc: 0.566406, fake acc: 0.621094]  [Adverserial loss: 0.841307, acc: 0.221680]\n",
            "798: [Discriminator loss: 0.665663, real acc: 0.569336, fake acc: 0.613281]  [Adverserial loss: 0.838501, acc: 0.233398]\n",
            "799: [Discriminator loss: 0.659076, real acc: 0.606445, fake acc: 0.606445]  [Adverserial loss: 0.853021, acc: 0.214844]\n",
            "800: [Discriminator loss: 0.662240, real acc: 0.612305, fake acc: 0.587891]  [Adverserial loss: 0.859537, acc: 0.198242]\n",
            "801: [Discriminator loss: 0.667024, real acc: 0.602539, fake acc: 0.578125]  [Adverserial loss: 0.827563, acc: 0.242188]\n",
            "802: [Discriminator loss: 0.661624, real acc: 0.584961, fake acc: 0.613281]  [Adverserial loss: 0.850176, acc: 0.187500]\n",
            "803: [Discriminator loss: 0.667092, real acc: 0.570312, fake acc: 0.595703]  [Adverserial loss: 0.833751, acc: 0.242188]\n",
            "804: [Discriminator loss: 0.664047, real acc: 0.599609, fake acc: 0.601562]  [Adverserial loss: 0.857332, acc: 0.205078]\n",
            "805: [Discriminator loss: 0.660142, real acc: 0.598633, fake acc: 0.590820]  [Adverserial loss: 0.856053, acc: 0.185547]\n",
            "806: [Discriminator loss: 0.660686, real acc: 0.599609, fake acc: 0.607422]  [Adverserial loss: 0.850682, acc: 0.216797]\n",
            "807: [Discriminator loss: 0.677284, real acc: 0.612305, fake acc: 0.545898]  [Adverserial loss: 0.828673, acc: 0.208008]\n",
            "808: [Discriminator loss: 0.663754, real acc: 0.618164, fake acc: 0.591797]  [Adverserial loss: 0.857142, acc: 0.209961]\n",
            "809: [Discriminator loss: 0.664485, real acc: 0.589844, fake acc: 0.594727]  [Adverserial loss: 0.838077, acc: 0.205078]\n",
            "810: [Discriminator loss: 0.659065, real acc: 0.617188, fake acc: 0.611328]  [Adverserial loss: 0.887571, acc: 0.163086]\n",
            "811: [Discriminator loss: 0.660571, real acc: 0.580078, fake acc: 0.646484]  [Adverserial loss: 0.853068, acc: 0.204102]\n",
            "812: [Discriminator loss: 0.660300, real acc: 0.599609, fake acc: 0.628906]  [Adverserial loss: 0.882093, acc: 0.185547]\n",
            "813: [Discriminator loss: 0.669346, real acc: 0.590820, fake acc: 0.614258]  [Adverserial loss: 0.845481, acc: 0.221680]\n",
            "814: [Discriminator loss: 0.660205, real acc: 0.592773, fake acc: 0.613281]  [Adverserial loss: 0.869849, acc: 0.194336]\n",
            "815: [Discriminator loss: 0.663553, real acc: 0.598633, fake acc: 0.582031]  [Adverserial loss: 0.841562, acc: 0.228516]\n",
            "816: [Discriminator loss: 0.664747, real acc: 0.611328, fake acc: 0.577148]  [Adverserial loss: 0.857516, acc: 0.210938]\n",
            "817: [Discriminator loss: 0.667287, real acc: 0.599609, fake acc: 0.592773]  [Adverserial loss: 0.845331, acc: 0.208984]\n",
            "818: [Discriminator loss: 0.661386, real acc: 0.602539, fake acc: 0.605469]  [Adverserial loss: 0.852943, acc: 0.229492]\n",
            "819: [Discriminator loss: 0.671551, real acc: 0.571289, fake acc: 0.598633]  [Adverserial loss: 0.837798, acc: 0.227539]\n",
            "820: [Discriminator loss: 0.655744, real acc: 0.643555, fake acc: 0.617188]  [Adverserial loss: 0.869556, acc: 0.208008]\n",
            "821: [Discriminator loss: 0.672668, real acc: 0.571289, fake acc: 0.604492]  [Adverserial loss: 0.838174, acc: 0.217773]\n",
            "822: [Discriminator loss: 0.662401, real acc: 0.615234, fake acc: 0.596680]  [Adverserial loss: 0.849673, acc: 0.209961]\n",
            "823: [Discriminator loss: 0.669100, real acc: 0.594727, fake acc: 0.591797]  [Adverserial loss: 0.832413, acc: 0.231445]\n",
            "824: [Discriminator loss: 0.667794, real acc: 0.606445, fake acc: 0.584961]  [Adverserial loss: 0.856905, acc: 0.214844]\n",
            "825: [Discriminator loss: 0.656280, real acc: 0.610352, fake acc: 0.628906]  [Adverserial loss: 0.847739, acc: 0.236328]\n",
            "826: [Discriminator loss: 0.665930, real acc: 0.603516, fake acc: 0.594727]  [Adverserial loss: 0.851863, acc: 0.218750]\n",
            "827: [Discriminator loss: 0.662926, real acc: 0.603516, fake acc: 0.598633]  [Adverserial loss: 0.846318, acc: 0.216797]\n",
            "828: [Discriminator loss: 0.664117, real acc: 0.597656, fake acc: 0.617188]  [Adverserial loss: 0.854830, acc: 0.203125]\n",
            "829: [Discriminator loss: 0.662424, real acc: 0.622070, fake acc: 0.585938]  [Adverserial loss: 0.838967, acc: 0.230469]\n",
            "830: [Discriminator loss: 0.654288, real acc: 0.626953, fake acc: 0.617188]  [Adverserial loss: 0.858016, acc: 0.207031]\n",
            "831: [Discriminator loss: 0.657968, real acc: 0.620117, fake acc: 0.627930]  [Adverserial loss: 0.847785, acc: 0.196289]\n",
            "832: [Discriminator loss: 0.657507, real acc: 0.630859, fake acc: 0.595703]  [Adverserial loss: 0.878629, acc: 0.202148]\n",
            "833: [Discriminator loss: 0.668429, real acc: 0.569336, fake acc: 0.581055]  [Adverserial loss: 0.829854, acc: 0.223633]\n",
            "834: [Discriminator loss: 0.675867, real acc: 0.607422, fake acc: 0.570312]  [Adverserial loss: 0.852752, acc: 0.218750]\n",
            "835: [Discriminator loss: 0.662495, real acc: 0.616211, fake acc: 0.591797]  [Adverserial loss: 0.855374, acc: 0.204102]\n",
            "836: [Discriminator loss: 0.664181, real acc: 0.623047, fake acc: 0.563477]  [Adverserial loss: 0.852181, acc: 0.198242]\n",
            "837: [Discriminator loss: 0.668633, real acc: 0.605469, fake acc: 0.596680]  [Adverserial loss: 0.842270, acc: 0.219727]\n",
            "838: [Discriminator loss: 0.669215, real acc: 0.561523, fake acc: 0.587891]  [Adverserial loss: 0.866886, acc: 0.211914]\n",
            "839: [Discriminator loss: 0.667063, real acc: 0.592773, fake acc: 0.616211]  [Adverserial loss: 0.858163, acc: 0.194336]\n",
            "840: [Discriminator loss: 0.659067, real acc: 0.592773, fake acc: 0.618164]  [Adverserial loss: 0.867347, acc: 0.193359]\n",
            "841: [Discriminator loss: 0.663210, real acc: 0.576172, fake acc: 0.639648]  [Adverserial loss: 0.842929, acc: 0.208008]\n",
            "842: [Discriminator loss: 0.670644, real acc: 0.583008, fake acc: 0.589844]  [Adverserial loss: 0.853194, acc: 0.192383]\n",
            "843: [Discriminator loss: 0.666604, real acc: 0.622070, fake acc: 0.587891]  [Adverserial loss: 0.845514, acc: 0.216797]\n",
            "844: [Discriminator loss: 0.670432, real acc: 0.571289, fake acc: 0.603516]  [Adverserial loss: 0.857134, acc: 0.196289]\n",
            "845: [Discriminator loss: 0.660607, real acc: 0.612305, fake acc: 0.625977]  [Adverserial loss: 0.839101, acc: 0.208008]\n",
            "846: [Discriminator loss: 0.658847, real acc: 0.598633, fake acc: 0.626953]  [Adverserial loss: 0.855488, acc: 0.207031]\n",
            "847: [Discriminator loss: 0.668911, real acc: 0.567383, fake acc: 0.601562]  [Adverserial loss: 0.857346, acc: 0.195312]\n",
            "848: [Discriminator loss: 0.669316, real acc: 0.590820, fake acc: 0.610352]  [Adverserial loss: 0.861364, acc: 0.216797]\n",
            "849: [Discriminator loss: 0.673586, real acc: 0.567383, fake acc: 0.594727]  [Adverserial loss: 0.840347, acc: 0.230469]\n",
            "850: [Discriminator loss: 0.665718, real acc: 0.589844, fake acc: 0.605469]  [Adverserial loss: 0.885573, acc: 0.184570]\n",
            "851: [Discriminator loss: 0.669024, real acc: 0.553711, fake acc: 0.617188]  [Adverserial loss: 0.840753, acc: 0.208984]\n",
            "852: [Discriminator loss: 0.668067, real acc: 0.586914, fake acc: 0.583008]  [Adverserial loss: 0.862735, acc: 0.220703]\n",
            "853: [Discriminator loss: 0.667293, real acc: 0.560547, fake acc: 0.600586]  [Adverserial loss: 0.840274, acc: 0.209961]\n",
            "854: [Discriminator loss: 0.660522, real acc: 0.605469, fake acc: 0.620117]  [Adverserial loss: 0.862005, acc: 0.204102]\n",
            "855: [Discriminator loss: 0.661141, real acc: 0.583984, fake acc: 0.612305]  [Adverserial loss: 0.853515, acc: 0.207031]\n",
            "856: [Discriminator loss: 0.669831, real acc: 0.604492, fake acc: 0.598633]  [Adverserial loss: 0.860228, acc: 0.207031]\n",
            "857: [Discriminator loss: 0.664665, real acc: 0.590820, fake acc: 0.604492]  [Adverserial loss: 0.854799, acc: 0.211914]\n",
            "858: [Discriminator loss: 0.666415, real acc: 0.593750, fake acc: 0.594727]  [Adverserial loss: 0.851540, acc: 0.227539]\n",
            "859: [Discriminator loss: 0.666556, real acc: 0.590820, fake acc: 0.597656]  [Adverserial loss: 0.865224, acc: 0.198242]\n",
            "860: [Discriminator loss: 0.668719, real acc: 0.590820, fake acc: 0.594727]  [Adverserial loss: 0.843704, acc: 0.212891]\n",
            "861: [Discriminator loss: 0.666105, real acc: 0.570312, fake acc: 0.595703]  [Adverserial loss: 0.862031, acc: 0.207031]\n",
            "862: [Discriminator loss: 0.667306, real acc: 0.572266, fake acc: 0.652344]  [Adverserial loss: 0.846214, acc: 0.194336]\n",
            "863: [Discriminator loss: 0.668673, real acc: 0.593750, fake acc: 0.590820]  [Adverserial loss: 0.865342, acc: 0.217773]\n",
            "864: [Discriminator loss: 0.656279, real acc: 0.609375, fake acc: 0.619141]  [Adverserial loss: 0.831207, acc: 0.230469]\n",
            "865: [Discriminator loss: 0.663169, real acc: 0.619141, fake acc: 0.593750]  [Adverserial loss: 0.837927, acc: 0.216797]\n",
            "866: [Discriminator loss: 0.662839, real acc: 0.596680, fake acc: 0.603516]  [Adverserial loss: 0.846298, acc: 0.209961]\n",
            "867: [Discriminator loss: 0.669245, real acc: 0.610352, fake acc: 0.594727]  [Adverserial loss: 0.854891, acc: 0.210938]\n",
            "868: [Discriminator loss: 0.662589, real acc: 0.603516, fake acc: 0.591797]  [Adverserial loss: 0.832479, acc: 0.221680]\n",
            "869: [Discriminator loss: 0.667492, real acc: 0.593750, fake acc: 0.583984]  [Adverserial loss: 0.863152, acc: 0.220703]\n",
            "870: [Discriminator loss: 0.666924, real acc: 0.586914, fake acc: 0.571289]  [Adverserial loss: 0.833234, acc: 0.224609]\n",
            "871: [Discriminator loss: 0.663185, real acc: 0.594727, fake acc: 0.628906]  [Adverserial loss: 0.884237, acc: 0.176758]\n",
            "872: [Discriminator loss: 0.659464, real acc: 0.598633, fake acc: 0.617188]  [Adverserial loss: 0.861404, acc: 0.205078]\n",
            "873: [Discriminator loss: 0.671137, real acc: 0.556641, fake acc: 0.600586]  [Adverserial loss: 0.880321, acc: 0.186523]\n",
            "874: [Discriminator loss: 0.677418, real acc: 0.551758, fake acc: 0.598633]  [Adverserial loss: 0.838317, acc: 0.217773]\n",
            "875: [Discriminator loss: 0.660041, real acc: 0.597656, fake acc: 0.590820]  [Adverserial loss: 0.843750, acc: 0.229492]\n",
            "876: [Discriminator loss: 0.658717, real acc: 0.624023, fake acc: 0.584961]  [Adverserial loss: 0.847338, acc: 0.208984]\n",
            "877: [Discriminator loss: 0.660827, real acc: 0.606445, fake acc: 0.628906]  [Adverserial loss: 0.861504, acc: 0.219727]\n",
            "878: [Discriminator loss: 0.660037, real acc: 0.616211, fake acc: 0.610352]  [Adverserial loss: 0.839706, acc: 0.233398]\n",
            "879: [Discriminator loss: 0.665895, real acc: 0.602539, fake acc: 0.600586]  [Adverserial loss: 0.851849, acc: 0.218750]\n",
            "880: [Discriminator loss: 0.671564, real acc: 0.588867, fake acc: 0.613281]  [Adverserial loss: 0.852309, acc: 0.203125]\n",
            "881: [Discriminator loss: 0.662239, real acc: 0.596680, fake acc: 0.609375]  [Adverserial loss: 0.889954, acc: 0.183594]\n",
            "882: [Discriminator loss: 0.669935, real acc: 0.552734, fake acc: 0.609375]  [Adverserial loss: 0.843282, acc: 0.200195]\n",
            "883: [Discriminator loss: 0.669331, real acc: 0.603516, fake acc: 0.590820]  [Adverserial loss: 0.847904, acc: 0.223633]\n",
            "884: [Discriminator loss: 0.668144, real acc: 0.585938, fake acc: 0.579102]  [Adverserial loss: 0.855070, acc: 0.199219]\n",
            "885: [Discriminator loss: 0.668294, real acc: 0.591797, fake acc: 0.586914]  [Adverserial loss: 0.853708, acc: 0.210938]\n",
            "886: [Discriminator loss: 0.654532, real acc: 0.607422, fake acc: 0.612305]  [Adverserial loss: 0.852295, acc: 0.208984]\n",
            "887: [Discriminator loss: 0.662548, real acc: 0.589844, fake acc: 0.624023]  [Adverserial loss: 0.855557, acc: 0.204102]\n",
            "888: [Discriminator loss: 0.662349, real acc: 0.574219, fake acc: 0.636719]  [Adverserial loss: 0.859561, acc: 0.208984]\n",
            "889: [Discriminator loss: 0.659166, real acc: 0.563477, fake acc: 0.646484]  [Adverserial loss: 0.928194, acc: 0.142578]\n",
            "890: [Discriminator loss: 0.680202, real acc: 0.547852, fake acc: 0.638672]  [Adverserial loss: 0.841276, acc: 0.243164]\n",
            "891: [Discriminator loss: 0.672035, real acc: 0.575195, fake acc: 0.602539]  [Adverserial loss: 0.852987, acc: 0.228516]\n",
            "892: [Discriminator loss: 0.668832, real acc: 0.597656, fake acc: 0.578125]  [Adverserial loss: 0.829152, acc: 0.246094]\n",
            "893: [Discriminator loss: 0.655324, real acc: 0.606445, fake acc: 0.622070]  [Adverserial loss: 0.857768, acc: 0.208984]\n",
            "894: [Discriminator loss: 0.670250, real acc: 0.579102, fake acc: 0.611328]  [Adverserial loss: 0.836443, acc: 0.225586]\n",
            "895: [Discriminator loss: 0.666634, real acc: 0.604492, fake acc: 0.594727]  [Adverserial loss: 0.864130, acc: 0.189453]\n",
            "896: [Discriminator loss: 0.660476, real acc: 0.612305, fake acc: 0.604492]  [Adverserial loss: 0.844217, acc: 0.206055]\n",
            "897: [Discriminator loss: 0.658382, real acc: 0.606445, fake acc: 0.614258]  [Adverserial loss: 0.876027, acc: 0.195312]\n",
            "898: [Discriminator loss: 0.663909, real acc: 0.587891, fake acc: 0.611328]  [Adverserial loss: 0.857169, acc: 0.204102]\n",
            "899: [Discriminator loss: 0.663439, real acc: 0.565430, fake acc: 0.617188]  [Adverserial loss: 0.879911, acc: 0.198242]\n",
            "900: [Discriminator loss: 0.672165, real acc: 0.586914, fake acc: 0.583984]  [Adverserial loss: 0.847260, acc: 0.218750]\n",
            "901: [Discriminator loss: 0.661639, real acc: 0.612305, fake acc: 0.628906]  [Adverserial loss: 0.864128, acc: 0.200195]\n",
            "902: [Discriminator loss: 0.664732, real acc: 0.571289, fake acc: 0.608398]  [Adverserial loss: 0.834150, acc: 0.222656]\n",
            "903: [Discriminator loss: 0.665153, real acc: 0.602539, fake acc: 0.590820]  [Adverserial loss: 0.864514, acc: 0.193359]\n",
            "904: [Discriminator loss: 0.669476, real acc: 0.610352, fake acc: 0.571289]  [Adverserial loss: 0.837277, acc: 0.238281]\n",
            "905: [Discriminator loss: 0.665908, real acc: 0.622070, fake acc: 0.573242]  [Adverserial loss: 0.850704, acc: 0.248047]\n",
            "906: [Discriminator loss: 0.665904, real acc: 0.591797, fake acc: 0.598633]  [Adverserial loss: 0.845910, acc: 0.213867]\n",
            "907: [Discriminator loss: 0.669611, real acc: 0.587891, fake acc: 0.584961]  [Adverserial loss: 0.842442, acc: 0.238281]\n",
            "908: [Discriminator loss: 0.665892, real acc: 0.587891, fake acc: 0.593750]  [Adverserial loss: 0.870413, acc: 0.212891]\n",
            "909: [Discriminator loss: 0.672369, real acc: 0.580078, fake acc: 0.593750]  [Adverserial loss: 0.842516, acc: 0.243164]\n",
            "910: [Discriminator loss: 0.666226, real acc: 0.564453, fake acc: 0.634766]  [Adverserial loss: 0.869298, acc: 0.205078]\n",
            "911: [Discriminator loss: 0.673289, real acc: 0.567383, fake acc: 0.581055]  [Adverserial loss: 0.851310, acc: 0.187500]\n",
            "912: [Discriminator loss: 0.663296, real acc: 0.583008, fake acc: 0.635742]  [Adverserial loss: 0.856125, acc: 0.220703]\n",
            "913: [Discriminator loss: 0.667688, real acc: 0.581055, fake acc: 0.613281]  [Adverserial loss: 0.843660, acc: 0.231445]\n",
            "914: [Discriminator loss: 0.663966, real acc: 0.615234, fake acc: 0.576172]  [Adverserial loss: 0.837130, acc: 0.249023]\n",
            "915: [Discriminator loss: 0.668984, real acc: 0.591797, fake acc: 0.556641]  [Adverserial loss: 0.854045, acc: 0.210938]\n",
            "916: [Discriminator loss: 0.656628, real acc: 0.628906, fake acc: 0.608398]  [Adverserial loss: 0.869841, acc: 0.201172]\n",
            "917: [Discriminator loss: 0.664810, real acc: 0.600586, fake acc: 0.617188]  [Adverserial loss: 0.850936, acc: 0.220703]\n",
            "918: [Discriminator loss: 0.664327, real acc: 0.586914, fake acc: 0.615234]  [Adverserial loss: 0.876792, acc: 0.193359]\n",
            "919: [Discriminator loss: 0.661366, real acc: 0.601562, fake acc: 0.622070]  [Adverserial loss: 0.849023, acc: 0.201172]\n",
            "920: [Discriminator loss: 0.665274, real acc: 0.599609, fake acc: 0.585938]  [Adverserial loss: 0.902545, acc: 0.187500]\n",
            "921: [Discriminator loss: 0.666696, real acc: 0.616211, fake acc: 0.583008]  [Adverserial loss: 0.854389, acc: 0.197266]\n",
            "922: [Discriminator loss: 0.672202, real acc: 0.573242, fake acc: 0.596680]  [Adverserial loss: 0.874833, acc: 0.199219]\n",
            "923: [Discriminator loss: 0.667446, real acc: 0.594727, fake acc: 0.602539]  [Adverserial loss: 0.833467, acc: 0.239258]\n",
            "924: [Discriminator loss: 0.656179, real acc: 0.623047, fake acc: 0.641602]  [Adverserial loss: 0.880026, acc: 0.197266]\n",
            "925: [Discriminator loss: 0.665311, real acc: 0.592773, fake acc: 0.596680]  [Adverserial loss: 0.847633, acc: 0.232422]\n",
            "926: [Discriminator loss: 0.671977, real acc: 0.596680, fake acc: 0.598633]  [Adverserial loss: 0.845733, acc: 0.231445]\n",
            "927: [Discriminator loss: 0.661187, real acc: 0.620117, fake acc: 0.613281]  [Adverserial loss: 0.864404, acc: 0.202148]\n",
            "928: [Discriminator loss: 0.662107, real acc: 0.605469, fake acc: 0.623047]  [Adverserial loss: 0.852801, acc: 0.220703]\n",
            "929: [Discriminator loss: 0.665989, real acc: 0.597656, fake acc: 0.624023]  [Adverserial loss: 0.871939, acc: 0.196289]\n",
            "930: [Discriminator loss: 0.669016, real acc: 0.582031, fake acc: 0.621094]  [Adverserial loss: 0.856445, acc: 0.223633]\n",
            "931: [Discriminator loss: 0.663261, real acc: 0.605469, fake acc: 0.606445]  [Adverserial loss: 0.863274, acc: 0.194336]\n",
            "932: [Discriminator loss: 0.665933, real acc: 0.607422, fake acc: 0.614258]  [Adverserial loss: 0.853454, acc: 0.212891]\n",
            "933: [Discriminator loss: 0.663974, real acc: 0.564453, fake acc: 0.625000]  [Adverserial loss: 0.875085, acc: 0.182617]\n",
            "934: [Discriminator loss: 0.662566, real acc: 0.587891, fake acc: 0.617188]  [Adverserial loss: 0.843726, acc: 0.219727]\n",
            "935: [Discriminator loss: 0.665641, real acc: 0.594727, fake acc: 0.584961]  [Adverserial loss: 0.871701, acc: 0.229492]\n",
            "936: [Discriminator loss: 0.672408, real acc: 0.582031, fake acc: 0.578125]  [Adverserial loss: 0.834850, acc: 0.226562]\n",
            "937: [Discriminator loss: 0.650817, real acc: 0.626953, fake acc: 0.618164]  [Adverserial loss: 0.884296, acc: 0.181641]\n",
            "938: [Discriminator loss: 0.663798, real acc: 0.575195, fake acc: 0.632812]  [Adverserial loss: 0.849508, acc: 0.203125]\n",
            "939: [Discriminator loss: 0.664503, real acc: 0.589844, fake acc: 0.633789]  [Adverserial loss: 0.872583, acc: 0.216797]\n",
            "940: [Discriminator loss: 0.660262, real acc: 0.632812, fake acc: 0.613281]  [Adverserial loss: 0.838149, acc: 0.239258]\n",
            "941: [Discriminator loss: 0.667365, real acc: 0.627930, fake acc: 0.583008]  [Adverserial loss: 0.841260, acc: 0.257812]\n",
            "942: [Discriminator loss: 0.665026, real acc: 0.628906, fake acc: 0.569336]  [Adverserial loss: 0.865311, acc: 0.210938]\n",
            "943: [Discriminator loss: 0.662229, real acc: 0.611328, fake acc: 0.623047]  [Adverserial loss: 0.860616, acc: 0.193359]\n",
            "944: [Discriminator loss: 0.664212, real acc: 0.607422, fake acc: 0.613281]  [Adverserial loss: 0.871836, acc: 0.202148]\n",
            "945: [Discriminator loss: 0.675919, real acc: 0.562500, fake acc: 0.576172]  [Adverserial loss: 0.858536, acc: 0.208984]\n",
            "946: [Discriminator loss: 0.665770, real acc: 0.601562, fake acc: 0.608398]  [Adverserial loss: 0.853054, acc: 0.225586]\n",
            "947: [Discriminator loss: 0.668326, real acc: 0.597656, fake acc: 0.620117]  [Adverserial loss: 0.847147, acc: 0.218750]\n",
            "948: [Discriminator loss: 0.658054, real acc: 0.594727, fake acc: 0.641602]  [Adverserial loss: 0.877290, acc: 0.189453]\n",
            "949: [Discriminator loss: 0.666957, real acc: 0.596680, fake acc: 0.575195]  [Adverserial loss: 0.835082, acc: 0.230469]\n",
            "950: [Discriminator loss: 0.665025, real acc: 0.588867, fake acc: 0.620117]  [Adverserial loss: 0.870053, acc: 0.206055]\n",
            "951: [Discriminator loss: 0.673929, real acc: 0.575195, fake acc: 0.594727]  [Adverserial loss: 0.845403, acc: 0.204102]\n",
            "952: [Discriminator loss: 0.666541, real acc: 0.606445, fake acc: 0.589844]  [Adverserial loss: 0.879544, acc: 0.200195]\n",
            "953: [Discriminator loss: 0.674240, real acc: 0.564453, fake acc: 0.593750]  [Adverserial loss: 0.850263, acc: 0.184570]\n",
            "954: [Discriminator loss: 0.665284, real acc: 0.625000, fake acc: 0.594727]  [Adverserial loss: 0.858210, acc: 0.216797]\n",
            "955: [Discriminator loss: 0.661782, real acc: 0.602539, fake acc: 0.594727]  [Adverserial loss: 0.840367, acc: 0.227539]\n",
            "956: [Discriminator loss: 0.662722, real acc: 0.590820, fake acc: 0.625000]  [Adverserial loss: 0.860948, acc: 0.211914]\n",
            "957: [Discriminator loss: 0.664087, real acc: 0.542969, fake acc: 0.637695]  [Adverserial loss: 0.868547, acc: 0.190430]\n",
            "958: [Discriminator loss: 0.671565, real acc: 0.570312, fake acc: 0.632812]  [Adverserial loss: 0.861407, acc: 0.213867]\n",
            "959: [Discriminator loss: 0.665278, real acc: 0.601562, fake acc: 0.617188]  [Adverserial loss: 0.877609, acc: 0.207031]\n",
            "960: [Discriminator loss: 0.671161, real acc: 0.571289, fake acc: 0.625977]  [Adverserial loss: 0.860696, acc: 0.203125]\n",
            "961: [Discriminator loss: 0.670384, real acc: 0.573242, fake acc: 0.580078]  [Adverserial loss: 0.859783, acc: 0.215820]\n",
            "962: [Discriminator loss: 0.666758, real acc: 0.602539, fake acc: 0.613281]  [Adverserial loss: 0.840018, acc: 0.239258]\n",
            "963: [Discriminator loss: 0.665212, real acc: 0.624023, fake acc: 0.600586]  [Adverserial loss: 0.858993, acc: 0.218750]\n",
            "964: [Discriminator loss: 0.656390, real acc: 0.644531, fake acc: 0.599609]  [Adverserial loss: 0.851142, acc: 0.219727]\n",
            "965: [Discriminator loss: 0.659040, real acc: 0.639648, fake acc: 0.601562]  [Adverserial loss: 0.846020, acc: 0.212891]\n",
            "966: [Discriminator loss: 0.660886, real acc: 0.586914, fake acc: 0.610352]  [Adverserial loss: 0.857246, acc: 0.216797]\n",
            "967: [Discriminator loss: 0.667924, real acc: 0.579102, fake acc: 0.586914]  [Adverserial loss: 0.845727, acc: 0.214844]\n",
            "968: [Discriminator loss: 0.664136, real acc: 0.604492, fake acc: 0.595703]  [Adverserial loss: 0.890502, acc: 0.197266]\n",
            "969: [Discriminator loss: 0.669921, real acc: 0.576172, fake acc: 0.625977]  [Adverserial loss: 0.860263, acc: 0.194336]\n",
            "970: [Discriminator loss: 0.670795, real acc: 0.567383, fake acc: 0.602539]  [Adverserial loss: 0.862330, acc: 0.223633]\n",
            "971: [Discriminator loss: 0.652334, real acc: 0.622070, fake acc: 0.656250]  [Adverserial loss: 0.863111, acc: 0.196289]\n",
            "972: [Discriminator loss: 0.664838, real acc: 0.590820, fake acc: 0.590820]  [Adverserial loss: 0.865333, acc: 0.246094]\n",
            "973: [Discriminator loss: 0.670464, real acc: 0.578125, fake acc: 0.630859]  [Adverserial loss: 0.847920, acc: 0.243164]\n",
            "974: [Discriminator loss: 0.661917, real acc: 0.609375, fake acc: 0.574219]  [Adverserial loss: 0.871383, acc: 0.214844]\n",
            "975: [Discriminator loss: 0.663369, real acc: 0.583008, fake acc: 0.610352]  [Adverserial loss: 0.838002, acc: 0.227539]\n",
            "976: [Discriminator loss: 0.663321, real acc: 0.601562, fake acc: 0.630859]  [Adverserial loss: 0.884230, acc: 0.211914]\n",
            "977: [Discriminator loss: 0.668681, real acc: 0.607422, fake acc: 0.568359]  [Adverserial loss: 0.832586, acc: 0.229492]\n",
            "978: [Discriminator loss: 0.670443, real acc: 0.580078, fake acc: 0.599609]  [Adverserial loss: 0.858574, acc: 0.229492]\n",
            "979: [Discriminator loss: 0.657583, real acc: 0.584961, fake acc: 0.643555]  [Adverserial loss: 0.853957, acc: 0.187500]\n",
            "980: [Discriminator loss: 0.663836, real acc: 0.590820, fake acc: 0.625000]  [Adverserial loss: 0.875897, acc: 0.204102]\n",
            "981: [Discriminator loss: 0.671062, real acc: 0.578125, fake acc: 0.602539]  [Adverserial loss: 0.847125, acc: 0.190430]\n",
            "982: [Discriminator loss: 0.662012, real acc: 0.580078, fake acc: 0.627930]  [Adverserial loss: 0.866529, acc: 0.205078]\n",
            "983: [Discriminator loss: 0.662888, real acc: 0.596680, fake acc: 0.613281]  [Adverserial loss: 0.830795, acc: 0.262695]\n",
            "984: [Discriminator loss: 0.662130, real acc: 0.610352, fake acc: 0.625977]  [Adverserial loss: 0.861523, acc: 0.210938]\n",
            "985: [Discriminator loss: 0.664860, real acc: 0.603516, fake acc: 0.599609]  [Adverserial loss: 0.833243, acc: 0.235352]\n",
            "986: [Discriminator loss: 0.664193, real acc: 0.609375, fake acc: 0.595703]  [Adverserial loss: 0.857873, acc: 0.250977]\n",
            "987: [Discriminator loss: 0.657678, real acc: 0.622070, fake acc: 0.610352]  [Adverserial loss: 0.841932, acc: 0.228516]\n",
            "988: [Discriminator loss: 0.670591, real acc: 0.588867, fake acc: 0.578125]  [Adverserial loss: 0.907197, acc: 0.184570]\n",
            "989: [Discriminator loss: 0.665030, real acc: 0.588867, fake acc: 0.615234]  [Adverserial loss: 0.861578, acc: 0.203125]\n",
            "990: [Discriminator loss: 0.663519, real acc: 0.576172, fake acc: 0.607422]  [Adverserial loss: 0.866614, acc: 0.199219]\n",
            "991: [Discriminator loss: 0.678394, real acc: 0.574219, fake acc: 0.594727]  [Adverserial loss: 0.855910, acc: 0.209961]\n",
            "992: [Discriminator loss: 0.669632, real acc: 0.608398, fake acc: 0.594727]  [Adverserial loss: 0.852000, acc: 0.242188]\n",
            "993: [Discriminator loss: 0.681632, real acc: 0.564453, fake acc: 0.579102]  [Adverserial loss: 0.850258, acc: 0.239258]\n",
            "994: [Discriminator loss: 0.664975, real acc: 0.583008, fake acc: 0.599609]  [Adverserial loss: 0.859339, acc: 0.213867]\n",
            "995: [Discriminator loss: 0.658755, real acc: 0.608398, fake acc: 0.636719]  [Adverserial loss: 0.851939, acc: 0.232422]\n",
            "996: [Discriminator loss: 0.662401, real acc: 0.582031, fake acc: 0.603516]  [Adverserial loss: 0.843001, acc: 0.230469]\n",
            "997: [Discriminator loss: 0.659790, real acc: 0.608398, fake acc: 0.609375]  [Adverserial loss: 0.844425, acc: 0.240234]\n",
            "998: [Discriminator loss: 0.668023, real acc: 0.583008, fake acc: 0.603516]  [Adverserial loss: 0.852448, acc: 0.217773]\n",
            "999: [Discriminator loss: 0.657359, real acc: 0.611328, fake acc: 0.627930]  [Adverserial loss: 0.870259, acc: 0.219727]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAACgCAYAAAALmQcXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3wU9bn48c8zs7eEhBAwXgAV7KmIKAEhoqJitYjaemm1x9uh0Gpbz0/rsbYetbWn19Oj1lPrpT+8gVZ/Vm1R0aN4aIWirRcUFKsIKiBilLshIZfd7O48vz9msgRIQgLZbLL7vF+vhdm57TM7m2e/+52ZZ0RVMcYYUzicXAdgjDGmZ1niN8aYAmOJ3xhjCowlfmOMKTCW+I0xpsBY4jfGmAJjid8UPBF5QER+2cl514jIF/d2PcbkkiV+Y4wpMJb4jTGmwFjiN31C0MVyjYj8Q0QaRGSmiOwnIs+JyDYReV5EylvNf5aILBORrSKyUERGtpo2VkTeCJZ7DIjt9FpfFpGlwbIvi8joPYz5WyKyUkQ+E5GnRWRwMF5E5FYR2SgidSLytogcEUw7Q0TeDWL7RER+0Jm4ROTaYP5tIvKeiJyyJzGbwmCJ3/Ql5wKTgUOBM4HngB8CFfif5SsBRORQ4BHgqmDaXOB/RCQiIhFgDvAQMBD4U7BegmXHArOA7wCDgLuBp0Uk2pVAReRk4L+AfwYOAD4CHg0mnwqcGGxHWTDPlmDaTOA7qloKHAEs2F1cIjICuAKoCpabAqzpSrymsFjiN33JHaq6QVU/Af4GLFLVN1U1DjwJjA3mOx94VlX/oqpJ4BagCDgOOAYIA79V1aSqzgZeb/Ua3wbuVtVFqppW1d8DiWC5rrgYmKWqb6hqArgeOFZEhgFJoBQ4DBBVXa6q64LlksDhItJfVWtU9Y1OxJUGosFyYVVdo6qruhivKSCW+E1fsqHVcFMbz0uC4cH4LWwAVNUDPgaGBNM+0R2rE37Uavhg4PtBd8pWEdkKHBgs1xU7x1CP36ofoqoLgDuB3wEbReQeEekfzHoucAbwkYi8ICLH7i4uVV2J/+vmp8H6Hm3pVjKmLZb4TT76FD9RAn6fOn6S/ARYBwwJxrU4qNXwx8B/quqAVo9iVX1kL2Poh99F8wmAqt6uquOAw/G7fK4Jxr+uqmcD++J3Sf2xM3Gp6h9U9fjgNRW4qYvxmgJiid/koz8CXxKRU0QkDHwfv1vkZeAVIAVcKSJhEfkqcHSrZe8FLhORCcFB2H4i8iURKe1iDI8A3xCRMcHxgV/hd02tEZGqYP1hoAGIA15wDOJiESkLuqjqAG93cYnICBE5OXidOP6vH2+XiIwJWOI3eUdV3wP+BbgD2Ix/IPhMVW1W1Wbgq8B04DP84wFPtFp2MfAt/K6YGmBlMG9XY3ge+DHwOP6vjM8BFwST++Mn8hr87qAtwK+DaVOBNSJSB1yGf6xgd3FFgRuDbV2P/2vh+q7GbAqH2I1YjDGmsFiL3xhjCowlfmOMKTCW+I0xpsBY4jfGmAJjid8YYwpMKFsrFpEY8CL+qWYhYLaq/kREhuPXLBkELAGmBqfYtWufffbRYcOGZStUY4zJS0uWLNmsqhU7j89a4se/YOZkVa0PLlT5u4g8B1wN3Kqqj4rIXcAlwIyOVjRs2DAWL16cxVCNMSb/iMhHbY3PWleP+uqDp+HgocDJwOxg/O+Bc7IVgzHGmF1ltY9fRFwRWQpsBP4CrAK2qmoqmKUav3BWr7GteRvrG9bnOgxjjMmarCb+oHzsGGAofj2Uwzq7rIh8W0QWi8jiTZs2ZS3GnZ379LlMnj25x17PGGN6Wjb7+DNUdauI/BU4FhggIqGg1T+UoFphG8vcA9wDMH78+B6rK7GuYd3uZzKmQCSTSaqrq4nH47kOxXQgFosxdOhQwuFwp+bP5lk9FUAySPpF+HdOugn4K3Ae/pk904CnshWDMWbvVFdXU1payrBhw9ixkrXpLVSVLVu2UF1dzfDhwzu1TDa7eg4A/ioi/8C/w9FfVPUZ4FrgahFZiX9K58wsxmCM2QvxeJxBgwZZ0u/FRIRBgwZ16VdZ1lr8qvoPtt8Kr/X41exY/9wY04tZ0u/9urqP7MpdY0yv5rouY8aMYdSoUVRWVvLf//3feF7b95mZPn06s2fPbnNaX/bTn/6UW265pdvW1yMHd40xZk8VFRWxdOlSADZu3MhFF11EXV0dP/vZz3Ic2Z5JpVKEQrlNvdbiN8b0Gfvuuy/33HMPd955J7u7idT8+fMZO3YsRx55JN/85jdJJBIAXHfddRx++OGMHj2aH/zgBwD86U9/4ogjjqCyspITTzxxl3WpKtdccw1HHHEERx55JI899hgACxcu5KSTTuK8887jsMMO4+KLL24zrpNOOomrrrqK8ePHc9ttt7FkyRImTZrEuHHjmDJlCuvW+WcT3nvvvVRVVVFZWcm5555LY2PjXr1f7bEWvzGmU372P8t499O6bl3n4YP785MzR3VpmUMOOYR0Os3GjRvZb7/92pwnHo8zffp05s+fz6GHHsrXv/51ZsyYwdSpU3nyySdZsWIFIsLWrVsB+PnPf868efMYMmRIZlxrTzzxBEuXLuWtt95i8+bNVFVVZb4g3nzzTZYtW8bgwYOZOHEiL730Escff/wu62hubmbx4sUkk0kmTZrEU089RUVFBY899hg/+tGPmDVrFl/96lf51re+BcANN9zAzJkz+e53v9ul96czrMVvjMk77733HsOHD+fQQw8FYNq0abz44ouUlZURi8W45JJLeOKJJyguLgZg4sSJTJ8+nXvvvZd0Or3L+v7+979z4YUX4rou++23H5MmTeL1118H4Oijj2bo0KE4jsOYMWNYs2ZNmzGdf/75mdjeeecdJk+ezJgxY/jlL39JdXU1AO+88w4nnHACRx55JA8//DDLli3r7rcGsBa/MaaTutoyz5bVq1fjui777rsv3/jGN3jzzTcZPHgwc+fO3e2yoVCI1157jfnz5zN79mzuvPNOFixYwF133cWiRYt49tlnGTduHEuWLGHQoEGdiicajWaGXdcllUq1OV+/fv0Av9to1KhRvPLKK7vMM336dObMmUNlZSUPPPAACxcu7FQMXWUtfmNMn7Fp0yYuu+wyrrjiCkSE+++/n6VLl+6S9EeMGMGaNWtYuXIlAA899BCTJk2ivr6e2tpazjjjDG699VbeeustAFatWsWECRP4+c9/TkVFBR9//PEO6zvhhBN47LHHSKfTbNq0iRdffJGjj96zs9JHjBjBpk2bMok/mUxmWvbbtm3jgAMOIJlM8vDDD+/R+jvDWvzGmF6tqamJMWPGkEwmCYVCTJ06lauvvrrDZWKxGPfffz9f+9rXSKVSVFVVcdlll/HZZ59x9tlnE4/HUVV+85vfAHDNNdfwwQcfoKqccsopVFZW7rC+r3zlK7zyyitUVlYiItx8883sv//+rFixosvbE4lEmD17NldeeSW1tbWkUimuuuoqRo0axS9+8QsmTJhARUUFEyZMYNu2bV1ef2fI7o6M9wbjx4/XnqrHf+TvjwTg7Wlv98jrGdObLV++nJEjR+Y6DNMJbe0rEVmiquN3nte6eowxpsBY4jfGmAJjid8YYwqMJX5jjCkwlviNMabAWOI3xpgCY4nfGNOr5UNZ5hUrVjBmzBjGjh3LqlWr2p2vpKSkR+KxC7iMMb1aPpRlnjNnDueddx433HBDrkMBrMVvjOlD+mJZ5rlz5/Lb3/6WGTNm8IUvfAGAc845h3HjxjFq1CjuueeeXV5v8+bNHHvssTz77LNs2rSJc889l6qqKqqqqnjppZe6/sbtxFr8xpjOee46WN/NV7TvfyScfmOXFulrZZnPOOMMLrvsMkpKSjJfNLNmzWLgwIE0NTVRVVXFueeemykKt2HDBs466yx++ctfMnnyZC666CK+973vcfzxx7N27VqmTJnC8uXLu/Se7cxa/MaYvNMbyzK3dvvtt1NZWckxxxzDxx9/zAcffAD4BdtOOeUUbr75ZiZPngzA888/zxVXXMGYMWM466yzqKuro76+fq/eH2vxG2M6p4st82zpq2WZWyxcuJDnn3+eV155heLiYk466STi8XgmvnHjxjFv3jwmTZoEgOd5vPrqq8RisU7F0xlZa/GLyIEi8lcReVdElonIvwXjB4rIX0Tkg+D/8mzFYIzJL/lQlrm2tpby8nKKi4tZsWIFr776amaaiDBr1ixWrFjBTTfdBMCpp57KHXfckZmn5UD33shmiz8FfF9V3xCRUmCJiPwFmA7MV9UbReQ64Drg2izGYYzpw/KtLPNpp53GXXfdxciRIxkxYgTHHHPMDtNd1+WRRx7hrLPOorS0lNtvv53LL7+c0aNHk0qlOPHEE7nrrru6/Lqt9VhZZhF5CrgzeJykqutE5ABgoaqO6GhZK8tsTG5YWea+o9eVZRaRYcBYYBGwn6quCyatB9o+LG+MMSYrsp74RaQEeBy4SlXrWk9T/+dGmz85ROTbIrJYRBZv2rQp22EaY0zByGriF5EwftJ/WFWfCEZvCLp4CP7f2NayqnqPqo5X1fEVFRXZDNMYYwpKNs/qEWAmsFxVf9Nq0tPAtGB4GvBUtmLYG33hlpTGGLMnsnlWz0RgKvC2iLScf/RD4EbgjyJyCfAR8M9ZjGGPKYoguQ7DGGO6XdYSv6r+HdrNnKdk63W7i6q2H70xxvRhVrKhHdr2MWdjTA/bsGEDF110EYcccgjjxo3j2GOP5cknn+yWdffWMs7ZZom/HZb4jck9VeWcc87hxBNPZPXq1SxZsoRHH32U6urqXIfWp1nib4/lfWNybsGCBUQiES677LLMuIMPPpjvfve7AKTTaa655hqqqqoYPXo0d999N9C5csk7684yzr1dp/r4gzo79wPbgPvwL8a6TlX/nMXYcspa/Mbs6KbXbmLFZ10vUdCRwwYexrVHt1+xZdmyZRx11FHtTp85cyZlZWW8/vrrJBIJJk6cyKmnngrsvlxya91dxrm362yL/5vBxVenAuX4Z+v0jlJ9xpiCcfnll1NZWUlVVRUAf/7zn3nwwQcZM2YMEyZMYMuWLZkSx10pl9zdZZx7u86e1dNyfssZwEOquiw4Tz9vWYvfmB111DLPllGjRvH4449nnv/ud79j8+bNjB/vl59RVe644w6mTJmyw3ILFy7scrnktmSjjHNv0NkW/xIR+TN+4p8XVNts+27HecIu4DIm904++WTi8TgzZszIjGtsbMwMT5kyhRkzZpBMJgF4//33aWho6PLrdHcZ596usy3+S4AxwGpVbRSRgcA3shdW7lmL35jcExHmzJnD9773PW6++WYqKiro169fplb9pZdeypo1azjqqKNQVSoqKpgzZ06XX6e7yzj3dp0qyywiE4GlqtogIv8CHAXcpqofZTtAyE1Z5kUXLeL9mvcZGBvIQf0P6pHXNqa3sbLMfUc2yjLPABpFpBL4PrAKeHBvA+3tpj43lS89+aVch2GMMd2qs4k/FZRQPhu4U1V/B5RmL6zcs64eY0y+6mwf/zYRuR7/NM4TRMQBwtkLK/fs4K4xJl91tsV/PpDAP59/PTAU+HXWouoFrMVvjMlXnUr8QbJ/GCgTkS8DcVXN+z5+Y4zJR51K/CLyz8BrwNfw6+cvEpHzshlYrlmL3xiTrzrb1fMjoEpVp6nq14GjgR9nL6zcsz5+Y3oHK8vc/Tqb+B1VbX1v3C1dWNYYY/aIlWXOjs4m7/8VkXkiMl1EpgPPAnOzF1buWYvfmNyzsszZ0anTOVX1GhE5F/8+ugD3qGr3/NbqpayP35gdrf/Vr0gs796yzNGRh7H/D3/Y7nQry5wdne6uUdXHVfXq4JHXSR8grX2v1Kox+c7KMnePDlv8IrKNtu9FJYCqav+sRNULpLyul3A1Jp911DLPFivLnB0dtvhVtVRV+7fxKN1d0heRWSKyUUTeaTVuoIj8RUQ+CP4v764N6W7W4jcm96wsc3Z0tmTDnngAuJMdi7ldB8xX1RtF5Lrgec/f3aET0p4lfmNyzcoyZ0enyjLv8cpFhgHPqOoRwfP3gJNUdZ2IHAAsVNURu1tPLsoyP3XOU5w952wA3p72do+8tjG9jZVl7juyUZa5u+ynquuC4fXAfj38+p22oa5x9zMZY0wflLOLsIIyz+3+3BCRb4vIYhFZvGnTph6MzPfq6o27n8kYY/qgnk78G4IuHoL/282uqnqPqo5X1fEVFRU9FmALj+19/Nbfb4zJJz2d+J8GpgXD04Cnevj1O631WT1JL5nDSIzJLbuKvffr6j7KWuIXkUeAV4ARIlItIpcANwKTReQD4IvB817Ja5X4W87pX7RuEb9f9vtchWRMj4vFYmzZssWSfy+mqmzZsoVYLNbpZbJ2OqeqXtjOpFOy9ZrdyWP7xR4tLf5L/3wpANNGTWtzGWPyzdChQ6muriYXx9lM58ViMYYOHdrp+bN5Hn+f1laL35hCEw6HGT58eK7DMN3MSiu3I+U1Z4atj98Yk08s8bej2WvKDO+c+JNp+yIwxvRdlvjbkWiV+Hfu6vnBCz/o6XCMMabbFHTi99Rj5tszaUjuWtSp2dt+5e7OLf4FHy9oc30pL8W7W97t3iCNMaabFXTiX7B2Ab9947f8ZvFvdpnW3EGLvy1NqSZuWXwL5z9zPmtq13RnmMYY060K+qyeeDoOQH2yfpdprRP/q+tepTHZce2eox8+OjP8X6/9F3dPvrubojTGmO5V0Im/o4tSEq26em5747Z2l1+5dSW3v3n7DuNf/vTl7gnQGGOyoKATf0e2pbZQFi1jRPkIXlv/2i7TPfU488kzWbttbQ6iM8aYPVfQffwi0u60bckt7Fu8LzOnzOTeU+/dZXrlg5UdJv01tWu46bWbaE43tzuPMcbkgrX421Gf2sJhRWMBGFrS+UuhW5w550wAThhyAscNOa5bYzPGmL1R0C3+jiS8RvYt3heA/fvtv8v080ecz0sXvsTdk+/m9GGn853R3+G2L+x6LOA7z3+Hlz+xPn9jTO9hLf4OVBT79wEIObu+TTcccwMAxw0+juMG+y3692vez0wXBA3uMzP3w7mM2mcUZdGybIdsjDG7ZS3+DlQUbb8BzKkHn7rb+SNOJDM8aegkDi0/FICnVj3FOU+dw31v38f6hvVW4tYYk1OW+DvQ0tUDcNOJN3H68NM7nN913MzwcUOO45EvPcIXD/oiAJubNnPbG7cxefZkRj84mtveuI265rrsBG6MMR0o6MS/u5Z3Ragf1KwB/O6e8mj5blbo/xdxIlww4gIiboT/PP4/efysx5l56swdZr3v7fuY+MhErn3xWl7+5GU2NGywWzwaY3qE9fF3oOLpq2H9O/DT2h3G/2vlv7Y5/6CiQQBcN+G6zKmixeHiTJfPoosW8V7Ne5RFy/jh337Isi3LmPvhXOZ+ODezjtOHn84VY67goP4HZWOTjDHGEn9HBq1/Z4fnLQdr2ztIWxwu5u1pb7e7vuJwMWP39U8RfeRLj5BIJ3j8g8e58bXtd6B87sPneO7D55h6+FT2L96fResXMfXwqRxzwDF7uznGGAMUeOLfXfG1cMuAlwbH7daDsiJCLBTj4pEXc8GIC4in4yxYu4D/eOk/SGmKh959KDPvi9Uv8sBpD3Bg6YFE3Sj9I/07vPjMGGM6kv+JP50EN9z2pJbbK8bbOsi6PbF6zY04sdJMi1/o3qTrOi79nH6c+bkz+fIhX878AvjDij9k5pn+v9Mzw5ceeSlXjr3Skr8xZo/kd+L/wwXQVAOXzGtzcuZg6gfzYKezNYvdAcBHADQ21lMSK81My2bCFRGun3A9ABcediE/eulHrK1bi6LUJvxjDfe9fR9zVs6hNlHLwNhAph4+lXP+6Ry7TsAY0yn5nfiLB6Lr3qL28cdJ122DnbpqBm58g68vS1MRc6l57I9INMLBG5SP9hP6udvP4Gmor6dkIKz/eCUAK/6xGA67MOvhDysbxlcG/YJVEufa0w/nvS2ruWX+v/NWYi0HlR7EG01vsKFxA7csvoVbFt/CgOgAPl/+eQ4sPZChJUMZUjKEkkgJA6IDGFwymKgbJeyEibgRHCnoE7oM/lltHTVi2pre0TKqiqde5rTm5nQzTammTIOkrrmOqBsl6kZpTjcTdsJtrktViafjRJwIbtDF2pRqIhaK4YiDpx4pL0U8HadfqB8AjjgoSjwVx1OPolBR5he6oqDg4aGqKErICeGKS1rTNKWaKAoVkUglSHpJBkQHsKFxAyXhEmKhGE2pJqJulJATojndjIgQdsLUJ+tJeSliboyQEyLlpRARVDVz0WfICZH0kqS9dOZvLuL61/sk0gkSqQSu4+KKS9SNZuJp2daWxl13/73mJPGLyGnAbYAL3KeqN+5mkT3yq/qNFH2a5LR7b2hz+tDgASHW/+0nAPwaWD4UHpq2vfXc0Ojfoau0ZgtEwVm9tN3XbPhsHcWfvooc8RWWr1yF27SFkv0/xwH7DAT87x7HafVh99Ik4/XUx1Nsqm+mvLQfDz37POOOqiL5wq2ct2Em89Ljeb32C1St+DUt5eLWb45RXreWRbEYLxYX8Y9olP0aNvBp/Tbec5dQ53gdvjeuCiEghEMI8f9XIYwQ8TxEokTQYCq46vnn/oqDBJ1dLUMCwXO/G8wBHBVEdjomoq0GxL+22V/a/xOVzCzb/229Bm31trX8URP8ISPODsdgtNVrK5JZjwKiKVRcf/3SEr3ntwtEdoxBgyHZfvqvCKSDCBwcUA8NEs/OcWeGxY+59TSv1Za0nt9rdz2a+b/19J3X23q+nZfPvAcIDSSJ4uIErynB56CRFGEcEqQJ49BMmlINoSJsI4mLEMYh1WrveCjp4HkMP/En8UijhIL3P91qfvA/P9pqnIME4yCFhwARXBJsP815+2dkRw6Sed86a0+W6ar24u2KP37hQUYeNLY7wsno8cQvIi7wO2AyUA28LiJPq2q337NwcSLKF9aHSTmQ+o/TGH3GTyFTfkGZ939/wPBZL7B2eJqTH/gbG9atZNsFlzKyGi54oaHlW4GGBv9GLeevbOLrT6c45Etv8tKzD3LgiHHESgdQWlSEm46z+aNlDJ5znr/Q7OmMbBXL23yOIknxYWoQZeE0w7SasJegH01EJE050PIb42qAlduXneIuhhWLd9i2/WvfAuDEpjgnNsV3mKZAnSNsckM0OsIW12W969IsQjNCyoF6IqRESQnExSElEvyvIB5xHBLikBI/QaWAdNA684LXUGlJVP4fqyfbp6UFJA2RlD/sKAyoh5Trj3M8CKX9dZQ2QNqFQVuhqBlqS/zp8Qgkw7DPVmgOQyISLIP/B1W+DWpKYehGZfVQoSjur9sLviAcIO34ry2qhNIQj8LWUnC94KH+a6UdKGmEVMhfd9oBz/HjCqcgGXxswilojEEkvX04lgB1lFBa/OVccNNKrDmIO+rHUtSkpF1IRiDaDOr67wfix6bBspEkJGKK4/kb6qbBVUVFSIeVaBzSIRDP395UWIkXQyjpxyT425aI+cuG0tu/sBxVPEcIJYGYRyrpIGkhHfJfwxMl4iihhOAVezSGhFCDQzIVIhxKoWGPIs//PLhAyoFoQnBVaS6CBhHCKX/7HAfSUQ834RBWj0jIoz4dIu2Ahj1IC7Hg6y8ZElKOEEoppISmiDAg7pFw/Q9VxPXnT6sQDvlxxTx/3pbPXjQFUcejMeJ/OB2Ulu9//zPg7wdVIYmQEIh6kA4rpIWStEfShQQOdWGhJA6EPUKukkoL0iwUp5R0FEgKobBHXUQoSgTvuSiNjh9PUZPgxTySOKhAVBXPUUKekE4JXghiSY+iJGwLCemQ4MUd1FFSRUpps+IBUVdJba2Fbj67Oxct/qOBlaq6GkBEHgXOBro98T9w0HQ+Xr2KtfvU8qetz3DEQ3+jsXgoKmHSIsQ2rGE4sDXi8LO5/4fV1PDjYNmxb7zDlvp+oND03r+zrChG9OV1AGx8s4zDPrku8zott2yJAlvot2MQCilcyqmhiGbK2cJmLSMuwicyGNRjkNZQTIJq3YeNsg8H6EZWeYM5OLyVgelNrNNBpML9EU0imqYxuj8lBx5BOFlHU0M9saJ+RCIRwiWDiEYihEMuEVfop0KR5zFQPUaSRiXkt9hFcER3aIpo3P8Ji+uAgjY3o81+SWlNp9CmOJpKocnk9vGeh9fURLquDk0mSdfUQNpvnWkigSaT4HX8y6PbLMp1GYyduyza6g7p6wfj3d3PYrrdgV+p2P1MXZSLxD8E+LjV82pgws4zici3gW8DHHTQnn3dbZ09G6emloazqngj9havSwJYlZm+z8HK2cCT41yWOe8TUuXjkSkOXB4i3SxsXOp395RSs8N66z+NUf9prMvxbKNlGaWBCDH8XxINRGkgSpRmDuRTAEbg1/pvJEoZ9cD220OW8xm85n9PFu/0Gong0WXh4MynIFFLNIoE48R1cYqKMuOdWLAdIjjFxUQGD0YiESQawYnGAMUpLkYiUdyy/qin4Hm45eWgitOvGAmHkUgETaUI7bMPXn09XjyOhMOE9qnAKS7Ca2zCa6hHQiFwHCQcQSIt9ZD812heuxY8DycWw+nfH6e4mHRtHaSSOP3L0OYEOA5OLIaEQjR/9JH/xRUKIaEw4vh9sumtW3GK++EUFyGRKKRTaDqNNif97p9kknTNZ2gyhTuwHHEctLmZdN02QhUVEHQ3ievgxeNoMom4Ll5T3H+N4AtRIlG8pkbc/v3RpN8813gcr7GJ9LY6QgMH4paVkd66FU17aHMzbll/cF0Ivmjd0lK8eAIJ+fslVVNDauNG3HI/rnRtHe6AAWi8yd/OSMTve06lkZCLl0ggboh0XR1uaQmEQnjb6pGo//5qUxNeY5PfZPeCGMrLcYpipGpq/H3rOEH8/mcAEdI1n5GuqSE0eDBuaSnpzz4j9VkNkYMOBMTfl5EImkrjbavDKe3v78lE3H+fwmGcoiIkEia9tRZ30EC8hgbEDaGplP/eNsX9zwPgJeK4/cv8z6l64LpoPIEm/HWpp4gjII7/c0cEHPEbOJljC37cTml/f5lEHFkMVkYAAAcfSURBVEIhUhs2IpEwTjSamVci/t+E19CA07/Ubww1J4JpEf9zJQKquAMHka6pQUKuPy6ITSIRnOKiTOPJKSkBzyNVU4Nb2h8JuaRra5FYLLMvig4etid/0R2Sni4YJiLnAaep6qXB86nABFW9or1lxo8fr4sXL25vcru8xsYg0fTDU4/mVJKGZDNeOomrHhKKEnbDNDc3UhSJEg1FcdwQnqf+H36i2e/EcPwPjv9h8tBEotUHJ7NlrbZxl41ue7gL03ZpK3bHOu10UGPymogsUdXxO4/PRYv/E+DAVs+HBuO6nVO8vT3siEMsHCUWju46Y3TH1rvjCDjh7a3gVgQgEtllvDHG9BW5OKfvdeDzIjJcRCLABcDTOYjDGGMKUo+3+FU1JSJXAPPwjxbNUtVlPR2HMcYUqh7v498TIrKJlstou24fYHM3htMX2DYXBtvmwrA323ywqu5yWlCfSPx7Q0QWt3VwI5/ZNhcG2+bCkI1ttuv2jTGmwFjiN8aYAlMIif+eXAeQA7bNhcG2uTB0+zbnfR+/McaYHRVCi98YY0wreZ34ReQ0EXlPRFaKyHW7X6L3E5EDReSvIvKuiCwTkX8Lxg8Ukb+IyAfB/+XBeBGR24P34B8iclRut2DPiYgrIm+KyDPB8+EisijYtseCCwIRkWjwfGUwfVgu495TIjJARGaLyAoRWS4ix+b7fhaR7wWf63dE5BERieXbfhaRWSKyUUTeaTWuy/tVRKYF838gItO6EkPeJv5W5Z9PBw4HLhSRw3MbVbdIAd9X1cOBY4DLg+26Dpivqp8H5gfPwd/+zwePbwMzej7kbvNvwPJWz28CblXVfwJqgEuC8ZcANcH4W4P5+qLbgP9V1cOASvxtz9v9LCJDgCuB8ap6BP4FnheQf/v5AeC0ncZ1ab+KyEDgJ/gFLo8GftLyZdEpqpqXD+BYYF6r59cD1+c6rixs51P49zZ4DzggGHcA8F4wfDdwYav5M/P1pQd+Taf5wMnAM/hlkzYDoZ33N/5V4ccGw6FgPsn1NnRxe8uAD3eOO5/3M9sr9w4M9tszwJR83M/AMOCdPd2vwIXA3a3G7zDf7h552+Kn7fLPQ3IUS1YEP23HAouA/VR1XTBpPbBfMJwv78NvgX+n5cZVMAjYqqqp4Hnr7cpsczC9Npi/LxkObALuD7q37hORfuTxflbVT4BbgLXAOvz9toT83s8turpf92p/53Piz2siUgI8DlylqnWtp6nfBMib07VE5MvARlVdkutYelAIOAqYoapjgQa2//wH8nI/l+PflGk4MBjox65dInmvJ/ZrPif+Hiv/3NNEJIyf9B9W1SeC0RtE5IBg+gHAxmB8PrwPE4GzRGQN8Ch+d89twAARaSk02Hq7MtscTC8DtvRkwN2gGqhW1UXB89n4XwT5vJ+/CHyoqptUNQk8gb/v83k/t+jqft2r/Z3PiT8vyz+LiAAzgeWq+ptWk54GWo7sT8Pv+28Z//Xg7IBjgNpWPyn7BFW9XlWHquow/P24QFUvBv4KBDc53mWbW96L84L5+1TLWFXXAx+LyIhg1Cn4tyfN2/2M38VzjIgUB5/zlm3O2/3cSlf36zzgVBEpD34pnRqM65xcH+TI8gGUM4D38e+3+KNcx9NN23Q8/s/AfwBLg8cZ+H2b84EPgOeBgcH8gn920yrgbfwzJnK+HXux/ScBzwTDhwCv4d+a/k9ANBgfC56vDKYfkuu493BbxwCLg309ByjP9/0M/AxYAbwDPIR/K+u82s/AI/jHMJL4v+wu2ZP9Cnwz2PaVwDe6EoNduWuMMQUmn7t6jDHGtMESvzHGFBhL/MYYU2As8RtjTIGxxG+MMQXGEr8xWSYiJ7VUFDWmN7DEb4wxBcYSvzEBEfkXEXlNRJaKyN1B/f96Ebk1qBE/X0QqgnnHiMirQY30J1vVT/8nEXleRN4SkTdE5HPB6kta1dZ/OLgy1ZicsMRvDCAiI4HzgYmqOgZIAxfjFwpbrKqjgBfwa6ADPAhcq6qj8a+obBn/MPA7Va0EjsO/QhP8KqpX4d8b4hD8GjTG5ERo97MYUxBOAcYBrweN8SL8Qlke8Fgwz/8DnhCRMmCAqr4QjP898CcRKQWGqOqTAKoaBwjW95qqVgfPl+LXY/979jfLmF1Z4jfGJ8DvVfX6HUaK/Hin+fa0xkmi1XAa+9szOWRdPcb45gPnici+kLkH6sH4fyMtlSEvAv6uqrVAjYicEIyfCrygqtuAahE5J1hHVESKe3QrjOkEa3UYA6jquyJyA/BnEXHwKydejn8DlKODaRvxjwOAXzr3riCxrwa+EYyfCtwtIj8P1vG1HtwMYzrFqnMa0wERqVfVklzHYUx3sq4eY4wpMNbiN8aYAmMtfmOMKTCW+I0xpsBY4jfGmAJjid8YYwqMJX5jjCkwlviNMabA/H/NbazJpVR+/gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Elapsed: 9.20930567185084 min \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_7u4rLfV_Mt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cfbe5ca-66dc-49ba-cf7b-453c4a7e1b80"
      },
      "source": [
        "%sx read -p \"enter epochs :\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enter epochs :90']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4WCOpRrpMnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        " args = parser.parse_args(args=[])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}